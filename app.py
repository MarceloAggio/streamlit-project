import pandas as pd
import streamlit as st
import numpy as np
from datetime import datetime
import plotly.graph_objects as go
from scipy import stats, signal
from scipy.fft import fft, fftfreq
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
import warnings
from multiprocessing import Pool, cpu_count
import holidays
from functools import partial
from collections import Counter
import math
import sys
from pathlib import Path

# Adicionar caminho para imports locais

sys.path.insert(0, str(Path(**file**).parent))

# Importar mÃ³dulos de comparaÃ§Ã£o e storage

try:
from compare_results import ResultsComparator
from data_storage import DataStorage
from src.athena.teste import process_query_external
ATHENA_AVAILABLE = True
except ImportError as e:
st.warning(fâ€âš ï¸ MÃ³dulos de integraÃ§Ã£o nÃ£o disponÃ­veis: {e}â€)
ATHENA_AVAILABLE = False

warnings.filterwarnings(â€˜ignoreâ€™)

st.set_page_config(
page_title=â€œAnalisador de Alertasâ€,
page_icon=â€œğŸš¨â€,
layout=â€œwideâ€,
initial_sidebar_state=â€œexpandedâ€
)

# â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“

# Helpers para multiprocessing

# â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“

def analyze_single_u_alert_id_recurrence(u_alert_id, df_original):
try:
df_ci = df_original[df_original[â€˜u_alert_idâ€™] == u_alert_id].copy()
df_ci[â€˜created_onâ€™] = pd.to_datetime(df_ci[â€˜created_onâ€™], errors=â€˜coerceâ€™)
df_ci = df_ci.dropna(subset=[â€˜created_onâ€™]).sort_values(â€˜created_onâ€™)

```
if len(df_ci) < 3:
  return {
    'u_alert_id': u_alert_id,
    'total_occurrences': len(df_ci),
    'score': 0,
    'classification': 'âšª DADOS INSUFICIENTES',
    'mean_interval_hours': None,
    'cv': None,
    'regularity_score': 0,
    'periodicity_detected': False,
    'predictability_score': 0
  }

analyzer = AdvancedRecurrenceAnalyzer(df_ci, u_alert_id)
return analyzer.analyze_complete_silent()
```

except Exception as e:
return {
â€˜u_alert_idâ€™: u_alert_id,
â€˜total_occurrencesâ€™: 0,
â€˜scoreâ€™: 0,
â€˜classificationâ€™: fâ€™âšª ERRO: {str(e)[:50]}â€™,
â€˜mean_interval_hoursâ€™: None,
â€˜cvâ€™: None,
â€˜regularity_scoreâ€™: 0,
â€˜periodicity_detectedâ€™: False,
â€˜predictability_scoreâ€™: 0
}

def analyze_chunk_recurrence(u_alert_id_list, df_original):
results = []
for u_alert_id in u_alert_id_list:
result = analyze_single_u_alert_id_recurrence(u_alert_id, df_original)
if result:
results.append(result)
return results

# ============================================================

# [MANTER TODA A CLASSE AdvancedRecurrenceAnalyzer ORIGINAL]

# (CÃ³digo muito longo - mantido exatamente como estÃ¡)

# ============================================================

class AdvancedRecurrenceAnalyzer:
def **init**(self, df, alert_id):
self.df = df.copy() if df is not None else None
self.alert_id = alert_id

def _prepare_data(self):
if self.df is None or len(self.df) < 3:
return None
df = self.df.sort_values(â€˜created_onâ€™).copy()
df[â€˜created_onâ€™] = pd.to_datetime(df[â€˜created_onâ€™], errors=â€˜coerceâ€™)
df = df.dropna(subset=[â€˜created_onâ€™])
df[â€˜timestampâ€™] = df[â€˜created_onâ€™].astype(â€˜int64â€™) // 10**9
df[â€˜time_diff_secondsâ€™] = df[â€˜timestampâ€™].diff()
df[â€˜time_diff_hoursâ€™] = df[â€˜time_diff_secondsâ€™] / 3600
dt = df[â€˜created_onâ€™].dt
df[â€˜hourâ€™] = dt.hour
df[â€˜day_of_weekâ€™] = dt.dayofweek
df[â€˜day_of_monthâ€™] = dt.day
df[â€˜week_of_yearâ€™] = dt.isocalendar().week
df[â€˜monthâ€™] = dt.month
df[â€˜day_nameâ€™] = dt.day_name()
df[â€˜is_weekendâ€™] = df[â€˜day_of_weekâ€™].isin([5, 6])
df[â€˜is_business_hoursâ€™] = (df[â€˜hourâ€™] >= 9) & (df[â€˜hourâ€™] <= 17)
return df

def analyze(self):
â€œâ€â€œModo interativo (Streamlit)â€â€â€
st.header(â€œğŸ”„ AnÃ¡lise AvanÃ§ada de ReincidÃªncia Temporalâ€)
df = self._prepare_data()
if df is None:
st.warning(â€œâš ï¸ Dados insuficientes (mÃ­nimo 3 ocorrÃªncias).â€)
return

```
st.info(f"ğŸ“Š Analisando **{len(df)}** ocorrÃªncias do Short CI: **{self.alert_id}**")
intervals_hours = df['time_diff_hours'].dropna().values
if len(intervals_hours) < 2:
  st.warning("âš ï¸ Intervalos insuficientes.")
  return

results = {}
results['basic_stats'] = self._analyze_basic_statistics(intervals_hours, render=True)
results['regularity'] = self._analyze_regularity(intervals_hours, render=True)
results['periodicity'] = self._analyze_periodicity(intervals_hours, render=True)
results['autocorr'] = self._analyze_autocorrelation(intervals_hours, render=True)
results['temporal'] = self._analyze_temporal_patterns(df, render=True)
results['clusters'] = self._analyze_clusters(df, intervals_hours, render=True)
results['bursts'] = self._detect_bursts(intervals_hours, render=True)
results['seasonality'] = self._analyze_seasonality(df, render=True)
results['changepoints'] = self._detect_changepoints(intervals_hours, render=True)
results['anomalies'] = self._detect_anomalies(intervals_hours, render=True)
results['predictability'] = self._calculate_predictability(intervals_hours, render=True)
results['stability'] = self._analyze_stability(intervals_hours, df, render=True)
results['contextual'] = self._analyze_contextual_dependencies(df, render=True)
results['vulnerability'] = self._identify_vulnerability_windows(df, intervals_hours, render=True)
results['maturity'] = self._analyze_pattern_maturity(df, intervals_hours, render=True)
results['prediction_confidence'] = self._calculate_prediction_confidence(intervals_hours, render=True)
results['markov'] = self._analyze_markov_chains(intervals_hours, render=True)
results['randomness'] = self._advanced_randomness_tests(intervals_hours, render=True)

self._final_classification(results, df, intervals_hours)
```

def analyze_complete_silent(self):
â€œâ€â€œModo silencioso para batch: retorna dict resumoâ€â€â€
df = self._prepare_data()
if df is None or len(df) < 3:
return None
intervals_hours = df[â€˜time_diff_hoursâ€™].dropna().values
if len(intervals_hours) < 2:
return None

```
results = {}
try:
  results['basic_stats'] = self._analyze_basic_statistics(intervals_hours, render=False)
except Exception:
  results['basic_stats'] = {'mean': 0, 'median': 0, 'std': 0, 'cv': 0}

try:
  results['regularity'] = self._analyze_regularity(intervals_hours, render=False)
except Exception:
  results['regularity'] = {'cv': 0, 'regularity_score': 0}

try:
  results['periodicity'] = self._analyze_periodicity(intervals_hours, render=False)
except Exception:
  results['periodicity'] = {'has_strong_periodicity': False, 'has_moderate_periodicity': False, 'dominant_period_hours': None}

try:
  results['predictability'] = self._calculate_predictability(intervals_hours, render=False)
except Exception:
  results['predictability'] = {'predictability_score': 0, 'next_expected_hours': 0}

try:
  results['temporal'] = self._analyze_temporal_patterns(df, render=False)
except Exception:
  results['temporal'] = {'hourly_concentration': 0, 'daily_concentration': 0, 'peak_hours': [], 'peak_days': []}

final_score, classification = self._calculate_final_score_validated(results, df, intervals_hours)

return {
  'u_alert_id': self.alert_id,
  'total_occurrences': len(df),
  'score': final_score,
  'classification': classification,
  'mean_interval_hours': results['basic_stats'].get('mean'),
  'median_interval_hours': results['basic_stats'].get('median'),
  'cv': results['basic_stats'].get('cv'),
  'regularity_score': results['regularity'].get('regularity_score'),
  'periodicity_detected': results['periodicity'].get('has_strong_periodicity', False),
  'dominant_period_hours': results['periodicity'].get('dominant_period_hours'),
  'predictability_score': results['predictability'].get('predictability_score'),
  'next_occurrence_prediction_hours': results['predictability'].get('next_expected_hours'),
  'hourly_concentration': results['temporal'].get('hourly_concentration'),
  'daily_concentration': results['temporal'].get('daily_concentration'),
}
```

# [MANTER TODOS OS MÃ‰TODOS ORIGINAIS]

# _analyze_basic_statistics, _analyze_regularity, _analyze_periodicity, etc.

# (CÃ³digo original completo mantido)

def _analyze_basic_statistics(self, intervals, render=True):
stats_dict = {
â€˜meanâ€™: float(np.mean(intervals)),
â€˜medianâ€™: float(np.median(intervals)),
â€˜stdâ€™: float(np.std(intervals)),
â€˜minâ€™: float(np.min(intervals)),
â€˜maxâ€™: float(np.max(intervals)),
â€˜cvâ€™: float(np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float(â€˜infâ€™)),
â€˜q25â€™: float(np.percentile(intervals, 25)),
â€˜q75â€™: float(np.percentile(intervals, 75)),
â€˜iqrâ€™: float(np.percentile(intervals, 75) - np.percentile(intervals, 25))
}
if render:
st.subheader(â€œğŸ“Š 1. EstatÃ­sticas de Intervalosâ€)
col1, col2, col3, col4, col5 = st.columns(5)
col1.metric(â€œâ±ï¸ MÃ©diaâ€, fâ€{stats_dict[â€˜meanâ€™]:.1f}hâ€)
col2.metric(â€œğŸ“Š Medianaâ€, fâ€{stats_dict[â€˜medianâ€™]:.1f}hâ€)
col3.metric(â€œğŸ“ˆ Desvioâ€, fâ€{stats_dict[â€˜stdâ€™]:.1f}hâ€)
col4.metric(â€œâš¡ MÃ­nimoâ€, fâ€{stats_dict[â€˜minâ€™]:.1f}hâ€)
col5.metric(â€œğŸŒ MÃ¡ximoâ€, fâ€{stats_dict[â€˜maxâ€™]:.1f}hâ€)
return stats_dict

def _analyze_regularity(self, intervals, render=True):
cv = float(np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float(â€˜infâ€™))
if cv < 0.20:
regularity_score, pattern_type, pattern_color = 95, â€œğŸŸ¢ ALTAMENTE REGULARâ€, â€œgreenâ€
elif cv < 0.40:
regularity_score, pattern_type, pattern_color = 80, â€œğŸŸ¢ REGULARâ€, â€œlightgreenâ€
elif cv < 0.70:
regularity_score, pattern_type, pattern_color = 60, â€œğŸŸ¡ SEMI-REGULARâ€, â€œyellowâ€
elif cv < 1.20:
regularity_score, pattern_type, pattern_color = 35, â€œğŸŸ  IRREGULARâ€, â€œorangeâ€
else:
regularity_score, pattern_type, pattern_color = 15, â€œğŸ”´ MUITO IRREGULARâ€, â€œredâ€

```
if render:
  st.subheader("ğŸ¯ 2. Regularidade")
  col1, col2 = st.columns([3, 1])
  with col1:
    st.markdown(f"**ClassificaÃ§Ã£o:** {pattern_type}")
    st.write(f"**CV:** {cv:.2%}")
    if len(intervals) >= 3:
      _, p_value = stats.shapiro(intervals)
      if p_value > 0.05:
        st.info("ğŸ“Š DistribuiÃ§Ã£o aproximadamente normal")
      else:
        st.warning("ğŸ“Š DistribuiÃ§Ã£o nÃ£o-normal")
  with col2:
    fig = go.Figure(go.Indicator(
      mode="gauge+number",
      value=regularity_score,
      title={'text': "Regularidade"},
      gauge={'axis': {'range': [0, 100]}, 'bar': {'color': pattern_color}}
    ))
    fig.update_layout(height=250)
    st.plotly_chart(fig, use_container_width=True, key=f'reg_gauge_{self.alert_id}')
return {'cv': cv, 'regularity_score': regularity_score, 'type': pattern_type}
```

def _analyze_periodicity(self, intervals, render=True):
if len(intervals) < 10:
if render:
st.subheader(â€œğŸ” 3. Periodicidade (FFT)â€)
st.info(â€œğŸ“Š MÃ­nimo de 10 intervalos necessÃ¡riosâ€)
return {â€˜periodsâ€™: [], â€˜has_periodicityâ€™: False, â€˜has_strong_periodicityâ€™: False, â€˜dominant_period_hoursâ€™: None}

```
intervals_norm = (intervals - np.mean(intervals)) / np.std(intervals)
n_padded = 2**int(np.ceil(np.log2(len(intervals_norm))))
intervals_padded = np.pad(intervals_norm, (0, n_padded - len(intervals_norm)), 'constant')
fft_vals = fft(intervals_padded)
freqs = fftfreq(n_padded, d=1)
positive_idx = freqs > 0
freqs_pos = freqs[positive_idx]
fft_mag = np.abs(fft_vals[positive_idx])
threshold = np.mean(fft_mag) + 2 * np.std(fft_mag)
peaks_idx = fft_mag > threshold

dominant_periods = []
has_strong_periodicity = False
dominant_period_hours = None
if np.any(peaks_idx):
  dominant_freqs = freqs_pos[peaks_idx]
  dominant_periods = (1 / dominant_freqs)
  dominant_periods = dominant_periods[dominant_periods < len(intervals)][:3]
  if len(dominant_periods) > 0:
    has_strong_periodicity = True
    dominant_period_hours = float(dominant_periods[0] * np.mean(intervals))

if render:
  st.subheader("ğŸ” 3. Periodicidade (FFT)")
  if has_strong_periodicity:
    st.success("ğŸ¯ **Periodicidades Detectadas:**")
    for period in dominant_periods:
      est_time = period * np.mean(intervals)
      time_str = f"{est_time:.1f}h" if est_time < 24 else f"{est_time/24:.1f} dias"
      st.write(f"â€¢ PerÃ­odo: **{period:.1f}** ocorrÃªncias (~{time_str})")
  else:
    st.info("ğŸ“Š Nenhuma periodicidade forte detectada")

  fig = go.Figure()
  fig.add_trace(go.Scatter(
    x=1/freqs_pos[:len(freqs_pos)//4],
    y=fft_mag[:len(freqs_pos)//4],
    mode='lines',
    fill='tozeroy'
  ))
  fig.update_layout(title="Espectro de FrequÃªncia", xaxis_title="PerÃ­odo", yaxis_title="Magnitude", height=300, xaxis_type="log")
  st.plotly_chart(fig, use_container_width=True, key=f'fft_{self.alert_id}')

return {'periods': list(map(float, dominant_periods)) if len(dominant_periods) else [], 'has_periodicity': len(dominant_periods) > 0, 'has_strong_periodicity': has_strong_periodicity, 'dominant_period_hours': dominant_period_hours}
```

def _analyze_autocorrelation(self, intervals, render=True):
if len(intervals) < 5:
if render:
st.subheader(â€œğŸ“ˆ 4. AutocorrelaÃ§Ã£oâ€)
st.info(â€œInsuficiente para autocorrelaÃ§Ã£oâ€)
return {â€˜peaksâ€™: [], â€˜has_autocorrâ€™: False, â€˜max_autocorrâ€™: 0}

```
intervals_norm = (intervals - np.mean(intervals)) / np.std(intervals)
autocorr = signal.correlate(intervals_norm, intervals_norm, mode='full')
autocorr = autocorr[len(autocorr)//2:]
autocorr = autocorr / autocorr[0]
lags = np.arange(len(autocorr))
threshold = 2 / np.sqrt(len(intervals))
significant_peaks = [(i, float(autocorr[i])) for i in range(1, min(len(autocorr), 20)) if autocorr[i] > threshold]
max_autocorr = max([corr for _, corr in significant_peaks], default=0)

if render:
  st.subheader("ğŸ“ˆ 4. AutocorrelaÃ§Ã£o")
  if significant_peaks:
    st.success("âœ… **AutocorrelaÃ§Ã£o Significativa:**")
    for lag, corr in significant_peaks[:3]:
      st.write(f"â€¢ Lag {lag}: {corr:.2f}")
  else:
    st.info("ğŸ“Š Sem autocorrelaÃ§Ã£o significativa")

  fig = go.Figure()
  fig.add_trace(go.Scatter(x=lags[:min(30, len(lags))], y=autocorr[:min(30, len(autocorr))], mode='lines+markers'))
  fig.add_hline(y=threshold, line_dash="dash", line_color="red")
  fig.add_hline(y=-threshold, line_dash="dash", line_color="red")
  fig.update_layout(title="AutocorrelaÃ§Ã£o", height=300)
  st.plotly_chart(fig, use_container_width=True, key=f'autocorr_{self.alert_id}')

return {'peaks': significant_peaks, 'has_autocorr': len(significant_peaks) > 0, 'max_autocorr': max_autocorr}
```

def _analyze_temporal_patterns(self, df, render=True):
hourly = df.groupby(â€˜hourâ€™).size().reindex(range(24), fill_value=0)
daily = df.groupby(â€˜day_of_weekâ€™).size().reindex(range(7), fill_value=0)
hourly_pct = (hourly / hourly.sum() * 100) if hourly.sum() > 0 else pd.Series()
daily_pct = (daily / daily.sum() * 100) if daily.sum() > 0 else pd.Series()
hourly_conc = float(hourly_pct.nlargest(3).sum()) if len(hourly_pct) > 0 else 0.0
daily_conc = float(daily_pct.nlargest(3).sum()) if len(daily_pct) > 0 else 0.0
peak_hours = hourly[hourly > hourly.mean() + hourly.std()].index.tolist() if len(hourly) > 0 else []
peak_days = daily[daily > daily.mean() + daily.std()].index.tolist() if len(daily) > 0 else []

```
if render:
  st.subheader("â° 5. PadrÃµes Temporais")
  col1, col2 = st.columns(2)
  with col1:
    fig = go.Figure(go.Bar(x=list(range(24)), y=hourly.values, marker_color=['red' if v > hourly.mean() + hourly.std() else 'lightblue' for v in hourly.values]))
    fig.update_layout(title="Por Hora", xaxis_title="Hora", height=250)
    st.plotly_chart(fig, use_container_width=True, key=f'hourly_{self.alert_id}')
    if peak_hours:
      st.success(f"ğŸ• **Picos:** {', '.join([f'{h:02d}:00' for h in peak_hours])}")
  with col2:
    days_map = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'SÃ¡b', 'Dom']
    fig = go.Figure(go.Bar(x=days_map, y=daily.values, marker_color=['red' if v > daily.mean() + daily.std() else 'lightgreen' for v in daily.values]))
    fig.update_layout(title="Por Dia", xaxis_title="Dia", height=250)
    st.plotly_chart(fig, use_container_width=True, key=f'daily_{self.alert_id}')
    if peak_days:
      st.success(f"ğŸ“… **Picos:** {', '.join([days_map[d] for d in peak_days])}")

return {'hourly_concentration': hourly_conc, 'daily_concentration': daily_conc, 'peak_hours': peak_hours, 'peak_days': peak_days}
```

def _analyze_clusters(self, df, intervals, render=True):
if len(df) < 10:
if render:
st.subheader(â€œğŸ¯ 6. Clusters Temporaisâ€)
st.info(â€œMÃ­nimo de 10 ocorrÃªncias necessÃ¡rioâ€)
return {â€˜n_clustersâ€™: 0, â€˜n_noiseâ€™: 0}

```
first_ts = df['timestamp'].min()
time_features = ((df['timestamp'] - first_ts) / 3600).values.reshape(-1, 1)
eps = float(np.median(intervals) * 2) if len(intervals) > 0 else 1.0
clusters = DBSCAN(eps=eps, min_samples=3).fit_predict(time_features)
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
n_noise = list(clusters).count(-1)

if render:
  st.subheader("ğŸ¯ 6. Clusters Temporais")
  col1, col2, col3 = st.columns(3)
  col1.metric("ğŸ¯ Clusters", n_clusters)
  col2.metric("ğŸ“Š Em Clusters", len(clusters) - n_noise)
  col3.metric("ğŸ”´ Isolados", n_noise)
  if n_clusters > 0:
    st.success(f"âœ… **{n_clusters} clusters** identificados")

return {'n_clusters': int(n_clusters), 'n_noise': int(n_noise)}
```

def _detect_bursts(self, intervals, render=True):
if len(intervals) < 3:
if render:
st.subheader(â€œğŸ’¥ 7. DetecÃ§Ã£o de Burstsâ€)
st.info(â€œInsuficiente para detectar burstsâ€)
return {â€˜n_burstsâ€™: 0, â€˜has_burstsâ€™: False}

```
burst_threshold = np.percentile(intervals, 25)
is_burst = intervals < burst_threshold
burst_changes = np.diff(np.concatenate(([False], is_burst, [False])))
burst_starts = np.where(burst_changes == 1)[0]
burst_ends = np.where(burst_changes == -1)[0]
burst_sequences = [(int(start), int(end)) for start, end in zip(burst_starts, burst_ends) if end - start >= 3]

if render:
  st.subheader("ğŸ’¥ 7. DetecÃ§Ã£o de Bursts")
  col1, col2 = st.columns(2)
  col1.metric("ğŸ’¥ Bursts", len(burst_sequences))
  if burst_sequences:
    avg_size = np.mean([end - start for start, end in burst_sequences])
    col2.metric("ğŸ“Š Tamanho MÃ©dio", f"{avg_size:.1f}")
    st.warning(f"âš ï¸ **{len(burst_sequences)} bursts** detectados")
  else:
    st.success("âœ… Sem padrÃ£o de rajadas")

return {'n_bursts': int(len(burst_sequences)), 'has_bursts': len(burst_sequences) > 0}
```

def _analyze_seasonality(self, df, render=True):
date_range = (df[â€˜created_onâ€™].max() - df[â€˜created_onâ€™].min()).days
if render:
st.subheader(â€œğŸŒ¡ï¸ 8. Sazonalidadeâ€)
if date_range < 30:
if render:
st.info(â€œğŸ“Š PerÃ­odo curto para anÃ¡lise sazonalâ€)
return {â€˜trendâ€™: â€˜stableâ€™}

```
weekly = df.groupby('week_of_year').size()
if len(weekly) >= 4 and render:
  fig = go.Figure()
  fig.add_trace(go.Scatter(x=weekly.index, y=weekly.values, mode='lines+markers', fill='tozeroy'))
  fig.update_layout(title="EvoluÃ§Ã£o Semanal", height=250)
  st.plotly_chart(fig, use_container_width=True, key=f'weekly_{self.alert_id}')
  if len(weekly) > 3:
    slope, _, _, p_value, _ = stats.linregress(weekly.index.values, weekly.values)
    if p_value < 0.05:
      if slope > 0:
        st.warning("ğŸ“ˆ **TendÃªncia crescente**")
        return {'trend': 'increasing', 'slope': float(slope)}
      else:
        st.success("ğŸ“‰ **TendÃªncia decrescente**")
        return {'trend': 'decreasing', 'slope': float(slope)}
return {'trend': 'stable'}
```

def _detect_changepoints(self, intervals, render=True):
if len(intervals) < 20:
if render:
st.subheader(â€œğŸ”€ 9. Pontos de MudanÃ§aâ€)
st.info(â€œMÃ­nimo de 20 intervalos necessÃ¡rioâ€)
return {â€˜changepointsâ€™: [], â€˜has_changesâ€™: False}

```
cumsum = np.cumsum(intervals - np.mean(intervals))
window = 5
changes = []
for i in range(window, len(cumsum) - window):
  before = np.mean(intervals[max(0, i - window):i])
  after = np.mean(intervals[i:min(len(intervals), i + window)])
  if abs(before - after) > np.std(intervals):
    changes.append(int(i))

filtered = []
for cp in changes:
  if not filtered or cp - filtered[-1] > 5:
    filtered.append(cp)

if render:
  st.subheader("ğŸ”€ 9. Pontos de MudanÃ§a")
  if filtered:
    st.warning(f"âš ï¸ **{len(filtered)} pontos de mudanÃ§a** detectados")
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=list(range(len(cumsum))), y=cumsum, mode='lines'))
    for cp in filtered:
      fig.add_vline(x=cp, line_dash="dash", line_color="red")
    fig.update_layout(title="CUSUM", height=250)
    st.plotly_chart(fig, use_container_width=True, key=f'cusum_{self.alert_id}')
  else:
    st.success("âœ… Comportamento estÃ¡vel")

return {'changepoints': filtered, 'has_changes': len(filtered) > 0}
```

def _detect_anomalies(self, intervals, render=True):
if len(intervals) == 0:
return {â€˜anomaly_rateâ€™: 0.0, â€˜total_anomaliesâ€™: 0}

```
z_scores = np.abs(stats.zscore(intervals))
z_anomalies = int(np.sum(z_scores > 3))
q1, q3 = np.percentile(intervals, [25, 75])
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
iqr_anomalies = int(np.sum((intervals < lower) | (intervals > upper)))

iso_anomalies = 0
if len(intervals) >= 10:
  iso_forest = IsolationForest(contamination=0.1, random_state=42)
  predictions = iso_forest.fit_predict(intervals.reshape(-1, 1))
  iso_anomalies = int(np.sum(predictions == -1))

total_anomalies = max(z_anomalies, iqr_anomalies, iso_anomalies)
anomaly_rate = float(total_anomalies / len(intervals) * 100)

if render:
  st.subheader("ğŸš¨ 10. DetecÃ§Ã£o de Anomalias")
  col1, col2, col3 = st.columns(3)
  col1.metric("Z-Score", f"{z_anomalies}")
  col2.metric("IQR", f"{iqr_anomalies}")
  col3.metric("Iso. Forest", f"{iso_anomalies}")
  if anomaly_rate > 10:
    st.warning(f"âš ï¸ **{anomaly_rate:.1f}%** de anomalias")
  else:
    st.success("âœ… Baixa taxa de anomalias")

return {'anomaly_rate': anomaly_rate, 'total_anomalies': total_anomalies}
```

def _calculate_predictability(self, intervals, render=True):
cv = float(np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float(â€˜infâ€™))
if cv < 0.20:
predictability = 95
elif cv < 0.40:
predictability = 80
elif cv < 0.70:
predictability = 55
elif cv < 1.20:
predictability = 30
else:
predictability = 10
mean_interval = float(np.mean(intervals))

```
if render:
  st.subheader("ğŸ”® 11. Previsibilidade")
  col1, col2 = st.columns(2)
  col1.metric("Score", f"{predictability}%")
  col2.metric("PrÃ³xima OcorrÃªncia", f"{mean_interval:.1f}h")
  if predictability > 70:
    st.success("âœ… Altamente previsÃ­vel")
  elif predictability > 50:
    st.info("ğŸ“Š Moderadamente previsÃ­vel")
  else:
    st.warning("âš ï¸ Pouco previsÃ­vel")

return {'predictability_score': int(predictability), 'next_expected_hours': mean_interval}
```

def _analyze_stability(self, intervals, df, render=True):
if len(intervals) < 10:
return {â€˜is_stableâ€™: True, â€˜stability_scoreâ€™: 50, â€˜drift_pctâ€™: 0.0}
mid = len(intervals) // 2
first_half = intervals[:mid]
second_half = intervals[mid:]
_, p_value = stats.ttest_ind(first_half, second_half)
is_stable = p_value > 0.05
mean_diff = abs(np.mean(second_half) - np.mean(first_half))
drift_pct = float((mean_diff / np.mean(first_half)) * 100 if np.mean(first_half) > 0 else 0)
stability_score = float(max(0, 100 - drift_pct))

```
if render:
  st.subheader("ğŸ“Š 12. Estabilidade")
  col1, col2 = st.columns(2)
  col1.metric("Score", f"{stability_score:.1f}%")
  col2.metric("Drift", f"{drift_pct:.1f}%")
  if is_stable and drift_pct < 20:
    st.success("âœ… PadrÃ£o estÃ¡vel")
  elif drift_pct < 50:
    st.info("ğŸ“Š Moderadamente estÃ¡vel")
  else:
    st.warning("âš ï¸ PadrÃ£o instÃ¡vel")

return {'is_stable': bool(is_stable), 'stability_score': stability_score, 'drift_pct': drift_pct}
```

def _analyze_contextual_dependencies(self, df, render=True):
try:
years = df[â€˜created_onâ€™].dt.year.unique()
br_holidays = holidays.Brazil(years=years)
df[â€˜is_holidayâ€™] = df[â€˜created_onâ€™].dt.date.apply(lambda x: x in br_holidays)
except Exception:
df[â€˜is_holidayâ€™] = False

```
business_days = df[~df['is_weekend'] & ~df['is_holiday']]
weekend_days = df[df['is_weekend']]
holiday_days = df[df['is_holiday']]

if render:
  st.subheader("ğŸŒ 13. DependÃªncias Contextuais")
  col1, col2, col3 = st.columns(3)
  col1.metric("ğŸ“Š Dias Ãšteis", f"{len(business_days)/len(df)*100:.1f}%")
  col2.metric("ğŸ‰ Fins de Semana", f"{len(weekend_days)/len(df)*100:.1f}%")
  col3.metric("ğŸŠ Feriados", f"{len(holiday_days)/len(df)*100:.1f}%")
  if len(holiday_days) > 0:
    st.warning(f"âš ï¸ {len(holiday_days)} alertas em feriados")

return {'holiday_correlation': float(len(holiday_days) / len(df) if len(df) > 0 else 0), 'weekend_correlation': float(len(weekend_days) / len(df) if len(df) > 0 else 0)}
```

def _identify_vulnerability_windows(self, df, intervals, render=True):
vulnerability_matrix = df.groupby([â€˜day_of_weekâ€™, â€˜hourâ€™]).size().reset_index(name=â€˜countâ€™)
if vulnerability_matrix.empty:
return {â€˜top_windowsâ€™: []}
vulnerability_matrix[â€˜risk_scoreâ€™] = (vulnerability_matrix[â€˜countâ€™] / vulnerability_matrix[â€˜countâ€™].max() * 100)
top_windows = vulnerability_matrix.nlargest(5, â€˜risk_scoreâ€™)
day_map = {0: â€˜Segâ€™, 1: â€˜Terâ€™, 2: â€˜Quaâ€™, 3: â€˜Quiâ€™, 4: â€˜Sexâ€™, 5: â€˜SÃ¡bâ€™, 6: â€˜Domâ€™}
if render:
st.subheader(â€œğŸ¯ 14. Janelas de Vulnerabilidadeâ€)
st.write(â€**ğŸ”´ Top 5 Janelas CrÃ­ticas:**â€)
for idx, row in top_windows.iterrows():
day = day_map[row[â€˜day_of_weekâ€™]]
hour = int(row[â€˜hourâ€™])
risk = row[â€˜risk_scoreâ€™]
st.write(fâ€â€¢ **{day} {hour:02d}:00** - Score: {risk:.1f} ({row[â€˜countâ€™]} alertas)â€)
return {â€˜top_windowsâ€™: top_windows.to_dict(â€˜recordsâ€™)}

def _analyze_pattern_maturity(self, df, intervals, render=True):
n_periods = 4
period_size = len(intervals) // n_periods
if period_size < 2:
if render:
st.subheader(â€œğŸ“ˆ 15. Maturidade do PadrÃ£oâ€)
st.info(â€œPerÃ­odo insuficienteâ€)
return {â€˜maturityâ€™: â€˜stableâ€™, â€˜slopeâ€™: 0.0}

```
periods_stats = []
for i in range(n_periods):
  start = i * period_size
  end = (i + 1) * period_size if i < n_periods - 1 else len(intervals)
  period_intervals = intervals[start:end]
  periods_stats.append({'period': i + 1, 'mean': float(np.mean(period_intervals)), 'cv': float(np.std(period_intervals) / np.mean(period_intervals) if np.mean(period_intervals) > 0 else 0)})

periods_df = pd.DataFrame(periods_stats)
slope = float(np.polyfit(periods_df['period'], periods_df['cv'], 1)[0])

if render:
  st.subheader("ğŸ“ˆ 15. Maturidade do PadrÃ£o")
  fig = go.Figure()
  fig.add_trace(go.Scatter(x=periods_df['period'], y=periods_df['cv'], mode='lines+markers', name='CV', line=dict(color='red', width=3)))
  fig.update_layout(title="EvoluÃ§Ã£o da Variabilidade", xaxis_title="PerÃ­odo", yaxis_title="CV", height=300)
  st.plotly_chart(fig, use_container_width=True, key=f'maturity_{self.alert_id}')
  if slope < -0.05:
    st.success("âœ… **Amadurecendo**: Variabilidade decrescente")
    maturity = "maturing"
  elif slope > 0.05:
    st.warning("âš ï¸ **Degradando**: Variabilidade crescente")
    maturity = "degrading"
  else:
    st.info("ğŸ“Š **EstÃ¡vel**: Variabilidade constante")
    maturity = "stable"
else:
  maturity = "maturing" if slope < -0.05 else ("degrading" if slope > 0.05 else "stable")

return {'maturity': maturity, 'slope': slope}
```

def _calculate_prediction_confidence(self, intervals, render=True):
if len(intervals) < 10:
return {â€˜confidenceâ€™: â€˜lowâ€™, â€˜scoreâ€™: 0}
cv = float(np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float(â€˜infâ€™))
n_samples = len(intervals)
regularity_score = max(0, 100 - cv * 100)
sample_score = min(100, (n_samples / 50) * 100)
mid = len(intervals) // 2
var1 = np.var(intervals[:mid])
var2 = np.var(intervals[mid:])
var_ratio = min(var1, var2) / max(var1, var2) if max(var1, var2) > 0 else 0
stationarity_score = var_ratio * 100
confidence_score = (regularity_score * 0.5 + sample_score * 0.3 + stationarity_score * 0.2)
confidence = â€˜highâ€™ if confidence_score > 70 else (â€˜mediumâ€™ if confidence_score > 40 else â€˜lowâ€™)

```
if render:
  st.subheader("ğŸ¯ 16. ConfianÃ§a de PrediÃ§Ã£o")
  col1, col2 = st.columns(2)
  col1.metric("ConfianÃ§a", confidence.upper())
  col2.metric("Score", f"{confidence_score:.1f}%")

return {'confidence': confidence, 'score': float(confidence_score)}
```

def _analyze_markov_chains(self, intervals, render=True):
if len(intervals) < 20:
if render:
st.subheader(â€œğŸ”— 17. Cadeias de Markovâ€)
st.info(â€œMÃ­nimo de 20 intervalos necessÃ¡rioâ€)
return {â€˜markov_scoreâ€™: 0.0}
q25, q50, q75 = np.percentile(intervals, [25, 50, 75])
def interval_to_state(val):
if val <= q25:
return 0
elif val <= q50:
return 1
elif val <= q75:
return 2
else:
return 3
states = [interval_to_state(i) for i in intervals]
n_states = 4
transition_matrix = np.zeros((n_states, n_states))
for i in range(len(states) - 1):
from_state = states[i]
to_state = states[i + 1]
transition_matrix[from_state, to_state] += 1
row_sums = transition_matrix.sum(axis=1, keepdims=True)
row_sums[row_sums == 0] = 1
transition_probs = transition_matrix / row_sums
max_probs = transition_probs.max(axis=1)
markov_score = float(np.mean(max_probs) * 100)

```
if render:
  st.subheader("ğŸ”— 17. Cadeias de Markov")
  state_labels = ['Muito Curto', 'Curto', 'Normal', 'Longo']
  fig = go.Figure(data=go.Heatmap(z=transition_probs, x=state_labels, y=state_labels, text=np.round(transition_probs, 2), texttemplate='%{text:.2f}', colorscale='Blues'))
  fig.update_layout(title="Matriz de TransiÃ§Ã£o", xaxis_title="Estado Seguinte", yaxis_title="Estado Atual", height=400)
  st.plotly_chart(fig, use_container_width=True, key=f'markov_matrix_{self.alert_id}')
  st.metric("Score Markoviano", f"{markov_score:.1f}%")
  if markov_score > 60:
    st.success("âœ… Forte padrÃ£o markoviano")
  elif markov_score > 30:
    st.info("ğŸ“Š PadrÃ£o moderado")
  else:
    st.warning("âš ï¸ PadrÃ£o fraco")

return {'markov_score': markov_score}
```

def _advanced_randomness_tests(self, intervals, render=True):
if len(intervals) < 10:
if render:
st.subheader(â€œğŸ² 18. Testes de Aleatoriedadeâ€)
st.info(â€œMÃ­nimo de 10 intervalos necessÃ¡rioâ€)
return {â€˜overall_randomness_scoreâ€™: 50}

```
if render:
  st.subheader("ğŸ² 18. Testes de Aleatoriedade")
  st.write("**1ï¸âƒ£ Runs Test**")
median = np.median(intervals)
runs = np.diff(intervals > median).sum() + 1
expected_runs = len(intervals) / 2
if render:
  col1, col2 = st.columns(2)
  col1.metric("Runs Observados", int(runs))
  col2.metric("Runs Esperados", f"{expected_runs:.1f}")

def permutation_entropy(series, order=3):
  n = len(series)
  permutations = []
  for i in range(n - order + 1):
    pattern = series[i:i+order]
    sorted_idx = np.argsort(pattern)
    perm = tuple(sorted_idx)
    permutations.append(perm)
  perm_counts = Counter(permutations)
  probs = np.array(list(perm_counts.values())) / len(permutations) if len(permutations) > 0 else np.array([1.0])
  entropy = -np.sum(probs * np.log2(probs))
  max_entropy = np.log2(math.factorial(order))
  return entropy / max_entropy if max_entropy > 0 else 0

perm_entropy = permutation_entropy(intervals)
complexity = float(perm_entropy * 100)
if render:
  st.write("**2ï¸âƒ£ Permutation Entropy**")
  col1, col2 = st.columns(2)
  col1.metric("Entropia", f"{perm_entropy:.3f}")
  col2.metric("Complexidade", f"{complexity:.1f}%")
  if complexity > 70:
    st.success("âœ… Alta complexidade")
  else:
    st.warning("âš ï¸ Baixa complexidade")

def hurst_exponent(series):
  n = len(series)
  if n < 20:
    return None
  lags = range(2, min(n//2, 20))
  tau = []
  for lag in lags:
    n_partitions = n // lag
    partitions = [series[i*lag:(i+1)*lag] for i in range(n_partitions)]
    rs_values = []
    for partition in partitions:
      if len(partition) == 0:
        continue
      mean = np.mean(partition)
      cumsum = np.cumsum(partition - mean)
      R = np.max(cumsum) - np.min(cumsum)
      S = np.std(partition)
      if S > 0:
        rs_values.append(R / S)
    if rs_values:
      tau.append(np.mean(rs_values))
  if len(tau) > 2:
    log_lags = np.log(list(lags[:len(tau)]))
    log_tau = np.log(tau)
    hurst = np.polyfit(log_lags, log_tau, 1)[0]
    return hurst
  return None

hurst = hurst_exponent(intervals) if len(intervals) >= 20 else None
if hurst is not None and render:
  st.write("**3ï¸âƒ£ Hurst Exponent**")
  st.metric("Hurst", f"{hurst:.3f}")
  if hurst < 0.45:
    st.info("ğŸ“‰ Anti-persistente")
  elif hurst > 0.55:
    st.warning("ğŸ“ˆ Persistente")
  else:
    st.success("ğŸ² Random Walk")

randomness_score = 50
if render:
  st.markdown("---")
  st.metric("Score de Aleatoriedade", f"{randomness_score:.0f}%")
  if randomness_score >= 60:
    st.success("âœ… Comportamento aleatÃ³rio")
  elif randomness_score >= 40:
    st.info("ğŸ“Š Comportamento misto")
  else:
    st.warning("âš ï¸ Comportamento determinÃ­stico")

return {'overall_randomness_score': randomness_score, 'hurst': hurst, 'perm_entropy': perm_entropy}
```

def _calculate_final_score_validated(self, results, df, intervals):
regularity_score = results[â€˜regularityâ€™][â€˜regularity_scoreâ€™] * 0.25

```
if results['periodicity'].get('has_strong_periodicity', False):
  periodicity_score = 100 * 0.25
elif results['periodicity'].get('has_moderate_periodicity', False):
  periodicity_score = 50 * 0.25
else:
  periodicity_score = 0 * 0.25

predictability_score = results['predictability']['predictability_score'] * 0.15

hourly_conc = results['temporal']['hourly_concentration']
daily_conc = results['temporal']['daily_concentration']
concentration_score = 0
if hourly_conc > 60 or daily_conc > 60:
  concentration_score = 100 * 0.20
elif hourly_conc > 40 or daily_conc > 40:
  concentration_score = 60 * 0.20
elif hourly_conc > 30 or daily_conc > 30:
  concentration_score = 30 * 0.20

total_occurrences = len(df)
period_days = (df['created_on'].max() - df['created_on'].min()).days + 1
freq_per_week = (total_occurrences / period_days * 7) if period_days > 0 else 0

if freq_per_week >= 3:
  frequency_score = 100 * 0.15
elif freq_per_week >= 1:
  frequency_score = 70 * 0.15
elif freq_per_week >= 0.5:
  frequency_score = 40 * 0.15
elif total_occurrences >= 10:
  frequency_score = 30 * 0.15
else:
  frequency_score = 10 * 0.15

final_score = (regularity_score + periodicity_score + predictability_score + concentration_score + frequency_score)

if final_score >= 70 and total_occurrences >= 10:
  classification = "ğŸ”´ REINCIDENTE CRÃTICO (R1)"
elif final_score >= 50 and total_occurrences >= 5:
  classification = "ğŸŸ  PARCIALMENTE REINCIDENTE (R2)"
elif final_score >= 35:
  classification = "ğŸŸ¡ PADRÃƒO DETECTÃVEL (R3)"
else:
  classification = "ğŸŸ¢ NÃƒO REINCIDENTE (R4)"

return round(float(final_score), 2), classification
```

def _final_classification(self, results, df, intervals):
st.markdown(â€â€”â€)
st.header(â€œğŸ¯ CLASSIFICAÃ‡ÃƒO FINALâ€)
final_score, classification = self._calculate_final_score_validated(results, df, intervals)

```
if final_score >= 70:
  level, color, priority, = "CRÃTICO", "red", "R1"
elif final_score >= 50:
  level, color, priority, = "ALTO", "orange", "R2"
elif final_score >= 35:
  level, color, priority, = "MÃ‰DIO", "yellow", "R3" 
else:
  level, color, priority, = "BAIXO", "green", "R4"

col1, col2 = st.columns([2, 1])
with col1:
  st.markdown(f"### {classification}")
  st.markdown(f"**NÃ­vel:** {level} | **Prioridade:** {priority}")
  st.metric("Score de ReincidÃªncia", f"{final_score:.0f}/100")
  st.markdown("#### ğŸ“Š Breakdown dos CritÃ©rios VALIDADOS")
  total_occurrences = len(df)
  period_days = (df['created_on'].max() - df['created_on'].min()).days + 1
  freq_per_week = (total_occurrences / period_days * 7) if period_days > 0 else 0
  regularity_pts = results['regularity']['regularity_score'] * 0.25
  periodicity_pts = (100 * 0.25) if results['periodicity'].get('has_strong_periodicity', False) else 0
  predictability_pts = results['predictability']['predictability_score'] * 0.15
  hourly_conc = results['temporal']['hourly_concentration']
  daily_conc = results['temporal']['daily_concentration']
  if hourly_conc > 60 or daily_conc > 60:
    concentration_pts = 100 * 0.20
  elif hourly_conc > 40 or daily_conc > 40:
    concentration_pts = 60 * 0.20
  else:
    concentration_pts = 30 * 0.20 if (hourly_conc > 30 or daily_conc > 30) else 0
  if freq_per_week >= 3:
    frequency_pts = 100 * 0.15
  elif freq_per_week >= 1:
    frequency_pts = 70 * 0.15
  else:
    frequency_pts = 40 * 0.15 if freq_per_week >= 0.5 else 10 * 0.15

  breakdown = {
    '1. Regularidade (25%)': regularity_pts,
    '2. Periodicidade (25%)': periodicity_pts,
    '3. Previsibilidade (15%)': predictability_pts,
    '4. ConcentraÃ§Ã£o Temporal (20%)': concentration_pts,
    '5. FrequÃªncia Absoluta (15%)': frequency_pts,
  }

  for criterion, points in breakdown.items():
    st.write(f"â€¢ {criterion}: **{points:.1f} pts**")

with col2:
  fig = go.Figure(go.Indicator(mode="gauge+number", value=final_score, title={'text': "Score Final"}, gauge={'axis': {'range': [0, 100]}, 'bar': {'color': color}}))
  fig.update_layout(height=300)
  st.plotly_chart(fig, use_container_width=True, key=f'final_gauge_{self.alert_id}')

st.markdown("---")
export_data = {
  'u_alert_id': self.alert_id,
  'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
  'score': final_score,
  'classificacao': classification,
  'nivel': level,
  'prioridade': priority,
  'total_occurrences': len(df),
  'freq_per_week': freq_per_week,
  'cv': results['basic_stats']['cv'],
  'regularidade': results['regularity']['regularity_score'],
  'periodicidade': results['periodicity'].get('has_strong_periodicity', False),
  'previsibilidade': results['predictability']['predictability_score'],
  'concentracao_horaria': results['temporal']['hourly_concentration'],
  'concentracao_diaria': results['temporal']['daily_concentration'],
  'bursts_detected': results['bursts']['has_bursts'],
  'n_bursts': results['bursts']['n_bursts'],
}
export_df = pd.DataFrame([export_data])
csv = export_df.to_csv(index=False)
st.download_button("â¬‡ï¸ Exportar RelatÃ³rio Completo", csv, f"reincidencia_{self.alert_id}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv", "text/csv", use_container_width=True)
```

# ============================================================

# StreamlitAlertAnalyzer: UI glue + batch processing

# ============================================================

class StreamlitAlertAnalyzer:
def **init**(self):
self.df_original = None
self.df = None
self.dates = None
self.alert_id = None

def load_data(self, uploaded_file):
try:
df_raw = pd.read_csv(uploaded_file)
st.success(fâ€âœ… Arquivo carregado: {len(df_raw)} registrosâ€)
with st.expander(â€œğŸ“‹ Previewâ€):
st.write(fâ€**Colunas:** {list(df_raw.columns)}â€)
st.dataframe(df_raw.head())
if â€˜created_onâ€™ not in df_raw.columns or â€˜u_alert_idâ€™ not in df_raw.columns:
st.error(â€œâŒ Colunas obrigatÃ³rias: â€˜created_onâ€™ e â€˜u_alert_idâ€™â€)
return False
df_raw[â€˜created_onâ€™] = pd.to_datetime(df_raw[â€˜created_onâ€™])
df_raw = df_raw.dropna(subset=[â€˜created_onâ€™]).sort_values([â€˜u_alert_idâ€™, â€˜created_onâ€™]).reset_index(drop=True)
self.df_original = df_raw
st.sidebar.write(fâ€**IDs:** {len(df_raw[â€˜u_alert_idâ€™].unique())}â€)
return True
except Exception as e:
st.error(fâ€âŒ Erro: {e}â€)
return False

def prepare_individual_analysis(self, alert_id):
df_filtered = self.df_original[self.df_original[â€˜u_alert_idâ€™] == alert_id].copy()
if len(df_filtered) == 0:
return False
df_filtered[â€˜dateâ€™] = df_filtered[â€˜created_onâ€™].dt.date
df_filtered[â€˜hourâ€™] = df_filtered[â€˜created_onâ€™].dt.hour
df_filtered[â€˜day_of_weekâ€™] = df_filtered[â€˜created_onâ€™].dt.dayofweek
df_filtered[â€˜day_nameâ€™] = df_filtered[â€˜created_onâ€™].dt.day_name()
df_filtered[â€˜is_weekendâ€™] = df_filtered[â€˜day_of_weekâ€™].isin([5, 6])
df_filtered[â€˜is_business_hoursâ€™] = (df_filtered[â€˜hourâ€™] >= 9) & (df_filtered[â€˜hourâ€™] <= 17)
df_filtered[â€˜time_diff_hoursâ€™] = df_filtered[â€˜created_onâ€™].diff().dt.total_seconds() / 3600
self.df = df_filtered
self.dates = df_filtered[â€˜created_onâ€™]
self.alert_id = alert_id
return True

def complete_analysis_all_u_alert_id(self, progress_bar=None):
try:
if self.df_original is None or len(self.df_original) == 0:
st.error(â€œâŒ Dados nÃ£o carregadosâ€)
return None
u_alert_id_list = list(self.df_original[â€˜u_alert_idâ€™].unique())
total = len(u_alert_id_list)
use_mp = total > 20
if use_mp:
n_processes = min(cpu_count(), total, 8)
st.info(fâ€ğŸš€ Usando {n_processes} processos para {total} alertasâ€)
chunk_size = max(1, total // n_processes)
chunks = [u_alert_id_list[i:i + chunk_size] for i in range(0, total, chunk_size)]
process_func = partial(analyze_chunk_recurrence, df_original=self.df_original)
try:
all_results = []
with Pool(processes=n_processes) as pool:
for idx, chunk_results in enumerate(pool.imap(process_func, chunks)):
all_results.extend(chunk_results)
if progress_bar:
progress = (len(all_results) / total)
progress_bar.progress(progress, text=fâ€{len(all_results)}/{total}â€)
df_results = pd.DataFrame(all_results)
if progress_bar:
progress_bar.progress(1.0, text=â€œâœ… Completa!â€)
return df_results
except Exception as e:
st.warning(fâ€âš ï¸ Erro no multiprocessing: {e}. Usando modo sequencialâ€¦â€)
use_mp = False
if not use_mp:
all_results = []
for idx, u_alert_id in enumerate(u_alert_id_list):
if progress_bar:
progress_bar.progress((idx + 1) / total, text=fâ€{idx + 1}/{total}â€)
result = analyze_single_u_alert_id_recurrence(u_alert_id, self.df_original)
if result:
all_results.append(result)
return pd.DataFrame(all_results)
except Exception as e:
st.error(fâ€Erro: {e}â€)
import traceback
st.error(traceback.format_exc())
return None

def show_basic_stats(self):
st.header(â€œğŸ“Š EstatÃ­sticas BÃ¡sicasâ€)
total = len(self.df)
period_days = (self.dates.max() - self.dates.min()).days + 1
avg_per_day = total / period_days if period_days > 0 else 0
unique_days = self.df[â€˜dateâ€™].nunique()
col1, col2, col3, col4, col5 = st.columns(5)
col1.metric(â€œğŸ”¥ Totalâ€, total)
col2.metric(â€œğŸ“… PerÃ­odoâ€, period_days)
col3.metric(â€œğŸ“† Dias Ãšnicosâ€, unique_days)
col4.metric(â€œğŸ“ˆ MÃ©dia/diaâ€, fâ€{avg_per_day:.2f}â€)
col5.metric(â€œğŸ• Ãšltimoâ€, self.dates.max().strftime(â€%d/%m %H:%Mâ€))
if unique_days == 1:
st.warning(â€œâš ï¸ Todos em 1 dia - Pode nÃ£o ser reincidenteâ€)
st.markdown(â€â€”â€)
st.subheader(â€œğŸ“Š FrequÃªnciasâ€)
total_hours = period_days * 24
avg_per_hour = total / total_hours if total_hours > 0 else 0
avg_per_week = total / (period_days / 7) if period_days > 0 else 0
avg_per_month = total / (period_days / 30.44) if period_days > 0 else 0
col1, col2, col3, col4 = st.columns(4)
col1.metric(â€œPor Diaâ€, fâ€{avg_per_day:.2f}â€)
col2.metric(â€œPor Horaâ€, fâ€{avg_per_hour:.4f}â€)
col3.metric(â€œPor Semanaâ€, fâ€{avg_per_week:.2f}â€)
col4.metric(â€œPor MÃªsâ€, fâ€{avg_per_month:.2f}â€)
intervals = self.df[â€˜time_diff_hoursâ€™].dropna()
if len(intervals) > 0:
st.markdown(â€â€”â€)
st.subheader(â€œâ±ï¸ Intervalosâ€)
col1, col2, col3, col4 = st.columns(4)
col1.metric(â€œMÃ©dia (h)â€, fâ€{intervals.mean():.2f}â€)
col2.metric(â€œMediana (h)â€, fâ€{intervals.median():.2f}â€)
col3.metric(â€œMÃ­nimo (h)â€, fâ€{intervals.min():.2f}â€)
col4.metric(â€œMÃ¡ximo (h)â€, fâ€{intervals.max():.2f}â€)

# ============================================================

# FUNÃ‡Ã•ES PARA COMPARAÃ‡ÃƒO COM ATHENA

# ============================================================

def load_athena_data_with_cache(storage):
â€œâ€â€œCarrega dados do Athena com suporte a cacheâ€â€â€
st.subheader(â€œğŸ“Š Dados do Athenaâ€)

```
# Verificar cache
cached = storage.has_cached_data()

if cached.get('athena', False):
    st.info("ğŸ“¦ Dados do Athena encontrados em cache")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("âœ… Usar dados salvos do Athena", type="primary", use_container_width=True):
            cache_data = storage.load_athena_results()
            if cache_data:
                df_athena, timestamp = cache_data
                st.success(f"âœ… {len(df_athena)} registros carregados do cache")
                with st.expander("ğŸ‘ï¸ Preview dos dados do Athena"):
                    st.dataframe(df_athena.head(10))
                return df_athena
    
    with col2:
        if st.button("ğŸ”„ Buscar novos dados do Athena", use_container_width=True):
            pass  # Continua para buscar novos
        else:
            st.stop()

# Carregar dados novos do Athena
st.info("ğŸ”„ Buscando dados do Athena...")

try:
    with st.spinner("Consultando Athena..."):
        df_athena = process_query_external()
    
    if df_athena is not None and len(df_athena) > 0:
        st.success(f"âœ… {len(df_athena)} registros carregados do Athena")
        
        # Salvar em cache
        storage.save_athena_results(df_athena)
        st.info("ğŸ’¾ Dados salvos em cache")
        
        # Preview
        with st.expander("ğŸ‘ï¸ Preview dos dados do Athena"):
            st.dataframe(df_athena.head(10))
        
        return df_athena
    else:
        st.error("âŒ Nenhum dado retornado do Athena")
        return None
        
except Exception as e:
    st.error(f"âŒ Erro ao buscar dados do Athena: {e}")
    st.code(str(e))
    return None
```

def compare_with_athena(df_pipeline, storage):
â€œâ€â€œCompara resultados da pipeline com dados do Athenaâ€â€â€
st.markdown(â€â€”â€)
st.header(â€œğŸ”„ ComparaÃ§Ã£o: Pipeline vs Athenaâ€)

```
# Carregar dados do Athena
df_athena = load_athena_data_with_cache(storage)

if df_athena is None:
    st.warning("âš ï¸ NÃ£o foi possÃ­vel carregar dados do Athena para comparaÃ§Ã£o")
    return

# Realizar comparaÃ§Ã£o
st.info("ğŸ”„ Realizando comparaÃ§Ã£o...")

try:
    comparator = ResultsComparator()
    comparator.load_athena_results(df_athena)
    comparator.load_pipeline_results(df_pipeline)
    
    comparison_report = comparator.compare()
    
    # Salvar comparaÃ§Ã£o em cache
    storage.save_comparison_results(comparator.export_to_dict())
    
    # Mostrar resultados
    st.markdown("---")
    
    # Resumo em cards
    metrics = comparison_report['metrics']
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ“Š Total de Alertas", metrics['total_alertas'])
    
    with col2:
        st.metric(
            "âœ… ConcordÃ¢ncia",
            f"{metrics['agreement_rate']*100:.1f}%",
            delta=f"{metrics['concordancias']} alertas"
        )
    
    with col3:
        athena_rate = (metrics['athena_reincidentes'] / metrics['total_alertas'] * 100) if metrics['total_alertas'] > 0 else 0
        st.metric(
            "ğŸ”´ Reincidentes (Athena)",
            metrics['athena_reincidentes'],
            delta=f"{athena_rate:.1f}%"
        )
    
    with col4:
        pipeline_rate = (metrics['pipeline_reincidentes'] / metrics['total_alertas'] * 100) if metrics['total_alertas'] > 0 else 0
        st.metric(
            "ğŸŸ  Reincidentes (Pipeline)",
            metrics['pipeline_reincidentes'],
            delta=f"{pipeline_rate:.1f}%"
        )
    
    # Matriz de ConfusÃ£o Visual
    st.subheader("ğŸ“Š Matriz de ConfusÃ£o")
    
    cm = metrics['confusion_matrix']
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        fig = go.Figure(data=go.Heatmap(
            z=[[cm['true_negatives'], cm['false_positives']],
               [cm['false_negatives'], cm['true_positives']]],
            x=['NÃ£o Rein. (Pipeline)', 'Rein. (Pipeline)'],
            y=['NÃ£o Rein. (Athena)', 'Rein. (Athena)'],
            text=[[cm['true_negatives'], cm['false_positives']],
                  [cm['false_negatives'], cm['true_positives']]],
            texttemplate='%{text}',
            colorscale='RdYlGn',
            reversescale=True
        ))
        fig.update_layout(title="Matriz de ConfusÃ£o", height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("### MÃ©tricas")
        st.metric("Accuracy", f"{metrics['metrics']['accuracy']*100:.2f}%")
        st.metric("Precision", f"{metrics['metrics']['precision']*100:.2f}%")
        st.metric("Recall", f"{metrics['metrics']['recall']*100:.2f}%")
        st.metric("F1-Score", f"{metrics['metrics']['f1_score']*100:.2f}%")
    
    # Tabela detalhada de comparaÃ§Ã£o
    st.subheader("ğŸ“‹ Detalhamento por Alerta")
    
    comparison_df = comparison_report['comparison_df']
    
    # Filtros
    col1, col2 = st.columns(2)
    with col1:
        filter_type = st.selectbox(
            "Filtrar por:",
            ["Todos", "Concordam", "Divergem", "Athena: Reincidente", "Pipeline: Reincidente"]
        )
    
    # Aplicar filtros
    if filter_type == "Concordam":
        filtered_df = comparison_df[comparison_df['match_type'].str.contains('Concordam')]
    elif filter_type == "Divergem":
        filtered_df = comparison_df[comparison_df['match_type'].str.contains('Divergem')]
    elif filter_type == "Athena: Reincidente":
        filtered_df = comparison_df[comparison_df['is_reincidente_athena'] == True]
    elif filter_type == "Pipeline: Reincidente":
        filtered_df = comparison_df[comparison_df['is_reincidente_pipeline'] == True]
    else:
        filtered_df = comparison_df
    
    st.dataframe(filtered_df.sort_values('match_type'), use_container_width=True, height=400)
    
    # Resumo textual
    st.markdown("---")
    st.text(comparator.get_summary())
    
    # Exportar resultados
    st.markdown("---")
    st.subheader("ğŸ“¥ Exportar Resultados da ComparaÃ§Ã£o")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        csv_comparison = filtered_df.to_csv(index=False)
        st.download_button(
            "â¬‡ï¸ ComparaÃ§Ã£o (CSV)",
            csv_comparison,
            f"comparacao_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            "text/csv",
            use_container_width=True
        )
    
    with col2:
        import json
        json_comparison = json.dumps(comparator.export_to_dict(), indent=2, default=str)
        st.download_button(
            "â¬‡ï¸ Completo (JSON)",
            json_comparison,
            f"comparacao_completa_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
            "application/json",
            use_container_width=True
        )
    
    with col3:
        divergencias = comparison_df[comparison_df['match_type'].str.contains('Divergem')]
        csv_div = divergencias.to_csv(index=False)
        st.download_button(
            "â¬‡ï¸ DivergÃªncias (CSV)",
            csv_div,
            f"divergencias_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            "text/csv",
            use_container_width=True
        )
    
except Exception as e:
    st.error(f"âŒ Erro na comparaÃ§Ã£o: {e}")
    import traceback
    st.code(traceback.format_exc())
```

# ============================================================

# MAIN MODIFICADO

# ============================================================

def main():
st.title(â€œğŸš¨ Analisador de Alertasâ€)
st.markdown(â€### 2 modos: Individual e Completa + CSV (com comparaÃ§Ã£o Athena)â€)

st.sidebar.header(â€œâš™ï¸ ConfiguraÃ§Ãµesâ€)

# Inicializar storage

if ATHENA_AVAILABLE:
if â€˜data_storageâ€™ not in st.session_state:
st.session_state.data_storage = DataStorage()

```
  storage = st.session_state.data_storage
  
  # Mostrar status do cache na sidebar
  cached = storage.has_cached_data()
  metadata = storage.get_metadata()
  
  with st.sidebar.expander("ğŸ’¾ Status do Cache"):
      for data_type, exists in cached.items():
          if exists:
              meta = metadata.get(data_type, {})
              last_update = meta.get('last_updated', 'Desconhecido')
              if last_update != 'Desconhecido':
                  try:
                      dt = datetime.fromisoformat(last_update)
                      last_update = dt.strftime('%d/%m/%Y %H:%M')
                  except:
                      pass
              
              st.success(f"âœ… {data_type.title()}")
              st.caption(f"Atualizado: {last_update}")
          else:
              st.info(f"âšª {data_type.title()}: NÃ£o disponÃ­vel")
      
      st.markdown("---")
      
      if st.button("ğŸ—‘ï¸ Limpar Cache", use_container_width=True):
          storage.clear_cache()
          st.success("Cache limpo!")
          st.rerun()
```

analysis_mode = st.sidebar.selectbox(
â€œğŸ¯ Modo de AnÃ¡liseâ€,
[â€œğŸ” Individualâ€, â€œğŸ“Š Completa + CSVâ€]
)

uploaded_file = st.sidebar.file_uploader(â€œğŸ“ Upload CSVâ€, type=[â€˜csvâ€™])

if uploaded_file:
analyzer = StreamlitAlertAnalyzer()

```
if analyzer.load_data(uploaded_file):
  if analysis_mode == "ğŸ” Individual":
    id_counts = analyzer.df_original['u_alert_id'].value_counts()
    id_options = [f"{uid} ({count})" for uid, count in id_counts.items()]
    selected = st.sidebar.selectbox("Short CI", id_options)
    selected_id = selected.split(" (")[0]
    
    if st.sidebar.button("ğŸš€ Analisar", type="primary"):
      if analyzer.prepare_individual_analysis(selected_id):
        st.success(f"Analisando: {selected_id}")
        tab1, tab2 = st.tabs(["ğŸ“Š BÃ¡sico", "ğŸ”„ ReincidÃªncia"])
        with tab1:
          analyzer.show_basic_stats()
        with tab2:
          recurrence_analyzer = AdvancedRecurrenceAnalyzer(analyzer.df, selected_id)
          recurrence_analyzer.analyze()

  elif analysis_mode == "ğŸ“Š Completa + CSV":
    st.subheader("ğŸ“Š AnÃ¡lise Completa")
    
    df_consolidated = None
    
    # Verificar se hÃ¡ dados em cache da pipeline
    if ATHENA_AVAILABLE:
        storage = st.session_state.data_storage
        cached = storage.has_cached_data()
        
        if cached.get('pipeline', False):
            st.info("ğŸ“¦ AnÃ¡lise anterior encontrada em cache")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("âœ… Usar dados salvos da Pipeline", type="primary", use_container_width=True):
                    cache_data = storage.load_pipeline_results()
                    if cache_data:
                        df_consolidated, timestamp, summary = cache_data
                        st.success(f"âœ… Carregado do cache: {summary['total']} alertas")
            
            with col2:
                if st.button("ğŸ”„ Nova anÃ¡lise da Pipeline", use_container_width=True):
                    pass  # Continua para nova anÃ¡lise
                else:
                    st.stop()
    
    # Se nÃ£o usou cache, executar nova anÃ¡lise
    if df_consolidated is None:
        if st.sidebar.button("ğŸš€ Executar", type="primary"):
            st.info("â±ï¸ Processando pipeline...")
            progress_bar = st.progress(0)
            df_consolidated = analyzer.complete_analysis_all_u_alert_id(progress_bar)
            progress_bar.empty()
            
            if df_consolidated is not None and len(df_consolidated) > 0:
                # Salvar em cache
                if ATHENA_AVAILABLE:
                    storage.save_pipeline_results(df_consolidated)
                    st.success("ğŸ’¾ Resultados da pipeline salvos em cache")
    
    # Mostrar resultados da pipeline
    if df_consolidated is not None and len(df_consolidated) > 0:
        st.success(f"âœ… {len(df_consolidated)} alertas processados!")
        
        st.header("ğŸ“Š Resumo da Pipeline")
        col1, col2, col3, col4 = st.columns(4)
        critical = len(df_consolidated[df_consolidated['classification'].str.contains('CRÃTICO', na=False)])
        col1.metric("ğŸ”´ R1", critical)
        high = len(df_consolidated[df_consolidated['classification'].str.contains('PARCIALMENTE', na=False)])
        col2.metric("ğŸŸ  R2", high)
        medium = len(df_consolidated[df_consolidated['classification'].str.contains('DETECTÃVEL', na=False)])
        col3.metric("ğŸŸ¡ R3", medium)
        low = len(df_consolidated[df_consolidated['classification'].str.contains('NÃƒO', na=False)])
        col4.metric("ğŸŸ¢ R4", low)
        
        st.subheader("Dataframe Completo")
        st.dataframe(df_consolidated, use_container_width=True, height=400)
        
        st.markdown("---")
        st.subheader("ğŸ“¥ Exportar Resultados da Pipeline")
        col1, col2 = st.columns(2)
        csv_full = df_consolidated.to_csv(index=False)
        col1.download_button(
            "â¬‡ï¸ CSV Completo",
            csv_full,
            f"completo_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            "text/csv",
            use_container_width=True
        )
        
        summary_cols = ['u_alert_id', 'score', 'classification', 'total_occurrences']
        available_summary = [col for col in summary_cols if col in df_consolidated.columns]
        summary = df_consolidated[available_summary].copy()
        csv_summary = summary.to_csv(index=False)
        col2.download_button(
            "â¬‡ï¸ CSV Resumido",
            csv_summary,
            f"resumo_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            "text/csv",
            use_container_width=True
        )
        
        # COMPARAÃ‡ÃƒO COM ATHENA
        if ATHENA_AVAILABLE:
            st.markdown("---")
            if st.button("ğŸ“Š Comparar com Athena", type="primary", use_container_width=True):
                compare_with_athena(df_consolidated, storage)
        else:
            st.warning("âš ï¸ IntegraÃ§Ã£o com Athena nÃ£o disponÃ­vel. Certifique-se de que os arquivos compare_results.py, data_storage.py e src/athena/teste.py estÃ£o presentes.")
```

else:
st.info(â€œğŸ‘† FaÃ§a upload de um CSVâ€)
with st.expander(â€œğŸ“– InstruÃ§Ãµesâ€):
st.markdown(â€â€â€
### âœ… CRITÃ‰RIOS VALIDADOS

```
  1. **Regularidade (25%)** - ConsistÃªncia via CV
  2. **Periodicidade (25%)** - Detecta ciclos via FFT
  3. **Previsibilidade (15%)** - Indica se podemos prever
  4. **ConcentraÃ§Ã£o Temporal (20%)** - HorÃ¡rios/dias fixos
  5. **FrequÃªncia Absoluta (15%)** - Volume mÃ­nimo necessÃ¡rio
  
  ### ğŸ†• Novo: ComparaÃ§Ã£o com Athena
  
  No modo "Completa + CSV", apÃ³s processar os alertas:
  - Clique em **"Comparar com Athena"**
  - Sistema busca dados do Athena (ou usa cache)
  - Compara classificaÃ§Ãµes e mostra mÃ©tricas
  - Identifica concordÃ¢ncias e divergÃªncias
  
  ### ğŸ’¾ Sistema de Cache
  
  - Dados salvos automaticamente
  - Reutilize anÃ¡lises anteriores
  - Cache persiste entre sessÃµes
  - Gerencie via sidebar
  """)
```

if **name** == â€œ**main**â€:
main()