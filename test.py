import pandas as pd
import streamlit as st
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import io
import warnings
from multiprocessing import Pool, cpu_count
from functools import partial

warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="Analisador de Alertas - Completo",
    page_icon="üö®",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# FUN√á√ïES AUXILIARES DE AGRUPAMENTO
# ============================================================

def identify_alert_groups(alert_data, max_gap_hours=24, min_group_size=3, 
                         spike_threshold_multiplier=5):
    """
    Identifica grupos/sess√µes de alertas baseado em intervalos de tempo.
    Alertas isolados s√£o aqueles que n√£o pertencem a nenhum grupo significativo.
    """
    if len(alert_data) == 0:
        return alert_data, []
    
    alert_data = alert_data.sort_values('created_on').reset_index(drop=True)
    alert_data['time_diff_hours'] = alert_data['created_on'].diff().dt.total_seconds() / 3600
    alert_data['date'] = alert_data['created_on'].dt.date
    unique_dates = alert_data['date'].nunique()
    
    if unique_dates == 1:
        alert_data['group_id'] = -1
        alert_data['is_isolated'] = True
        return alert_data, []
    
    alert_data['group_id'] = -1
    current_group = 0
    group_start_idx = 0
    
    for i in range(len(alert_data)):
        if i == 0:
            continue
            
        gap = alert_data.loc[i, 'time_diff_hours']
        
        if gap > max_gap_hours:
            group_size = i - group_start_idx
            if group_size >= min_group_size:
                alert_data.loc[group_start_idx:i-1, 'group_id'] = current_group
                current_group += 1
            group_start_idx = i
    
    group_size = len(alert_data) - group_start_idx
    if group_size >= min_group_size:
        alert_data.loc[group_start_idx:, 'group_id'] = current_group
    
    daily_counts = alert_data.groupby('date').size()
    avg_daily = daily_counts.mean()
    spike_threshold = avg_daily * spike_threshold_multiplier
    spike_dates = daily_counts[daily_counts > spike_threshold].index
    
    if len(spike_dates) > 0:
        alert_data.loc[alert_data['date'].isin(spike_dates), 'group_id'] = -1
    
    alert_data['is_isolated'] = alert_data['group_id'] == -1
    
    groups_info = []
    for group_id in alert_data[alert_data['group_id'] >= 0]['group_id'].unique():
        group_data = alert_data[alert_data['group_id'] == group_id]
        groups_info.append({
            'group_id': int(group_id),
            'size': len(group_data),
            'start_time': group_data['created_on'].min(),
            'end_time': group_data['created_on'].max(),
            'duration_hours': (group_data['created_on'].max() - 
                             group_data['created_on'].min()).total_seconds() / 3600
        })
    
    return alert_data, groups_info


def classify_alert_pattern(alert_data, max_gap_hours=24, min_group_size=3, 
                          spike_threshold_multiplier=5):
    """
    Classifica um alerta baseado na identifica√ß√£o de grupos.
    """
    n = len(alert_data)
    if n == 0:
        return {
            'pattern': 'isolated',
            'reason': 'Sem ocorr√™ncias',
            'occurrences': 0,
            'num_groups': 0,
            'isolated_occurrences': 0,
            'grouped_occurrences': 0,
            'groups_info': [],
            'unique_days': 0
        }
    
    unique_days = alert_data['created_on'].dt.date.nunique()
    
    if unique_days == 1:
        return {
            'pattern': 'isolated',
            'reason': f'Todos os {n} alertas ocorreram em um √∫nico dia',
            'occurrences': n,
            'num_groups': 0,
            'isolated_occurrences': n,
            'grouped_occurrences': 0,
            'groups_info': [],
            'unique_days': 1
        }
    
    alert_data_processed, groups_info = identify_alert_groups(
        alert_data, max_gap_hours, min_group_size, spike_threshold_multiplier
    )
    
    num_groups = len(groups_info)
    isolated_count = alert_data_processed['is_isolated'].sum()
    grouped_count = n - isolated_count
    isolated_pct = (isolated_count / n) * 100
    
    if num_groups == 0:
        pattern = 'isolated'
        reason = f'Nenhum grupo identificado ({n} ocorr√™ncias isoladas)'
    elif num_groups == 1 and isolated_pct > 50:
        pattern = 'isolated'
        reason = f'Apenas 1 grupo pequeno com {isolated_pct:.0f}% de alertas isolados'
    elif isolated_pct > 70:
        pattern = 'isolated'
        reason = f'{isolated_pct:.0f}% de alertas isolados ({isolated_count}/{n})'
    elif num_groups >= 2:
        pattern = 'continuous'
        reason = f'{num_groups} grupos cont√≠nuos identificados ({grouped_count} alertas agrupados)'
    elif num_groups == 1 and grouped_count >= min_group_size * 2:
        pattern = 'continuous'
        reason = f'1 grupo cont√≠nuo grande ({grouped_count} alertas)'
    else:
        pattern = 'isolated'
        reason = f'Padr√£o inconsistente: {num_groups} grupo(s), {isolated_pct:.0f}% isolados'
    
    return {
        'pattern': pattern,
        'reason': reason,
        'occurrences': n,
        'num_groups': num_groups,
        'isolated_occurrences': int(isolated_count),
        'grouped_occurrences': int(grouped_count),
        'groups_info': groups_info,
        'unique_days': unique_days
    }

# ============================================================
# FUN√á√ïES DE PROCESSAMENTO
# ============================================================

def process_single_alert(alert_id, df_original, max_gap_hours=24, min_group_size=3, 
                        spike_threshold_multiplier=5):
    try:
        df_alert = df_original[df_original['u_alert_id'] == alert_id].copy()
        if len(df_alert) < 1:
            return None
        
        pattern_info = classify_alert_pattern(df_alert, max_gap_hours, min_group_size, 
                                             spike_threshold_multiplier)
        
        df_alert['hour'] = df_alert['created_on'].dt.hour
        df_alert['day_of_week'] = df_alert['created_on'].dt.dayofweek
        df_alert['is_weekend'] = df_alert['day_of_week'].isin([5, 6])
        df_alert['is_business_hours'] = (df_alert['hour'] >= 9) & (df_alert['hour'] <= 17)
        df_alert = df_alert.sort_values('created_on')
        intervals_hours = df_alert['created_on'].diff().dt.total_seconds() / 3600
        intervals_hours = intervals_hours.dropna()
        
        period_days = (df_alert['created_on'].max() - df_alert['created_on'].min()).days + 1
        
        metrics = {
            'alert_id': alert_id,
            'pattern_type': pattern_info['pattern'],
            'pattern_reason': pattern_info['reason'],
            'total_ocorrencias': pattern_info['occurrences'],
            'num_grupos': pattern_info['num_groups'],
            'alertas_isolados': pattern_info['isolated_occurrences'],
            'alertas_agrupados': pattern_info['grouped_occurrences'],
            'pct_isolados': (pattern_info['isolated_occurrences'] / pattern_info['occurrences'] * 100) 
                           if pattern_info['occurrences'] > 0 else 0,
            'unique_days': pattern_info['unique_days'],
            'periodo_dias': period_days,
            'freq_dia': len(df_alert) / period_days if period_days > 0 else 0,
            'freq_semana': (len(df_alert) / period_days * 7) if period_days > 0 else 0,
            'freq_mes': (len(df_alert) / period_days * 30) if period_days > 0 else 0,
            'intervalo_medio_h': intervals_hours.mean() if len(intervals_hours) > 0 else None,
            'intervalo_mediano_h': intervals_hours.median() if len(intervals_hours) > 0 else None,
            'intervalo_std_h': intervals_hours.std() if len(intervals_hours) > 0 else None,
            'intervalo_min_h': intervals_hours.min() if len(intervals_hours) > 0 else None,
            'intervalo_max_h': intervals_hours.max() if len(intervals_hours) > 0 else None,
            'hora_pico': df_alert['hour'].mode().iloc[0] if len(df_alert['hour'].mode()) > 0 else 12,
            'pct_fins_semana': df_alert['is_weekend'].mean() * 100,
            'pct_horario_comercial': df_alert['is_business_hours'].mean() * 100,
            'variabilidade_intervalo': intervals_hours.std() / intervals_hours.mean() if len(intervals_hours) > 0 and intervals_hours.mean() > 0 else 0,
            'primeiro_alerta': df_alert['created_on'].min(),
            'ultimo_alerta': df_alert['created_on'].max()
        }
        return metrics
    except Exception:
        return None


def process_alert_chunk(alert_ids, df_original, max_gap_hours=24, min_group_size=3, 
                       spike_threshold_multiplier=5):
    return [metrics for alert_id in alert_ids 
            if (metrics := process_single_alert(alert_id, df_original, max_gap_hours, 
                                               min_group_size, spike_threshold_multiplier))]

# ============================================================
# CLASSE PRINCIPAL
# ============================================================

class StreamlitAlertAnalyzer:
    def __init__(self):
        self.df_original = None
        self.df_all_alerts = None
        self.df = None
        self.dates = None
        self.alert_id = None
        self.max_gap_hours = 24
        self.min_group_size = 3
        self.spike_threshold_multiplier = 5

    def load_data(self, uploaded_file):
        try:
            df_raw = pd.read_csv(uploaded_file)
            st.success(f"‚úÖ Arquivo carregado com {len(df_raw)} registros")
            with st.expander("üìã Informa√ß√µes do Dataset"):
                st.write(f"**Colunas:** {list(df_raw.columns)}")
                st.write(f"**Shape:** {df_raw.shape}")
                st.dataframe(df_raw.head())
            if 'created_on' not in df_raw.columns or 'u_alert_id' not in df_raw.columns:
                st.error("‚ùå Colunas 'created_on' e 'u_alert_id' s√£o obrigat√≥rias!")
                return False
            df_raw['created_on'] = pd.to_datetime(df_raw['created_on'])
            df_raw = df_raw.dropna(subset=['created_on'])
            df_raw = df_raw.sort_values(['u_alert_id', 'created_on']).reset_index(drop=True)
            self.df_original = df_raw
            st.sidebar.write(f"**IDs dispon√≠veis:** {len(df_raw['u_alert_id'].unique())}")
            return True
        except Exception as e:
            st.error(f"‚ùå Erro ao carregar dados: {e}")
            return False

    def prepare_individual_analysis(self, alert_id):
        df_filtered = self.df_original[self.df_original['u_alert_id'] == alert_id].copy()
        if len(df_filtered) == 0:
            return False

        df_filtered['date'] = df_filtered['created_on'].dt.date
        df_filtered['hour'] = df_filtered['created_on'].dt.hour
        df_filtered['day_of_week'] = df_filtered['created_on'].dt.dayofweek
        df_filtered['day_name'] = df_filtered['created_on'].dt.day_name()
        df_filtered['month'] = df_filtered['created_on'].dt.month
        df_filtered['month_name'] = df_filtered['created_on'].dt.month_name()
        df_filtered['is_weekend'] = df_filtered['day_of_week'].isin([5, 6])
        df_filtered['is_business_hours'] = (df_filtered['hour'] >= 9) & (df_filtered['hour'] <= 17)
        df_filtered['time_diff_hours'] = df_filtered['created_on'].diff().dt.total_seconds() / 3600
        df_filtered['time_diff_days'] = df_filtered['created_on'].diff().dt.total_seconds() / 86400

        df_filtered, groups_info = identify_alert_groups(
            df_filtered, 
            self.max_gap_hours, 
            self.min_group_size,
            self.spike_threshold_multiplier
        )

        self.df = df_filtered
        self.dates = df_filtered['created_on']
        self.alert_id = alert_id
        self.groups_info = groups_info
        return True

    def prepare_global_analysis(self, use_multiprocessing=True, max_gap_hours=24, 
                               min_group_size=3, spike_threshold_multiplier=5):
        st.header("üåç An√°lise Global de Todos os Alertas")
        self.max_gap_hours = max_gap_hours
        self.min_group_size = min_group_size
        self.spike_threshold_multiplier = spike_threshold_multiplier
        
        unique_ids = self.df_original['u_alert_id'].unique()
        total_ids = len(unique_ids)
        st.info(f"üìä Processando {total_ids} Alert IDs...")
        alert_metrics = []
        
        if use_multiprocessing:
            n_processes = min(cpu_count(), total_ids)
            st.write(f"üöÄ Usando {n_processes} processos paralelos")
            chunk_size = max(1, total_ids // n_processes)
            id_chunks = [unique_ids[i:i + chunk_size] for i in range(0, total_ids, chunk_size)]
            progress_bar = st.progress(0)
            status_text = st.empty()
            process_func = partial(process_alert_chunk, 
                                  df_original=self.df_original,
                                  max_gap_hours=max_gap_hours,
                                  min_group_size=min_group_size,
                                  spike_threshold_multiplier=spike_threshold_multiplier)
            try:
                with Pool(processes=n_processes) as pool:
                    results = pool.map(process_func, id_chunks)
                    for result in results:
                        alert_metrics.extend(result)
                    progress_bar.progress(1.0)
                    status_text.success(f"‚úÖ Processamento conclu√≠do! {len(alert_metrics)} alertas analisados")
            except Exception as e:
                st.error(f"‚ùå Erro no multiprocessing: {e}")
                st.warning("‚ö†Ô∏è Tentando processamento sequencial...")
                use_multiprocessing = False
                alert_metrics = []
        
        if not use_multiprocessing or len(alert_metrics) == 0:
            alert_metrics = []
            progress_bar = st.progress(0)
            for i, alert_id in enumerate(unique_ids):
                progress_bar.progress((i + 1) / total_ids)
                metrics = process_single_alert(alert_id, self.df_original, 
                                              max_gap_hours, min_group_size, 
                                              spike_threshold_multiplier)
                if metrics:
                    alert_metrics.append(metrics)
        
        if 'progress_bar' in locals():
            progress_bar.empty()
        
        self.df_all_alerts = pd.DataFrame(alert_metrics)
        
        isolated_count = len(self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'isolated'])
        continuous_count = len(self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous'])
        single_day_count = len(self.df_all_alerts[self.df_all_alerts['unique_days'] == 1])
        
        st.subheader("üìä Estat√≠sticas Globais")
        col1, col2, col3, col4, col5, col6, col7 = st.columns(7)
        with col1:
            st.metric("üî¢ Total de Alert IDs", len(unique_ids))
        with col2:
            st.metric("üìà IDs com Dados", len(self.df_all_alerts))
        with col3:
            st.metric("üö® Total de Alertas", self.df_original.shape[0])
        with col4:
            period_total = (self.df_original['created_on'].max() - self.df_original['created_on'].min()).days + 1
            st.metric("üìÖ Per√≠odo (dias)", period_total)
        with col5:
            st.metric("üî¥ Alertas Isolados", isolated_count)
        with col6:
            st.metric("üü¢ Alertas Cont√≠nuos", continuous_count)
        with col7:
            st.metric("üìÜ Alertas de 1 Dia", single_day_count)
        
        return len(self.df_all_alerts) > 0

    # ============================================================
    # AN√ÅLISE GLOBAL - ISOLADOS VS CONT√çNUOS
    # ============================================================

    def show_isolated_vs_continuous_analysis(self):
        st.header("üîç An√°lise de Alertas Isolados vs Cont√≠nuos (Baseado em Grupos)")

        self.df_all_alerts = self.df_all_alerts.drop_duplicates(subset=['alert_id'])

        df_isolated = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'isolated']
        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
        df_single_day = self.df_all_alerts[self.df_all_alerts['unique_days'] == 1]

        col1, col2 = st.columns(2)
        with col1:
            pattern_dist = self.df_all_alerts['pattern_type'].value_counts()
            fig_pie = px.pie(
                values=pattern_dist.values,
                names=pattern_dist.index,
                title="üìä Distribui√ß√£o de Padr√µes de Alerta",
                color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
            )
            st.plotly_chart(fig_pie, use_container_width=True, key='pattern_pie')

        with col2:
            st.subheader("üìà Compara√ß√£o de M√©tricas")
            comparison_data = pd.DataFrame({
                'M√©trica': ['Qtd Alertas', 'M√©dia Ocorr√™ncias', 'M√©dia Grupos', 
                            'M√©dia % Isolados', 'M√©dia Freq/Dia', 'Alertas 1 Dia'],
                'Isolados': [
                    len(df_isolated),
                    df_isolated['total_ocorrencias'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['num_grupos'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['pct_isolados'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['freq_dia'].mean() if len(df_isolated) > 0 else 0,
                    len(df_single_day)
                ],
                'Cont√≠nuos': [
                    len(df_continuous),
                    df_continuous['total_ocorrencias'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['num_grupos'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['pct_isolados'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['freq_dia'].mean() if len(df_continuous) > 0 else 0,
                    0
                ]
            })
            comparison_data = comparison_data.round(2)
            st.dataframe(comparison_data, use_container_width=True)

        st.subheader("üìà Evolu√ß√£o Temporal: Isolados vs Agrupados")
        
        df_with_dates = self.df_all_alerts.copy()
        df_with_dates['date'] = pd.to_datetime(df_with_dates['primeiro_alerta']).dt.date
        
        daily_isolated = df_isolated.copy()
        daily_isolated['date'] = pd.to_datetime(daily_isolated['primeiro_alerta']).dt.date
        isolated_counts = daily_isolated.groupby('date').size()
        
        daily_continuous = df_continuous.copy()
        daily_continuous['date'] = pd.to_datetime(daily_continuous['primeiro_alerta']).dt.date
        continuous_counts = daily_continuous.groupby('date').size()
        
        all_dates = pd.date_range(
            start=self.df_all_alerts['primeiro_alerta'].min(),
            end=self.df_all_alerts['ultimo_alerta'].max(),
            freq='D'
        ).date
        
        line_data = pd.DataFrame({'date': all_dates})
        line_data['Isolados'] = line_data['date'].map(isolated_counts).fillna(0)
        line_data['Cont√≠nuos'] = line_data['date'].map(continuous_counts).fillna(0)
        
        fig_lines = go.Figure()
        
        fig_lines.add_trace(go.Scatter(
            x=line_data['date'],
            y=line_data['Isolados'],
            mode='lines+markers',
            name='Isolados',
            line=dict(color='#ff4444', width=2),
            marker=dict(size=6),
            hovertemplate='%{x}<br>Isolados: %{y}<extra></extra>'
        ))
        
        fig_lines.add_trace(go.Scatter(
            x=line_data['date'],
            y=line_data['Cont√≠nuos'],
            mode='lines+markers',
            name='Cont√≠nuos',
            line=dict(color='#44ff44', width=2),
            marker=dict(size=6),
            hovertemplate='%{x}<br>Cont√≠nuos: %{y}<extra></extra>'
        ))
        
        fig_lines.update_layout(
            title="Quantidade de Alertas por Dia (Isolados vs Cont√≠nuos)",
            xaxis_title="Data",
            yaxis_title="Quantidade de Alertas",
            hovermode='x unified',
            height=400
        )
        
        st.plotly_chart(fig_lines, use_container_width=True, key='isolated_vs_continuous_lines')

        tab1, tab2, tab3 = st.tabs(["üî¥ Alertas Isolados", "üü¢ Alertas Cont√≠nuos", "üìä An√°lise Comparativa"])

        with tab1:
            st.subheader(f"üî¥ Alertas Isolados ({len(df_isolated)} alertas)")

            if len(df_isolated) > 0:
                if len(df_single_day) > 0:
                    st.info(f"üìÜ **{len(df_single_day)} alertas** ({len(df_single_day)/len(df_isolated)*100:.1f}%) ocorreram em apenas 1 dia")
                
                fig_iso = px.scatter(
                    df_isolated,
                    x='primeiro_alerta',
                    y='total_ocorrencias',
                    size='alertas_isolados',
                    color='pct_isolados',
                    title="‚è≥ Ocorr√™ncias de Alertas Isolados no Tempo",
                    hover_data=['alert_id', 'pattern_reason', 'num_grupos', 'unique_days'],
                    labels={'pct_isolados': '% Isolados', 'unique_days': 'Dias √önicos'}
                )
                st.plotly_chart(fig_iso, use_container_width=True, key='isolated_scatter')

                st.write("**üìù Raz√µes para Classifica√ß√£o como Isolado:**")
                reason_counts = df_isolated['pattern_reason'].value_counts()
                for reason, count in reason_counts.items():
                    st.write(f"‚Ä¢ {reason}: {count} alertas")

                st.write("**üîù Top 10 Alertas Isolados (por % de alertas isolados):**")
                top_isolated = df_isolated.nlargest(10, 'pct_isolados')[
                    ['alert_id', 'total_ocorrencias', 'alertas_isolados', 'num_grupos', 'pct_isolados', 'unique_days', 'pattern_reason']
                ]
                top_isolated.columns = ['Alert ID', 'Total Ocorr√™ncias', 'Alertas Isolados', 'N¬∫ Grupos', '% Isolados', 'Dias √önicos', 'Raz√£o']
                top_isolated['% Isolados'] = top_isolated['% Isolados'].round(1).astype(str) + '%'
                st.dataframe(top_isolated, use_container_width=True)

                with st.expander("üìã Ver todos os alertas isolados"):
                    isolated_list = df_isolated[['alert_id', 'total_ocorrencias', 'alertas_isolados',
                                                'num_grupos', 'pct_isolados', 'unique_days', 'pattern_reason']].copy()
                    isolated_list.columns = ['Alert ID', 'Total', 'Isolados', 'Grupos', '% Isolados', 'Dias √önicos', 'Raz√£o']
                    isolated_list['% Isolados'] = isolated_list['% Isolados'].round(1).astype(str) + '%'
                    st.dataframe(isolated_list, use_container_width=True)
            else:
                st.info("Nenhum alerta isolado encontrado com os crit√©rios atuais.")

        with tab2:
            st.subheader(f"üü¢ Alertas Cont√≠nuos ({len(df_continuous)} alertas)")

            if len(df_continuous) > 0:
                st.write("**üîù Top 10 Alertas Cont√≠nuos (maior n√∫mero de grupos):**")
                top_continuous = df_continuous.nlargest(10, 'num_grupos')[
                    ['alert_id', 'total_ocorrencias', 'num_grupos', 'alertas_agrupados', 'freq_dia', 'unique_days']
                ]
                top_continuous.columns = ['Alert ID', 'Total Ocorr√™ncias', 'N¬∫ Grupos', 'Alertas Agrupados', 'Freq/Dia', 'Dias √önicos']
                st.dataframe(top_continuous, use_container_width=True)

                col1, col2 = st.columns(2)
                with col1:
                    fig_groups = px.histogram(
                        df_continuous, 
                        x='num_grupos',
                        title="üìä Distribui√ß√£o de N√∫mero de Grupos",
                        labels={'num_grupos': 'N√∫mero de Grupos', 'count': 'Quantidade'}
                    )
                    st.plotly_chart(fig_groups, use_container_width=True, key='continuous_groups_hist')
                with col2:
                    fig_pct = px.histogram(
                        df_continuous,
                        x='pct_isolados',
                        title="üìä Distribui√ß√£o de % de Alertas Isolados",
                        labels={'pct_isolados': '% Alertas Isolados', 'count': 'Quantidade'}
                    )
                    st.plotly_chart(fig_pct, use_container_width=True, key='continuous_pct_hist')

                with st.expander("üìã Ver todos os alertas cont√≠nuos"):
                    continuous_list = df_continuous[['alert_id', 'total_ocorrencias', 'num_grupos',
                                                    'alertas_agrupados', 'alertas_isolados', 'pct_isolados', 'unique_days']].copy()
                    continuous_list.columns = ['Alert ID', 'Total', 'Grupos', 'Agrupados', 'Isolados', '% Isolados', 'Dias √önicos']
                    continuous_list['% Isolados'] = continuous_list['% Isolados'].round(1).astype(str) + '%'
                    st.dataframe(continuous_list, use_container_width=True)
            else:
                st.info("Nenhum alerta cont√≠nuo encontrado com os crit√©rios atuais.")

        with tab3:
            st.subheader("üìä An√°lise Comparativa Detalhada")

            fig_scatter = px.scatter(
                self.df_all_alerts,
                x='total_ocorrencias',
                y='intervalo_medio_h',
                color='pattern_type',
                title="üéØ Ocorr√™ncias vs Intervalo M√©dio",
                labels={
                    'total_ocorrencias': 'Total de Ocorr√™ncias',
                    'intervalo_medio_h': 'Intervalo M√©dio (horas)',
                    'pattern_type': 'Tipo de Padr√£o'
                },
                hover_data=['alert_id', 'unique_days'],
                color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
            )
            st.plotly_chart(fig_scatter, use_container_width=True, key='comparative_scatter')

            col1, col2 = st.columns(2)
            with col1:
                fig_box_occ = px.box(
                    self.df_all_alerts,
                    x='pattern_type',
                    y='total_ocorrencias',
                    title="üì¶ Distribui√ß√£o de Ocorr√™ncias",
                    color='pattern_type',
                    color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
                )
                st.plotly_chart(fig_box_occ, use_container_width=True, key='box_occurrences')

            with col2:
                fig_box_freq = px.box(
                    self.df_all_alerts,
                    x='pattern_type',
                    y='freq_dia',
                    title="üì¶ Distribui√ß√£o de Frequ√™ncia Di√°ria",
                    color='pattern_type',
                    color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
                )
                st.plotly_chart(fig_box_freq, use_container_width=True, key='box_frequency')

            st.subheader("üí° Recomenda√ß√µes de Tratamento")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**üî¥ Para Alertas Isolados:**")
                st.write("‚Ä¢ Considerar desativa√ß√£o ou revis√£o de configura√ß√£o")
                st.write("‚Ä¢ Verificar se s√£o falsos positivos")
                st.write("‚Ä¢ Analisar contexto espec√≠fico das ocorr√™ncias")
                st.write("‚Ä¢ Avaliar consolida√ß√£o com outros alertas similares")
                st.write("‚Ä¢ Alertas de 1 dia podem ser eventos √∫nicos sem recorr√™ncia")

            with col2:
                st.write("**üü¢ Para Alertas Cont√≠nuos:**")
                st.write("‚Ä¢ Priorizar automa√ß√£o de resposta")
                st.write("‚Ä¢ Implementar supress√£o inteligente")
                st.write("‚Ä¢ Criar runbooks espec√≠ficos")
                st.write("‚Ä¢ Considerar ajuste de thresholds")

    # ============================================================
    # NOVA AN√ÅLISE: VISUALIZA√á√ÉO DETALHADA DOS GRUPOS CONT√çNUOS
    # ============================================================

    def show_continuous_groups_detailed_view(self):
        """
        NOVA FUN√á√ÉO: Mostra visualiza√ß√£o detalhada dos grupos identificados nos alertas cont√≠nuos
        """
        st.header("üîç Visualiza√ß√£o Detalhada dos Grupos - Alertas Cont√≠nuos")
        
        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
        
        if len(df_continuous) == 0:
            st.warning("‚ö†Ô∏è Nenhum alerta cont√≠nuo encontrado para visualiza√ß√£o de grupos.")
            return
        
        st.info(f"üìä Analisando grupos detalhados de **{len(df_continuous)}** alertas cont√≠nuos")
        
        # Seletor de alertas para visualiza√ß√£o detalhada
        selected_alerts = st.multiselect(
            "üéØ Selecione alertas para visualizar grupos em detalhes (m√°x. 5):",
            options=df_continuous.nlargest(20, 'num_grupos')['alert_id'].tolist(),
            default=df_continuous.nlargest(3, 'num_grupos')['alert_id'].tolist()[:3],
            help="Mostrando os 20 alertas com mais grupos. Selecione at√© 5 para an√°lise detalhada."
        )
        
        if len(selected_alerts) > 5:
            st.warning("‚ö†Ô∏è M√°ximo de 5 alertas por vez. Mostrando apenas os 5 primeiros selecionados.")
            selected_alerts = selected_alerts[:5]
        
        if not selected_alerts:
            st.info("üëÜ Selecione pelo menos um alerta acima para ver os detalhes dos grupos")
            return
        
        # Para cada alerta selecionado, mostrar grupos detalhados
        for alert_id in selected_alerts:
            st.markdown("---")
            alert_info = df_continuous[df_continuous['alert_id'] == alert_id].iloc[0]
            
            with st.expander(f"üìä **Alert ID: {alert_id}** - {alert_info['num_grupos']} grupos identificados", expanded=True):
                # M√©tricas do alerta
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("Total Ocorr√™ncias", alert_info['total_ocorrencias'])
                with col2:
                    st.metric("N¬∫ de Grupos", alert_info['num_grupos'])
                with col3:
                    st.metric("Alertas Agrupados", alert_info['alertas_agrupados'])
                with col4:
                    st.metric("Alertas Isolados", alert_info['alertas_isolados'])
                with col5:
                    st.metric("Dias √önicos", alert_info['unique_days'])
                
                # Buscar dados detalhados do alerta
                alert_data = self.df_original[self.df_original['u_alert_id'] == alert_id].copy()
                alert_data, groups_info = identify_alert_groups(
                    alert_data,
                    self.max_gap_hours,
                    self.min_group_size,
                    self.spike_threshold_multiplier
                )
                
                if len(groups_info) > 0:
                    # Tabela de grupos
                    st.subheader("üìã Detalhes dos Grupos Identificados")
                    groups_df = pd.DataFrame(groups_info)
                    groups_df['start_time_str'] = pd.to_datetime(groups_df['start_time']).dt.strftime('%Y-%m-%d %H:%M')
                    groups_df['end_time_str'] = pd.to_datetime(groups_df['end_time']).dt.strftime('%Y-%m-%d %H:%M')
                    groups_df['duration_hours'] = groups_df['duration_hours'].round(2)
                    
                    groups_display = groups_df[['group_id', 'size', 'start_time_str', 'end_time_str', 'duration_hours']].copy()
                    groups_display.columns = ['ID Grupo', 'Tamanho', 'In√≠cio', 'Fim', 'Dura√ß√£o (h)']
                    st.dataframe(groups_display, use_container_width=True)
                    
                    # Timeline visual dos grupos
                    st.subheader("üìä Timeline Visual dos Grupos")
                    
                    fig_timeline = go.Figure()
                    
                    # Adicionar cada grupo como uma barra
                    colors = px.colors.qualitative.Plotly
                    for idx, group in groups_df.iterrows():
                        color = colors[int(group['group_id']) % len(colors)]
                        
                        fig_timeline.add_trace(go.Scatter(
                            x=[group['start_time'], group['end_time']],
                            y=[group['group_id'], group['group_id']],
                            mode='lines+markers',
                            name=f"Grupo {int(group['group_id'])}",
                            line=dict(color=color, width=15),
                            marker=dict(size=12, symbol='circle'),
                            hovertemplate=f"<b>Grupo {int(group['group_id'])}</b><br>" +
                                        f"Tamanho: {group['size']} alertas<br>" +
                                        f"Dura√ß√£o: {group['duration_hours']:.2f}h<br>" +
                                        f"In√≠cio: {group['start_time_str']}<br>" +
                                        f"Fim: {group['end_time_str']}<extra></extra>"
                        ))
                    
                    # Adicionar alertas isolados como pontos vermelhos
                    isolated_data = alert_data[alert_data['is_isolated']]
                    if len(isolated_data) > 0:
                        fig_timeline.add_trace(go.Scatter(
                            x=isolated_data['created_on'],
                            y=[-1] * len(isolated_data),
                            mode='markers',
                            name='Alertas Isolados',
                            marker=dict(size=10, color='red', symbol='x'),
                            hovertemplate='<b>Alerta Isolado</b><br>%{x}<extra></extra>'
                        ))
                    
                    fig_timeline.update_layout(
                        title=f"Timeline de Grupos - Alert ID: {alert_id}",
                        xaxis_title="Data/Hora",
                        yaxis_title="ID do Grupo",
                        yaxis=dict(
                            tickmode='linear',
                            tick0=-1,
                            dtick=1,
                            ticktext=['Isolados'] + [f'Grupo {i}' for i in range(len(groups_info))],
                            tickvals=[-1] + list(range(len(groups_info)))
                        ),
                        height=400,
                        hovermode='closest',
                        showlegend=True
                    )
                    
                    st.plotly_chart(fig_timeline, use_container_width=True, key=f'timeline_{alert_id}')
                    
                    # An√°lise de distribui√ß√£o temporal dos grupos
                    st.subheader("üìà An√°lise Temporal dos Grupos")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Gr√°fico de barras: tamanho dos grupos
                        fig_sizes = px.bar(
                            groups_df,
                            x='group_id',
                            y='size',
                            title="Tamanho de Cada Grupo",
                            labels={'group_id': 'ID do Grupo', 'size': 'Quantidade de Alertas'},
                            text='size'
                        )
                        fig_sizes.update_traces(textposition='outside')
                        st.plotly_chart(fig_sizes, use_container_width=True, key=f'sizes_{alert_id}')
                    
                    with col2:
                        # Gr√°fico de barras: dura√ß√£o dos grupos
                        fig_duration = px.bar(
                            groups_df,
                            x='group_id',
                            y='duration_hours',
                            title="Dura√ß√£o de Cada Grupo (horas)",
                            labels={'group_id': 'ID do Grupo', 'duration_hours': 'Dura√ß√£o (h)'},
                            text='duration_hours'
                        )
                        fig_duration.update_traces(textposition='outside', texttemplate='%{text:.1f}h')
                        st.plotly_chart(fig_duration, use_container_width=True, key=f'duration_{alert_id}')
                    
                    # Estat√≠sticas dos grupos
                    st.subheader("üìä Estat√≠sticas dos Grupos")
                    stats_col1, stats_col2, stats_col3, stats_col4 = st.columns(4)
                    
                    with stats_col1:
                        st.metric("Tamanho M√©dio", f"{groups_df['size'].mean():.1f} alertas")
                    with stats_col2:
                        st.metric("Maior Grupo", f"{groups_df['size'].max()} alertas")
                    with stats_col3:
                        st.metric("Dura√ß√£o M√©dia", f"{groups_df['duration_hours'].mean():.1f}h")
                    with stats_col4:
                        st.metric("Maior Dura√ß√£o", f"{groups_df['duration_hours'].max():.1f}h")
                    
                    # Intervalo entre grupos
                    if len(groups_df) > 1:
                        st.subheader("‚è±Ô∏è Intervalos Entre Grupos")
                        gaps = []
                        for i in range(len(groups_df) - 1):
                            gap = (groups_df.iloc[i+1]['start_time'] - groups_df.iloc[i]['end_time']).total_seconds() / 3600
                            gaps.append({
                                'De': f"Grupo {int(groups_df.iloc[i]['group_id'])}",
                                'Para': f"Grupo {int(groups_df.iloc[i+1]['group_id'])}",
                                'Intervalo (h)': round(gap, 2)
                            })
                        
                        gaps_df = pd.DataFrame(gaps)
                        st.dataframe(gaps_df, use_container_width=True)
                        
                        avg_gap = gaps_df['Intervalo (h)'].mean()
                        st.info(f"üìä Intervalo m√©dio entre grupos: **{avg_gap:.2f} horas**")
                
                else:
                    st.warning("Nenhum grupo identificado para este alerta.")
        
        # Resumo geral de todos os grupos
        st.markdown("---")
        st.header("üìä Resumo Geral dos Grupos - Todos os Alertas Cont√≠nuos")
        
        all_groups_data = []
        for _, alert in df_continuous.iterrows():
            alert_data = self.df_original[self.df_original['u_alert_id'] == alert['alert_id']].copy()
            _, groups_info = identify_alert_groups(
                alert_data,
                self.max_gap_hours,
                self.min_group_size,
                self.spike_threshold_multiplier
            )
            for group in groups_info:
                all_groups_data.append({
                    'alert_id': alert['alert_id'],
                    'group_id': group['group_id'],
                    'size': group['size'],
                    'duration_hours': group['duration_hours']
                })
        
        if all_groups_data:
            all_groups_df = pd.DataFrame(all_groups_data)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total de Grupos", len(all_groups_df))
            with col2:
                st.metric("Tamanho M√©dio", f"{all_groups_df['size'].mean():.1f} alertas")
            with col3:
                st.metric("Dura√ß√£o M√©dia", f"{all_groups_df['duration_hours'].mean():.1f}h")
            with col4:
                st.metric("Alertas/Grupo M√°x", int(all_groups_df['size'].max()))
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig_all_sizes = px.histogram(
                    all_groups_df,
                    x='size',
                    title="Distribui√ß√£o de Tamanhos dos Grupos",
                    labels={'size': 'Tamanho do Grupo (alertas)', 'count': 'Quantidade de Grupos'},
                    nbins=20
                )
                st.plotly_chart(fig_all_sizes, use_container_width=True, key='all_sizes_hist')
            
            with col2:
                fig_all_duration = px.histogram(
                    all_groups_df,
                    x='duration_hours',
                    title="Distribui√ß√£o de Dura√ß√µes dos Grupos",
                    labels={'duration_hours': 'Dura√ß√£o (horas)', 'count': 'Quantidade de Grupos'},
                    nbins=20
                )
                st.plotly_chart(fig_all_duration, use_container_width=True, key='all_duration_hist')

    # ============================================================
    # AN√ÅLISE DE RECORR√äNCIA - ALERTAS CONT√çNUOS
    # ============================================================

    def analyze_continuous_recurrence_patterns(self):
        """
        Analisa padr√µes de recorr√™ncia APENAS dos alertas cont√≠nuos.
        Identifica se h√° concentra√ß√£o por hora do dia ou dia da semana.
        """
        st.header("üîÅ An√°lise de Recorr√™ncia - Alertas Cont√≠nuos")
        
        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
        
        if len(df_continuous) == 0:
            st.warning("‚ö†Ô∏è Nenhum alerta cont√≠nuo encontrado para an√°lise de recorr√™ncia.")
            return
        
        st.info(f"üìä Analisando padr√µes de recorr√™ncia de **{len(df_continuous)}** alertas cont√≠nuos")
        
        continuous_alert_ids = df_continuous['alert_id'].unique()
        df_continuous_details = self.df_original[self.df_original['u_alert_id'].isin(continuous_alert_ids)].copy()
        
        df_continuous_details['hour'] = df_continuous_details['created_on'].dt.hour
        df_continuous_details['day_of_week'] = df_continuous_details['created_on'].dt.dayofweek
        df_continuous_details['day_name'] = df_continuous_details['created_on'].dt.day_name()
        
        st.subheader("‚è∞ Padr√£o de Recorr√™ncia por Hora do Dia")
        
        hourly_dist = df_continuous_details['hour'].value_counts().sort_index()
        hourly_pct = (hourly_dist / hourly_dist.sum() * 100).round(2)
        
        top_3_hours = hourly_pct.nlargest(3)
        total_top_3_hours = top_3_hours.sum()
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig_hourly = go.Figure()
            fig_hourly.add_trace(go.Bar(
                x=hourly_dist.index,
                y=hourly_dist.values,
                marker_color=['red' if i in top_3_hours.index else 'lightblue' 
                             for i in hourly_dist.index],
                text=hourly_pct.values,
                texttemplate='%{text:.1f}%',
                textposition='outside',
                hovertemplate='Hora: %{x}:00<br>Alertas: %{y}<br>% do total: %{text:.1f}%<extra></extra>'
            ))
            fig_hourly.update_layout(
                title="Distribui√ß√£o de Alertas Cont√≠nuos por Hora",
                xaxis_title="Hora do Dia",
                yaxis_title="Quantidade de Alertas",
                showlegend=False,
                height=400
            )
            st.plotly_chart(fig_hourly, use_container_width=True, key='recurrence_hourly')
        
        with col2:
            st.metric("üïê Hora com Mais Alertas", f"{top_3_hours.index[0]}:00")
            st.metric("üìä % nesta Hora", f"{top_3_hours.values[0]:.1f}%")
            st.metric("üîù Top 3 Horas (% total)", f"{total_top_3_hours:.1f}%")
            
            if total_top_3_hours > 60:
                pattern_hour = "üî¥ **Concentrado**"
                hour_desc = "Alertas altamente concentrados em poucas horas"
            elif total_top_3_hours > 40:
                pattern_hour = "üü° **Moderado**"
                hour_desc = "Alertas parcialmente concentrados"
            else:
                pattern_hour = "üü¢ **Distribu√≠do**"
                hour_desc = "Alertas bem distribu√≠dos ao longo do dia"
            
            st.write(f"**Padr√£o:** {pattern_hour}")
            st.write(hour_desc)
        
        st.write("**üîù Top 5 Hor√°rios:**")
        top_5_hours = hourly_pct.nlargest(5)
        for hour, pct in top_5_hours.items():
            st.write(f"‚Ä¢ **{hour:02d}:00** - {hourly_dist[hour]} alertas ({pct:.1f}%)")
        
        st.markdown("---")
        
        st.subheader("üìÖ Padr√£o de Recorr√™ncia por Dia da Semana")
        
        daily_dist = df_continuous_details['day_name'].value_counts()
        days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        daily_dist_ordered = daily_dist.reindex(days_order).fillna(0)
        daily_pct = (daily_dist_ordered / daily_dist_ordered.sum() * 100).round(2)
        
        top_3_days = daily_pct.nlargest(3)
        total_top_3_days = top_3_days.sum()
        
        day_translation = {
            'Monday': 'Segunda', 'Tuesday': 'Ter√ßa', 'Wednesday': 'Quarta',
            'Thursday': 'Quinta', 'Friday': 'Sexta', 'Saturday': 'S√°bado', 'Sunday': 'Domingo'
        }
        daily_pct_pt = daily_pct.rename(index=day_translation)
        daily_dist_ordered_pt = daily_dist_ordered.rename(index=day_translation)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig_daily = go.Figure()
            fig_daily.add_trace(go.Bar(
                x=list(daily_pct_pt.index),
                y=daily_dist_ordered_pt.values,
                marker_color=['red' if day in [day_translation[d] for d in top_3_days.index] else 'lightgreen' 
                             for day in daily_pct_pt.index],
                text=daily_pct_pt.values,
                texttemplate='%{text:.1f}%',
                textposition='outside',
                hovertemplate='Dia: %{x}<br>Alertas: %{y}<br>% do total: %{text:.1f}%<extra></extra>'
            ))
            fig_daily.update_layout(
                title="Distribui√ß√£o de Alertas Cont√≠nuos por Dia da Semana",
                xaxis_title="Dia da Semana",
                yaxis_title="Quantidade de Alertas",
                showlegend=False,
                height=400
            )
            st.plotly_chart(fig_daily, use_container_width=True, key='recurrence_daily')
        
        with col2:
            top_day_en = top_3_days.index[0]
            top_day_pt = day_translation[top_day_en]
            st.metric("üìÖ Dia com Mais Alertas", top_day_pt)
            st.metric("üìä % neste Dia", f"{top_3_days.values[0]:.1f}%")
            st.metric("üîù Top 3 Dias (% total)", f"{total_top_3_days:.1f}%")
            
            if total_top_3_days > 60:
                pattern_day = "üî¥ **Concentrado**"
                day_desc = "Alertas altamente concentrados em poucos dias"
            elif total_top_3_days > 45:
                pattern_day = "üü° **Moderado**"
                day_desc = "Alertas parcialmente concentrados"
            else:
                pattern_day = "üü¢ **Distribu√≠do**"
                day_desc = "Alertas bem distribu√≠dos na semana"
            
            st.write(f"**Padr√£o:** {pattern_day}")
            st.write(day_desc)
        
        st.write("**üîù Ranking de Dias:**")
        top_days_sorted = daily_pct.sort_values(ascending=False)
        for day, pct in top_days_sorted.items():
            day_pt = day_translation[day]
            count = daily_dist_ordered[day]
            st.write(f"‚Ä¢ **{day_pt}** - {int(count)} alertas ({pct:.1f}%)")
        
        st.markdown("---")
        
        st.subheader("üéØ Resumo do Padr√£o de Recorr√™ncia")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**‚è∞ Padr√£o Hor√°rio:**")
            st.write(f"‚Ä¢ {pattern_hour}")
            st.write(f"‚Ä¢ Top 3 horas concentram {total_top_3_hours:.1f}% dos alertas")
            st.write(f"‚Ä¢ Hor√°rio principal: **{top_3_hours.index[0]:02d}:00**")
            
            if total_top_3_hours > 50:
                st.write("‚Ä¢ üí° **Recomenda√ß√£o:** Considerar janela de manuten√ß√£o espec√≠fica")
        
        with col2:
            st.write("**üìÖ Padr√£o Semanal:**")
            st.write(f"‚Ä¢ {pattern_day}")
            st.write(f"‚Ä¢ Top 3 dias concentram {total_top_3_days:.1f}% dos alertas")
            st.write(f"‚Ä¢ Dia principal: **{day_translation[top_day_en]}**")
            
            if total_top_3_days > 50:
                st.write("‚Ä¢ üí° **Recomenda√ß√£o:** Aten√ß√£o redobrada nestes dias")
        
        st.markdown("---")
        st.subheader("üèÜ Padr√£o Dominante")
        
        if total_top_3_hours > total_top_3_days:
            st.success(f"‚è∞ **HORA DO DIA** √© o padr√£o dominante ({total_top_3_hours:.1f}% vs {total_top_3_days:.1f}%)")
            st.write(f"Os alertas cont√≠nuos tendem a ocorrer principalmente no hor√°rio das **{top_3_hours.index[0]:02d}:00**")
        elif total_top_3_days > total_top_3_hours:
            st.success(f"üìÖ **DIA DA SEMANA** √© o padr√£o dominante ({total_top_3_days:.1f}% vs {total_top_3_hours:.1f}%)")
            st.write(f"Os alertas cont√≠nuos tendem a ocorrer principalmente √†s **{day_translation[top_day_en]}**")
        else:
            st.info("üìä **Padr√£o BALANCEADO** - N√£o h√° concentra√ß√£o clara em hora ou dia espec√≠ficos")
        
        st.markdown("---")
        st.subheader("üî• Mapa de Calor: Hora √ó Dia da Semana")
        
        heatmap_data = df_continuous_details.groupby(['day_of_week', 'hour']).size().reset_index(name='count')
        heatmap_pivot = heatmap_data.pivot(index='hour', columns='day_of_week', values='count').fillna(0)
        
        day_map = {0: 'Seg', 1: 'Ter', 2: 'Qua', 3: 'Qui', 4: 'Sex', 5: 'S√°b', 6: 'Dom'}
        heatmap_pivot.columns = [day_map[col] for col in heatmap_pivot.columns]
        
        fig_heatmap = go.Figure(data=go.Heatmap(
            z=heatmap_pivot.values,
            x=heatmap_pivot.columns,
            y=heatmap_pivot.index,
            colorscale='Reds',
            hovertemplate='Dia: %{x}<br>Hora: %{y}:00<br>Alertas: %{z}<extra></extra>'
        ))
        
        fig_heatmap.update_layout(
            title="Concentra√ß√£o de Alertas por Dia e Hora",
            xaxis_title="Dia da Semana",
            yaxis_title="Hora do Dia",
            height=600
        )
        
        st.plotly_chart(fig_heatmap, use_container_width=True, key='recurrence_heatmap')

    # ============================================================
    # OUTRAS AN√ÅLISES GLOBAIS
    # ============================================================

    def show_global_overview(self, filter_isolated=False):
        st.subheader("üìà Vis√£o Geral dos Alertas")
        
        df_to_analyze = self.df_all_alerts
        if filter_isolated:
            df_to_analyze = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
            st.info(f"üîç Mostrando apenas alertas cont√≠nuos ({len(df_to_analyze)} de {len(self.df_all_alerts)})")
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**üî• Top 10 Alertas Mais Frequentes**")
            top_frequent = df_to_analyze.nlargest(10, 'total_ocorrencias')[['alert_id', 'total_ocorrencias', 'freq_dia', 'pattern_type', 'unique_days']]
            top_frequent.columns = ['Alert ID', 'Total Ocorr√™ncias', 'Frequ√™ncia/Dia', 'Tipo', 'Dias √önicos']
            st.dataframe(top_frequent, use_container_width=True)
        with col2:
            st.write("**‚ö° Top 10 Alertas Mais R√°pidos (Menor Intervalo)**")
            df_with_intervals = df_to_analyze.dropna(subset=['intervalo_medio_h'])
            if len(df_with_intervals) > 0:
                top_fast = df_with_intervals.nsmallest(10, 'intervalo_medio_h')[['alert_id', 'intervalo_medio_h', 'total_ocorrencias', 'pattern_type']]
                top_fast.columns = ['Alert ID', 'Intervalo M√©dio (h)', 'Total Ocorr√™ncias', 'Tipo']
                st.dataframe(top_fast, use_container_width=True)
            else:
                st.info("Sem dados de intervalo dispon√≠veis")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            fig_freq = px.histogram(df_to_analyze, x='freq_dia', title="üìä Distribui√ß√£o de Frequ√™ncia (alertas/dia)",
                                   labels={'freq_dia': 'Alertas por Dia', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_freq, use_container_width=True)
        with col2:
            fig_int = px.histogram(df_to_analyze, x='freq_semana', title="üìä Distribui√ß√£o de Frequ√™ncia (alertas/semana)",
                                  labels={'freq_semana': 'Alertas por semana', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_int, use_container_width=True)
        with col3:
            fig_int = px.histogram(df_to_analyze, x='freq_mes', title="üìä Distribui√ß√£o de Frequ√™ncia (alertas/mes)",
                                  labels={'freq_mes': 'Alertas por mes', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_int, use_container_width=True)
        with col4:
            df_with_intervals = df_to_analyze.dropna(subset=['intervalo_medio_h'])
            if len(df_with_intervals) > 0:
                fig_int = px.histogram(df_with_intervals, x='intervalo_medio_h', title="‚è±Ô∏è Distribui√ß√£o de Intervalos M√©dios",
                                      labels={'intervalo_medio_h': 'Intervalo M√©dio (horas)', 'count': 'Quantidade de Alert IDs'})
                st.plotly_chart(fig_int, use_container_width=True)

    def perform_clustering_analysis(self, use_only_continuous=True):
        st.subheader("üéØ Agrupamento de Alertas por Perfil de Comportamento")
        
        df_for_clustering = self.df_all_alerts
        if use_only_continuous:
            df_for_clustering = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous'].copy()
            st.info(f"üîç Usando apenas alertas cont√≠nuos para clustering ({len(df_for_clustering)} alertas)")
        
        if len(df_for_clustering) < 2:
            st.warning("‚ö†Ô∏è Dados insuficientes para clustering")
            return None
        
        features = [
            'freq_dia', 'intervalo_medio_h', 'intervalo_std_h',
            'hora_pico', 'pct_fins_semana', 'pct_horario_comercial', 'variabilidade_intervalo'
        ]
        X = df_for_clustering[features].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        st.write("**üîç Determinando N√∫mero √ìtimo de Clusters...**")
        max_clusters = min(10, len(X) - 1)
        silhouette_scores = []
        inertias = []
        
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)
            silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))
            inertias.append(kmeans.inertia_)
        
        optimal_k = range(2, max_clusters + 1)[np.argmax(silhouette_scores)]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üéØ N√∫mero √ìtimo de Clusters", optimal_k)
        with col2:
            st.metric("üìä Silhouette Score", f"{max(silhouette_scores):.3f}")
        
        kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        clusters = kmeans_final.fit_predict(X_scaled)
        
        df_for_clustering['cluster'] = clusters
        
        self.df_all_alerts['cluster'] = np.nan
        self.df_all_alerts.loc[df_for_clustering.index, 'cluster'] = df_for_clustering['cluster']
        
        col1, col2 = st.columns(2)
        with col1:
            fig_scatter = px.scatter(
                df_for_clustering,
                x='freq_dia',
                y='intervalo_medio_h',
                color='cluster',
                size='total_ocorrencias',
                hover_data=['alert_id'],
                title="üé® Clusters: Frequ√™ncia vs Intervalo M√©dio"
            )
            st.plotly_chart(fig_scatter, use_container_width=True, key='cluster_scatter')
        with col2:
            cluster_dist = df_for_clustering['cluster'].value_counts().sort_index()
            fig_dist = px.bar(
                x=cluster_dist.index,
                y=cluster_dist.values,
                title="üìä Distribui√ß√£o de Alertas por Cluster",
                labels={'x': 'Cluster', 'y': 'Quantidade de Alert IDs'}
            )
            st.plotly_chart(fig_dist, use_container_width=True, key='cluster_dist')
        return optimal_k

    def show_cluster_profiles(self, n_clusters):
        st.subheader("üë• Perfis dos Clusters")
        cluster_tabs = st.tabs([f"Cluster {i}" for i in range(n_clusters)])
        for i in range(n_clusters):
            with cluster_tabs[i]:
                cluster_data = self.df_all_alerts[self.df_all_alerts['cluster'] == i]
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üìä Quantidade de Alertas", len(cluster_data))
                with col2:
                    avg_freq = cluster_data['freq_dia'].mean()
                    st.metric("üìà Freq. M√©dia/Dia", f"{avg_freq:.2f}")
                with col3:
                    avg_interval = cluster_data['intervalo_medio_h'].mean()
                    st.metric("‚è±Ô∏è Intervalo M√©dio (h)", f"{avg_interval:.2f}")
                with col4:
                    avg_hour = cluster_data['hora_pico'].mean()
                    st.metric("üïê Hora Pico M√©dia", f"{avg_hour:.0f}:00")
                st.write("**üéØ Caracter√≠sticas do Cluster:**")
                weekend_pct = cluster_data['pct_fins_semana'].mean()
                business_pct = cluster_data['pct_horario_comercial'].mean()
                characteristics = []
                if avg_freq > self.df_all_alerts['freq_dia'].median():
                    characteristics.append("üî• **Alta frequ√™ncia**")
                else:
                    characteristics.append("üêå **Baixa frequ√™ncia**")
                if avg_interval < self.df_all_alerts['intervalo_medio_h'].median():
                    characteristics.append("‚ö° **Intervalos curtos**")
                else:
                    characteristics.append("‚è≥ **Intervalos longos**")
                if weekend_pct > 30:
                    characteristics.append("üóìÔ∏è **Ativo nos fins de semana**")
                if business_pct > 70:
                    characteristics.append("üè¢ **Predominantemente em hor√°rio comercial**")
                elif business_pct < 30:
                    characteristics.append("üåô **Predominantemente fora do hor√°rio comercial**")
                for char in characteristics:
                    st.write(f"‚Ä¢ {char}")
                with st.expander(f"üìã Alertas no Cluster {i}"):
                    cluster_alerts = cluster_data[['alert_id', 'total_ocorrencias', 'freq_dia', 'intervalo_medio_h']].copy()
                    cluster_alerts.columns = ['Alert ID', 'Total Ocorr√™ncias', 'Freq/Dia', 'Intervalo M√©dio (h)']
                    st.dataframe(cluster_alerts, use_container_width=True, key=f'cluster_table_{i}')

    def show_cluster_recommendations(self):
        st.subheader("üí° Recomenda√ß√µes por Cluster")
        for cluster_id in sorted(self.df_all_alerts['cluster'].dropna().unique()):
            cluster_data = self.df_all_alerts[self.df_all_alerts['cluster'] == cluster_id]
            avg_freq = cluster_data['freq_dia'].mean()
            avg_interval = cluster_data['intervalo_medio_h'].mean()
            weekend_pct = cluster_data['pct_fins_semana'].mean()
            business_pct = cluster_data['pct_horario_comercial'].mean()
            with st.expander(f"üéØ Recomenda√ß√µes para Cluster {int(cluster_id)} ({len(cluster_data)} alertas)"):
                recommendations = []
                if avg_freq > 5:
                    recommendations.append("üö® **Prioridade Alta**: Alertas muito frequentes - investigar causa raiz")
                    recommendations.append("üîß **A√ß√£o**: Considerar automa√ß√£o de resposta ou ajuste de thresholds")
                if avg_interval < 1:
                    recommendations.append("‚ö° **Rajadas detectadas**: Poss√≠vel tempestade de alertas")
                    recommendations.append("üõ°Ô∏è **A√ß√£o**: Implementar rate limiting ou supress√£o inteligente")
                if weekend_pct > 50:
                    recommendations.append("üóìÔ∏è **Padr√£o de fim de semana**: Alertas ativos nos fins de semana")
                    recommendations.append("üë• **A√ß√£o**: Verificar cobertura de plant√£o")
                if business_pct < 30:
                    recommendations.append("üåô **Padr√£o noturno**: Principalmente fora do hor√°rio comercial")
                    recommendations.append("üîÑ **A√ß√£o**: Considerar processos automatizados noturnos")
                if avg_freq < 0.5:
                    recommendations.append("üìâ **Baixa frequ√™ncia**: Alertas espor√°dicos")
                    recommendations.append("üìä **A√ß√£o**: Revisar relev√¢ncia e configura√ß√£o do alerta")
                for rec in recommendations:
                    st.write(f"‚Ä¢ {rec}")
                if not recommendations:
                    st.write("‚Ä¢ ‚úÖ **Padr√£o normal**: Nenhuma a√ß√£o espec√≠fica recomendada")

    # ==============================================================
    # AN√ÅLISES INDIVIDUAIS
    # ==============================================================
    # ==============================================================
    # AN√ÅLISES INDIVIDUAIS
    # ==============================================================

    def analyze_individual_recurrence_patterns(self):
        """
        Analisa padr√µes de recorr√™ncia para um alerta individual espec√≠fico.
        Similar √† an√°lise global, mas focada em um √∫nico alert_id.
        """
        st.header("üîÅ An√°lise de Recorr√™ncia - Alert Individual")
        
        if self.df is None or len(self.df) == 0:
            st.warning("‚ö†Ô∏è Nenhum dado dispon√≠vel para an√°lise de recorr√™ncia.")
            return
        
        st.info(f"üìä Analisando padr√µes de recorr√™ncia do Alert ID: **{self.alert_id}** ({len(self.df)} ocorr√™ncias)")
        
        st.subheader("‚è∞ Padr√£o de Recorr√™ncia por Hora do Dia")
        
        hourly_dist = self.df['hour'].value_counts().sort_index()
        hourly_pct = (hourly_dist / hourly_dist.sum() * 100).round(2)
        
        top_3_hours = hourly_pct.nlargest(3)
        total_top_3_hours = top_3_hours.sum()
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig_hourly = go.Figure()
            fig_hourly.add_trace(go.Bar(
                x=hourly_dist.index,
                y=hourly_dist.values,
                marker_color=['red' if i in top_3_hours.index else 'lightblue' 
                             for i in hourly_dist.index],
                text=hourly_pct.values,
                texttemplate='%{text:.1f}%',
                textposition='outside',
                hovertemplate='Hora: %{x}:00<br>Alertas: %{y}<br>% do total: %{text:.1f}%<extra></extra>'
            ))
            fig_hourly.update_layout(
                title="Distribui√ß√£o de Alertas por Hora",
                xaxis_title="Hora do Dia",
                yaxis_title="Quantidade de Alertas",
                showlegend=False,
                height=400
            )
            st.plotly_chart(fig_hourly, use_container_width=True, key='individual_recurrence_hourly')
        
        with col2:
            st.metric("üïê Hora com Mais Alertas", f"{top_3_hours.index[0]}:00")
            st.metric("üìä % nesta Hora", f"{top_3_hours.values[0]:.1f}%")
            st.metric("üîù Top 3 Horas (% total)", f"{total_top_3_hours:.1f}%")
            
            if total_top_3_hours > 60:
                pattern_hour = "üî¥ **Concentrado**"
                hour_desc = "Alertas altamente concentrados em poucas horas"
            elif total_top_3_hours > 40:
                pattern_hour = "üü° **Moderado**"
                hour_desc = "Alertas parcialmente concentrados"
            else:
                pattern_hour = "üü¢ **Distribu√≠do**"
                hour_desc = "Alertas bem distribu√≠dos ao longo do dia"
            
            st.write(f"**Padr√£o:** {pattern_hour}")
            st.write(hour_desc)
        
        st.write("**üîù Top 5 Hor√°rios:**")
        top_5_hours = hourly_pct.nlargest(5)
        for hour, pct in top_5_hours.items():
            st.write(f"‚Ä¢ **{hour:02d}:00** - {hourly_dist[hour]} alertas ({pct:.1f}%)")
        
        st.markdown("---")
        
        st.subheader("üìÖ Padr√£o de Recorr√™ncia por Dia da Semana")
        
        daily_dist = self.df['day_name'].value_counts()
        days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        daily_dist_ordered = daily_dist.reindex(days_order).fillna(0)
        daily_pct = (daily_dist_ordered / daily_dist_ordered.sum() * 100).round(2)
        
        top_3_days = daily_pct.nlargest(3)
        total_top_3_days = top_3_days.sum()
        
        day_translation = {
            'Monday': 'Segunda', 'Tuesday': 'Ter√ßa', 'Wednesday': 'Quarta',
            'Thursday': 'Quinta', 'Friday': 'Sexta', 'Saturday': 'S√°bado', 'Sunday': 'Domingo'
        }
        daily_pct_pt = daily_pct.rename(index=day_translation)
        daily_dist_ordered_pt = daily_dist_ordered.rename(index=day_translation)
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig_daily = go.Figure()
            fig_daily.add_trace(go.Bar(
                x=list(daily_pct_pt.index),
                y=daily_dist_ordered_pt.values,
                marker_color=['red' if day in [day_translation[d] for d in top_3_days.index] else 'lightgreen' 
                             for day in daily_pct_pt.index],
                text=daily_pct_pt.values,
                texttemplate='%{text:.1f}%',
                textposition='outside',
                hovertemplate='Dia: %{x}<br>Alertas: %{y}<br>% do total: %{text:.1f}%<extra></extra>'
            ))
            fig_daily.update_layout(
                title="Distribui√ß√£o de Alertas por Dia da Semana",
                xaxis_title="Dia da Semana",
                yaxis_title="Quantidade de Alertas",
                showlegend=False,
                height=400
            )
            st.plotly_chart(fig_daily, use_container_width=True, key='individual_recurrence_daily')
        
        with col2:
            top_day_en = top_3_days.index[0]
            top_day_pt = day_translation[top_day_en]
            st.metric("üìÖ Dia com Mais Alertas", top_day_pt)
            st.metric("üìä % neste Dia", f"{top_3_days.values[0]:.1f}%")
            st.metric("üîù Top 3 Dias (% total)", f"{total_top_3_days:.1f}%")
            
            if total_top_3_days > 60:
                pattern_day = "üî¥ **Concentrado**"
                day_desc = "Alertas altamente concentrados em poucos dias"
            elif total_top_3_days > 45:
                pattern_day = "üü° **Moderado**"
                day_desc = "Alertas parcialmente concentrados"
            else:
                pattern_day = "üü¢ **Distribu√≠do**"
                day_desc = "Alertas bem distribu√≠dos na semana"
            
            st.write(f"**Padr√£o:** {pattern_day}")
            st.write(day_desc)
        
        st.write("**üîù Ranking de Dias:**")
        top_days_sorted = daily_pct.sort_values(ascending=False)
        for day, pct in top_days_sorted.items():
            day_pt = day_translation[day]
            count = daily_dist_ordered[day]
            st.write(f"‚Ä¢ **{day_pt}** - {int(count)} alertas ({pct:.1f}%)")
        
        st.markdown("---")
        
        st.subheader("üéØ Resumo do Padr√£o de Recorr√™ncia")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**‚è∞ Padr√£o Hor√°rio:**")
            st.write(f"‚Ä¢ {pattern_hour}")
            st.write(f"‚Ä¢ Top 3 horas concentram {total_top_3_hours:.1f}% dos alertas")
            st.write(f"‚Ä¢ Hor√°rio principal: **{top_3_hours.index[0]:02d}:00**")
            
            if total_top_3_hours > 50:
                st.write("‚Ä¢ üí° **Recomenda√ß√£o:** Considerar janela de manuten√ß√£o espec√≠fica")
        
        with col2:
            st.write("**üìÖ Padr√£o Semanal:**")
            st.write(f"‚Ä¢ {pattern_day}")
            st.write(f"‚Ä¢ Top 3 dias concentram {total_top_3_days:.1f}% dos alertas")
            st.write(f"‚Ä¢ Dia principal: **{day_translation[top_day_en]}**")
            
            if total_top_3_days > 50:
                st.write("‚Ä¢ üí° **Recomenda√ß√£o:** Aten√ß√£o redobrada nestes dias")
        
        st.markdown("---")
        st.subheader("üèÜ Padr√£o Dominante")
        
        if total_top_3_hours > total_top_3_days:
            st.success(f"‚è∞ **HORA DO DIA** √© o padr√£o dominante ({total_top_3_hours:.1f}% vs {total_top_3_days:.1f}%)")
            st.write(f"Este alerta tende a ocorrer principalmente no hor√°rio das **{top_3_hours.index[0]:02d}:00**")
        elif total_top_3_days > total_top_3_hours:
            st.success(f"üìÖ **DIA DA SEMANA** √© o padr√£o dominante ({total_top_3_days:.1f}% vs {total_top_3_hours:.1f}%)")
            st.write(f"Este alerta tende a ocorrer principalmente √†s **{day_translation[top_day_en]}**")
        else:
            st.info("üìä **Padr√£o BALANCEADO** - N√£o h√° concentra√ß√£o clara em hora ou dia espec√≠ficos")
        
        st.markdown("---")
        st.subheader("üî• Mapa de Calor: Hora √ó Dia da Semana")
        
        heatmap_data = self.df.groupby(['day_of_week', 'hour']).size().reset_index(name='count')
        heatmap_pivot = heatmap_data.pivot(index='hour', columns='day_of_week', values='count').fillna(0)
        
        day_map = {0: 'Seg', 1: 'Ter', 2: 'Qua', 3: 'Qui', 4: 'Sex', 5: 'S√°b', 6: 'Dom'}
        heatmap_pivot.columns = [day_map[col] for col in heatmap_pivot.columns]
        
        fig_heatmap = go.Figure(data=go.Heatmap(
            z=heatmap_pivot.values,
            x=heatmap_pivot.columns,
            y=heatmap_pivot.index,
            colorscale='Reds',
            hovertemplate='Dia: %{x}<br>Hora: %{y}:00<br>Alertas: %{z}<extra></extra>'
        ))
        
        fig_heatmap.update_layout(
            title="Concentra√ß√£o de Alertas por Dia e Hora",
            xaxis_title="Dia da Semana",
            yaxis_title="Hora do Dia",
            height=600
        )
        
        st.plotly_chart(fig_heatmap, use_container_width=True, key='individual_recurrence_heatmap')

    def show_basic_stats(self):
        st.header("üìä Estat√≠sticas B√°sicas")
        total = len(self.df)
        period_days = (self.dates.max() - self.dates.min()).days + 1
        avg_per_day = total / period_days
        unique_days = self.df['date'].nunique()
        
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üî• Total de Ocorr√™ncias", total)
        with col2:
            st.metric("üìÖ Per√≠odo (dias)", period_days)
        with col3:
            st.metric("üìÜ Dias √önicos", unique_days)
        with col4:
            st.metric("üìà M√©dia/dia", f"{avg_per_day:.2f}")
        with col5:
            last_alert = self.dates.max().strftime("%d/%m %H:%M")
            st.metric("üïê √öltimo Alerta", last_alert)
        
        if unique_days == 1:
            st.warning("‚ö†Ô∏è **ATEN√á√ÉO:** Todos os alertas ocorreram em apenas 1 dia! Este alerta √© classificado como ISOLADO.")
        
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 0:
            st.subheader("‚è±Ô∏è Intervalos Entre Alertas")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("M√©dia (h)", f"{intervals.mean():.2f}")
            with col2:
                st.metric("Mediana (h)", f"{intervals.median():.2f}")
            with col3:
                st.metric("M√≠nimo (h)", f"{intervals.min():.2f}")
            with col4:
                st.metric("M√°ximo (h)", f"{intervals.max():.2f}")

    def show_individual_alert_analysis(self):
        st.header(f"üìå An√°lise Individual do Alert ID: {self.alert_id}")

        if self.df is None or len(self.df) == 0:
            st.info("Nenhum dado dispon√≠vel para este alerta.")
            return

        unique_days = self.df['date'].nunique()
        is_single_day = unique_days == 1

        df_isolated = self.df[self.df['is_isolated']]
        df_grouped = self.df[~self.df['is_isolated']]

        st.subheader("üìä Estat√≠sticas Gerais do Alert ID")
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        with col1:
            st.metric("Total Ocorr√™ncias", len(self.df))
        with col2:
            st.metric("üî¥ Isolados", len(df_isolated))
        with col3:
            st.metric("üü¢ Agrupados", len(df_grouped))
        with col4:
            st.metric("üì¶ N¬∫ de Grupos", len(self.groups_info))
        with col5:
            pct_isolated = (len(df_isolated) / len(self.df) * 100) if len(self.df) > 0 else 0
            st.metric("% Isolados", f"{pct_isolated:.1f}%")
        with col6:
            st.metric("üìÜ Dias √önicos", unique_days)

        if is_single_day:
            st.warning("‚ö†Ô∏è **ATEN√á√ÉO:** Todos os alertas ocorreram em apenas 1 dia! Este padr√£o √© classificado como ISOLADO.")
            st.info(f"üìÖ Data √∫nica: {self.df['date'].iloc[0]}")

        if len(self.groups_info) > 0:
            st.subheader("üì¶ Informa√ß√µes dos Grupos")
            groups_df = pd.DataFrame(self.groups_info)
            groups_df['start_time'] = pd.to_datetime(groups_df['start_time']).dt.strftime('%Y-%m-%d %H:%M')
            groups_df['end_time'] = pd.to_datetime(groups_df['end_time']).dt.strftime('%Y-%m-%d %H:%M')
            groups_df['duration_hours'] = groups_df['duration_hours'].round(2)
            groups_df.columns = ['ID Grupo', 'Tamanho', 'In√≠cio', 'Fim', 'Dura√ß√£o (h)']
            st.dataframe(groups_df, use_container_width=True)

        st.subheader("üìà Gr√°fico de Linhas: Alertas ao Longo do Tempo")

        df_daily = self.df.groupby(['date', 'is_isolated']).size().reset_index(name='count')
        df_daily_pivot = df_daily.pivot(index='date', columns='is_isolated', values='count').fillna(0)

        # CORRE√á√ÉO: Verificar quais colunas existem e renomear adequadamente
        new_column_names = {}
        if False in df_daily_pivot.columns:
            new_column_names[False] = 'Agrupados'
        if True in df_daily_pivot.columns:
            new_column_names[True] = 'Isolados'

        df_daily_pivot = df_daily_pivot.rename(columns=new_column_names)

        # Garantir que ambas as colunas existam (mesmo que com valores zero)
        if 'Agrupados' not in df_daily_pivot.columns:
            df_daily_pivot['Agrupados'] = 0
        if 'Isolados' not in df_daily_pivot.columns:
            df_daily_pivot['Isolados'] = 0

        fig_timeline = go.Figure()

        fig_timeline.add_trace(go.Scatter(
            x=df_daily_pivot.index,
            y=df_daily_pivot['Isolados'],
            mode='lines+markers',
            name='Isolados',
            line=dict(color='red', width=2),
            marker=dict(size=8),
            fill='tozeroy',
            fillcolor='rgba(255, 68, 68, 0.3)'
        ))

        fig_timeline.add_trace(go.Scatter(
            x=df_daily_pivot.index,
            y=df_daily_pivot['Agrupados'],
            mode='lines+markers',
            name='Agrupados',
            line=dict(color='green', width=2),
            marker=dict(size=8),
            fill='tozeroy',
            fillcolor='rgba(68, 255, 68, 0.3)'
        ))

        fig_timeline.update_layout(
            title="Evolu√ß√£o Di√°ria: Alertas Isolados vs Agrupados",
            xaxis_title="Data",
            yaxis_title="Quantidade de Alertas",
            hovermode='x unified',
            height=400
        )

        st.plotly_chart(fig_timeline, use_container_width=True, key='individual_line_chart')

        # ... resto do c√≥digo continua igual
    
        tab1, tab2, tab3 = st.tabs(["üî¥ Ocorr√™ncias Isoladas", "üü¢ Ocorr√™ncias Agrupadas", "üìä Visualiza√ß√£o Temporal"])
    
        with tab1:
            st.subheader(f"üî¥ Ocorr√™ncias Isoladas ({len(df_isolated)})")
            if len(df_isolated) > 0:
                isolated_display = df_isolated[['created_on', 'hour', 'day_name', 'time_diff_hours', 'date']].copy()
                isolated_display['created_on'] = isolated_display['created_on'].dt.strftime('%Y-%m-%d %H:%M:%S')
                isolated_display.columns = ['Data/Hora', 'Hora', 'Dia da Semana', 'Intervalo (h)', 'Data']
                st.dataframe(isolated_display, use_container_width=True)
                st.write(f"**Percentual:** {len(df_isolated)/len(self.df)*100:.2f}% das ocorr√™ncias s√£o isoladas")
                
                daily_counts = df_isolated.groupby('date').size().sort_values(ascending=False)
                if len(daily_counts) > 0:
                    st.write("**üìà Dias com Mais Alertas Isolados:**")
                    top_days = daily_counts.head(5)
                    for date, count in top_days.items():
                        st.write(f"‚Ä¢ {date}: {count} alertas")
            else:
                st.info("Nenhuma ocorr√™ncia isolada detectada neste alerta.")
    
        with tab2:
            st.subheader(f"üü¢ Ocorr√™ncias Agrupadas ({len(df_grouped)})")
            if len(df_grouped) > 0:
                grouped_display = df_grouped[['created_on', 'hour', 'day_name', 'time_diff_hours', 'group_id']].copy()
                grouped_display['created_on'] = grouped_display['created_on'].dt.strftime('%Y-%m-%d %H:%M:%S')
                grouped_display.columns = ['Data/Hora', 'Hora', 'Dia da Semana', 'Intervalo (h)', 'Grupo']
                st.dataframe(grouped_display, use_container_width=True)
                st.write(f"**Percentual:** {len(df_grouped)/len(self.df)*100:.2f}% das ocorr√™ncias est√£o agrupadas")
            else:
                st.info("Nenhuma ocorr√™ncia agrupada detectada neste alerta.")
        
        with tab3:
            st.subheader("üìä Visualiza√ß√£o Temporal dos Alertas")
            
            fig = go.Figure()
            
            if len(df_isolated) > 0:
                fig.add_trace(go.Scatter(
                    x=df_isolated['created_on'],
                    y=[1] * len(df_isolated),
                    mode='markers',
                    name='Isolados',
                    marker=dict(size=10, color='red', symbol='x'),
                    hovertemplate='%{x}<br>Isolado<extra></extra>'
                ))
            
            for group_info in self.groups_info:
                group_id = group_info['group_id']
                group_data = df_grouped[df_grouped['group_id'] == group_id]
                fig.add_trace(go.Scatter(
                    x=group_data['created_on'],
                    y=[1] * len(group_data),
                    mode='markers',
                    name=f'Grupo {group_id}',
                    marker=dict(size=10),
                    hovertemplate='%{x}<br>Grupo ' + str(group_id) + '<extra></extra>'
                ))
            
            fig.update_layout(
                title="Timeline de Alertas (Isolados vs Agrupados)",
                xaxis_title="Data/Hora",
                yaxis=dict(showticklabels=False, title=""),
                height=400,
                hovermode='closest'
            )
            st.plotly_chart(fig, use_container_width=True, key='individual_alert_timeline')

    # ADICIONE ESTE M√âTODO DENTRO DA CLASSE StreamlitAlertAnalyzer
# Substitua o m√©todo analyze_advanced_recurrence_patterns existente

    def show_advanced_recurrence_analysis(self):
        """
        M√©todo corrigido para an√°lise avan√ßada de recorr√™ncia individual.
        Deve ser chamado em uma tab da an√°lise individual.
        """
        st.header(f"üî¨ An√°lise Avan√ßada de Recorr√™ncia - Alert ID: {self.alert_id}")

        if self.df is None or len(self.df) == 0:
            st.warning("‚ö†Ô∏è Sem dados para an√°lise")
            return

        df = self.df.copy()

        # ============================================================
        # 1. AN√ÅLISE POR MINUTO (GRANULARIDADE M√ÅXIMA)
        # ============================================================
        st.subheader("üïê Padr√£o por Minuto do Dia")

        df['hour_minute'] = df['created_on'].dt.hour * 60 + df['created_on'].dt.minute
        minute_dist = df['hour_minute'].value_counts().sort_index()

        # Identificar top 10 minutos
        top_10_minutes = minute_dist.nlargest(10)
        total_top_10 = top_10_minutes.sum()
        pct_top_10 = (total_top_10 / len(df)) * 100

        col1, col2, col3 = st.columns(3)
        with col1:
            top_minute = top_10_minutes.index[0]
            st.metric("üéØ Minuto Mais Frequente", 
                     f"{top_minute//60:02d}:{top_minute%60:02d}",
                     f"{top_10_minutes.values[0]} alertas")
        with col2:
            st.metric("üìä Top 10 Minutos", f"{pct_top_10:.1f}%",
                     f"{total_top_10} alertas")
        with col3:
            unique_minutes = len(minute_dist)
            st.metric("üî¢ Minutos √önicos", unique_minutes,
                     f"de 1440 poss√≠veis")

        # Gr√°fico de distribui√ß√£o por minuto (agregado por hora)
        df['hour_bin'] = df['hour_minute'] // 60
        hourly_counts = df.groupby('hour_bin').size()

        fig_minutes = go.Figure()
        fig_minutes.add_trace(go.Scatter(
            x=hourly_counts.index,
            y=hourly_counts.values,
            mode='lines+markers',
            fill='tozeroy',
            name='Alertas por hora'
        ))
        fig_minutes.update_layout(
            title="Distribui√ß√£o Temporal (agregada por hora)",
            xaxis_title="Hora do Dia",
            yaxis_title="Quantidade de Alertas",
            height=300
        )
        st.plotly_chart(fig_minutes, use_container_width=True, key='adv_minutes_dist')

        with st.expander("üîù Top 20 Minutos Mais Frequentes"):
            top_20 = minute_dist.nlargest(20)
            minute_data = []
            for minute, count in top_20.items():
                pct = (count / len(df)) * 100
                minute_data.append({
                    'Hor√°rio': f"{minute//60:02d}:{minute%60:02d}",
                    'Alertas': count,
                    '% do Total': f"{pct:.2f}%"
                })
            st.dataframe(pd.DataFrame(minute_data), use_container_width=True)

        # ============================================================
        # 2. AN√ÅLISE POR HORA COM DETEC√á√ÉO DE PADR√ïES
        # ============================================================
        st.markdown("---")
        st.subheader("‚è∞ An√°lise Detalhada por Hora")

        hourly_dist = df['hour'].value_counts().sort_index()
        hourly_pct = (hourly_dist / len(df) * 100).round(2)

        # Calcular estat√≠sticas
        mean_per_hour = hourly_dist.mean()
        std_per_hour = hourly_dist.std()
        cv_hourly = (std_per_hour / mean_per_hour) if mean_per_hour > 0 else 0

        # Detectar padr√µes
        top_3_hours = hourly_pct.nlargest(3)
        top_5_hours = hourly_pct.nlargest(5)
        pct_top_3 = top_3_hours.sum()
        pct_top_5 = top_5_hours.sum()

        # Classificar padr√£o hor√°rio
        if pct_top_3 > 70:
            pattern_hourly = "üî¥ ALTAMENTE CONCENTRADO"
            pattern_desc = "Alertas extremamente concentrados em poucos hor√°rios"
        elif pct_top_3 > 50:
            pattern_hourly = "üü† MUITO CONCENTRADO"
            pattern_desc = "Forte concentra√ß√£o em hor√°rios espec√≠ficos"
        elif pct_top_5 > 50:
            pattern_hourly = "üü° MODERADAMENTE CONCENTRADO"
            pattern_desc = "Concentra√ß√£o moderada em alguns hor√°rios"
        elif cv_hourly < 0.3:
            pattern_hourly = "üü¢ UNIFORME"
            pattern_desc = "Distribui√ß√£o uniforme ao longo do dia"
        else:
            pattern_hourly = "üîµ DISTRIBU√çDO"
            pattern_desc = "Bem distribu√≠do com varia√ß√£o normal"

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä Padr√£o Identificado", pattern_hourly)
        with col2:
            st.metric("üéØ Top 3 Horas", f"{pct_top_3:.1f}%")
        with col3:
            st.metric("üìà Top 5 Horas", f"{pct_top_5:.1f}%")
        with col4:
            st.metric("üìâ Coef. Varia√ß√£o", f"{cv_hourly:.3f}")

        st.info(f"**Interpreta√ß√£o:** {pattern_desc}")

        # Gr√°fico de barras com destaque
        fig_hourly = go.Figure()
        colors = ['red' if i in top_3_hours.index else 'lightblue' for i in hourly_dist.index]
        fig_hourly.add_trace(go.Bar(
            x=hourly_dist.index,
            y=hourly_dist.values,
            marker_color=colors,
            text=hourly_pct.values,
            texttemplate='%{text:.1f}%',
            textposition='outside',
            hovertemplate='Hora: %{x}:00<br>Alertas: %{y}<br>%{text}<extra></extra>'
        ))
        fig_hourly.update_layout(
            title="Distribui√ß√£o por Hora (Top 3 em vermelho)",
            xaxis_title="Hora",
            yaxis_title="Quantidade",
            height=400,
            showlegend=False
        )
        st.plotly_chart(fig_hourly, use_container_width=True, key='adv_hourly_dist')

        # An√°lise de per√≠odos do dia
        st.write("**üìä An√°lise por Per√≠odos do Dia:**")
        periods = {
            'Madrugada (00-05h)': df[df['hour'].between(0, 5)],
            'Manh√£ (06-11h)': df[df['hour'].between(6, 11)],
            'Tarde (12-17h)': df[df['hour'].between(12, 17)],
            'Noite (18-23h)': df[df['hour'].between(18, 23)]
        }

        period_data = []
        for period_name, period_df in periods.items():
            count = len(period_df)
            pct = (count / len(df)) * 100
            period_data.append({
                'Per√≠odo': period_name,
                'Alertas': count,
                '% do Total': f"{pct:.1f}%"
            })

        period_df_display = pd.DataFrame(period_data)
        col1, col2 = st.columns([1, 1])
        with col1:
            st.dataframe(period_df_display, use_container_width=True)
        with col2:
            fig_periods = px.pie(period_df_display, values='Alertas', names='Per√≠odo',
                                title="Distribui√ß√£o por Per√≠odo")
            st.plotly_chart(fig_periods, use_container_width=True, key='adv_periods')

        # ============================================================
        # 3. AN√ÅLISE POR DIA DA SEMANA
        # ============================================================
        st.markdown("---")
        st.subheader("üìÖ An√°lise Detalhada por Dia da Semana")

        days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        day_translation = {
            'Monday': 'Segunda', 'Tuesday': 'Ter√ßa', 'Wednesday': 'Quarta',
            'Thursday': 'Quinta', 'Friday': 'Sexta', 'Saturday': 'S√°bado', 'Sunday': 'Domingo'
        }

        daily_dist = df['day_name'].value_counts().reindex(days_order).fillna(0)
        daily_pct = (daily_dist / len(df) * 100).round(2)

        # Estat√≠sticas
        mean_per_day = daily_dist.mean()
        std_per_day = daily_dist.std()
        cv_daily = (std_per_day / mean_per_day) if mean_per_day > 0 else 0

        top_3_days = daily_pct.nlargest(3)
        pct_top_3_days = top_3_days.sum()

        # An√°lise fim de semana vs dias √∫teis
        weekday_count = daily_dist[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']].sum()
        weekend_count = daily_dist[['Saturday', 'Sunday']].sum()
        weekday_pct = (weekday_count / len(df)) * 100
        weekend_pct = (weekend_count / len(df)) * 100

        # Classificar padr√£o semanal
        if pct_top_3_days > 70:
            pattern_weekly = "üî¥ ALTAMENTE CONCENTRADO"
            pattern_desc_weekly = "Concentra√ß√£o extrema em poucos dias"
        elif pct_top_3_days > 50:
            pattern_weekly = "üü† CONCENTRADO"
            pattern_desc_weekly = "Forte concentra√ß√£o em dias espec√≠ficos"
        elif weekend_pct > 40:
            pattern_weekly = "üîµ PADR√ÉO FIM DE SEMANA"
            pattern_desc_weekly = "Alta atividade em fins de semana"
        elif weekday_pct > 85:
            pattern_weekly = "üíº PADR√ÉO DIAS √öTEIS"
            pattern_desc_weekly = "Predominante em dias √∫teis"
        elif cv_daily < 0.2:
            pattern_weekly = "üü¢ UNIFORME"
            pattern_desc_weekly = "Distribui√ß√£o uniforme na semana"
        else:
            pattern_weekly = "üîµ DISTRIBU√çDO"
            pattern_desc_weekly = "Distribui√ß√£o variada"

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä Padr√£o Semanal", pattern_weekly)
        with col2:
            st.metric("üéØ Top 3 Dias", f"{pct_top_3_days:.1f}%")
        with col3:
            st.metric("üíº Dias √öteis", f"{weekday_pct:.1f}%")
        with col4:
            st.metric("üóìÔ∏è Fins de Semana", f"{weekend_pct:.1f}%")

        st.info(f"**Interpreta√ß√£o:** {pattern_desc_weekly}")

        # Gr√°fico
        daily_pct_pt = daily_pct.rename(index=day_translation)
        colors_daily = ['red' if day in [day_translation[d] for d in top_3_days.index] else 'lightgreen' 
                        for day in daily_pct_pt.index]

        fig_daily = go.Figure()
        fig_daily.add_trace(go.Bar(
            x=list(daily_pct_pt.index),
            y=daily_dist.values,
            marker_color=colors_daily,
            text=daily_pct_pt.values,
            texttemplate='%{text:.1f}%',
            textposition='outside'
        ))
        fig_daily.update_layout(
            title="Distribui√ß√£o por Dia da Semana (Top 3 em vermelho)",
            xaxis_title="Dia",
            yaxis_title="Quantidade",
            height=400
        )
        st.plotly_chart(fig_daily, use_container_width=True, key='adv_daily_dist')

        # ============================================================
        # 4. AN√ÅLISE POR DIA DO M√äS
        # ============================================================
        st.markdown("---")
        st.subheader("üìÜ An√°lise por Dia do M√™s")

        df['day_of_month'] = df['created_on'].dt.day
        dom_dist = df['day_of_month'].value_counts().sort_index()
        dom_pct = (dom_dist / len(df) * 100).round(2)

        top_5_dom = dom_pct.nlargest(5)
        pct_top_5_dom = top_5_dom.sum()

        # Detectar padr√µes mensais
        inicio_mes = dom_dist[dom_dist.index <= 5].sum()
        meio_mes = dom_dist[dom_dist.index.isin(range(11, 21))].sum()
        fim_mes = dom_dist[dom_dist.index >= 26].sum()

        inicio_pct = (inicio_mes / len(df)) * 100
        meio_pct = (meio_mes / len(df)) * 100
        fim_pct = (fim_mes / len(df)) * 100

        if fim_pct > 30:
            pattern_monthly = "üìÖ PADR√ÉO FIM DO M√äS"
            pattern_desc_monthly = "Concentra√ß√£o no fim do m√™s"
        elif inicio_pct > 30:
            pattern_monthly = "üìÖ PADR√ÉO IN√çCIO DO M√äS"
            pattern_desc_monthly = "Concentra√ß√£o no in√≠cio do m√™s"
        elif meio_pct > 30:
            pattern_monthly = "üìÖ PADR√ÉO MEIO DO M√äS"
            pattern_desc_monthly = "Concentra√ß√£o no meio do m√™s"
        elif pct_top_5_dom > 40:
            pattern_monthly = "üü° DIAS ESPEC√çFICOS"
            pattern_desc_monthly = "Concentra√ß√£o em dias espec√≠ficos do m√™s"
        else:
            pattern_monthly = "üü¢ DISTRIBU√çDO"
            pattern_desc_monthly = "Sem padr√£o mensal claro"

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä Padr√£o Mensal", pattern_monthly)
        with col2:
            st.metric("üìÖ In√≠cio (1-5)", f"{inicio_pct:.1f}%")
        with col3:
            st.metric("üìÖ Meio (11-20)", f"{meio_pct:.1f}%")
        with col4:
            st.metric("üìÖ Fim (26+)", f"{fim_pct:.1f}%")

        st.info(f"**Interpreta√ß√£o:** {pattern_desc_monthly}")

        fig_dom = go.Figure()
        colors_dom = ['red' if i in top_5_dom.index else 'lightcoral' for i in dom_dist.index]
        fig_dom.add_trace(go.Bar(
            x=dom_dist.index,
            y=dom_dist.values,
            marker_color=colors_dom,
            hovertemplate='Dia: %{x}<br>Alertas: %{y}<extra></extra>'
        ))
        fig_dom.update_layout(
            title="Distribui√ß√£o por Dia do M√™s (Top 5 em vermelho)",
            xaxis_title="Dia do M√™s",
            yaxis_title="Quantidade",
            height=400
        )
        st.plotly_chart(fig_dom, use_container_width=True, key='adv_dom_dist')

        # ============================================================
        # 5. MAPA DE CALOR E HOTSPOTS
        # ============================================================
        st.markdown("---")
        st.subheader("üî• Mapa de Calor e Hotspots")

        # Identificar os 10 maiores hotspots
        st.write("**üî• Top 10 Hotspots (Hora + Dia):**")
        hotspot_data = []
        day_map = {0: 'Seg', 1: 'Ter', 2: 'Qua', 3: 'Qui', 4: 'Sex', 5: 'S√°b', 6: 'Dom'}

        for _, row in df.iterrows():
            day_idx = row['day_of_week']
            hour = row['hour']
            hotspot_data.append({
                'Dia': day_map[day_idx],
                'Hora': hour,
                'Count': 1
            })

        hotspot_df = pd.DataFrame(hotspot_data).groupby(['Dia', 'Hora']).sum().reset_index()
        hotspot_df['% Total'] = (hotspot_df['Count'] / len(df) * 100).round(2)
        hotspot_df = hotspot_df.sort_values('Count', ascending=False).head(10)
        hotspot_df['Hora'] = hotspot_df['Hora'].apply(lambda x: f"{x:02d}:00")
        hotspot_df.columns = ['Dia', 'Hora', 'Alertas', '% Total']
        hotspot_df['% Total'] = hotspot_df['% Total'].astype(str) + '%'
        st.dataframe(hotspot_df, use_container_width=True)

        # ============================================================
        # 6. RESUMO FINAL E CLASSIFICA√á√ÉO
        # ============================================================
        st.markdown("---")
        st.header("üéØ RESUMO FINAL DO PADR√ÉO DE RECORR√äNCIA")

        # Calcular score de concentra√ß√£o geral
        concentration_score = (pct_top_3 * 0.4 + pct_top_3_days * 0.3 + pct_top_5_dom * 0.3)

        if concentration_score > 60:
            overall_pattern = "üî¥ PADR√ÉO ALTAMENTE PREVIS√çVEL"
            overall_desc = "Forte concentra√ß√£o temporal - alto potencial de automa√ß√£o"
            automation_potential = "ALTO"
        elif concentration_score > 40:
            overall_pattern = "üü° PADR√ÉO MODERADAMENTE PREVIS√çVEL"
            overall_desc = "Padr√£o identific√°vel com alguma varia√ß√£o"
            automation_potential = "M√âDIO"
        else:
            overall_pattern = "üü¢ PADR√ÉO VARI√ÅVEL/DISTRIBU√çDO"
            overall_desc = "Distribui√ß√£o ampla sem forte concentra√ß√£o"
            automation_potential = "BAIXO"

        st.success(f"### {overall_pattern}")
        st.info(overall_desc)

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Score de Concentra√ß√£o", f"{concentration_score:.1f}/100")
        with col2:
            st.metric("Potencial de Automa√ß√£o", automation_potential)
        with col3:
            st.metric("Alertas Analisados", len(df))

        # Tabela resumo
        st.write("**üìä Resumo dos Padr√µes Identificados:**")
        summary_data = {
            'Dimens√£o': ['Hora do Dia', 'Dia da Semana', 'Dia do M√™s', 'Minuto Espec√≠fico'],
            'Padr√£o': [pattern_hourly, pattern_weekly, pattern_monthly, 
                       f"Top 10: {pct_top_10:.1f}%"],
            'Principal': [
                f"{top_3_hours.index[0]:02d}:00 ({top_3_hours.values[0]:.1f}%)",
                f"{day_translation[top_3_days.index[0]]} ({top_3_days.values[0]:.1f}%)",
                f"Dia {top_5_dom.index[0]} ({top_5_dom.values[0]:.1f}%)",
                f"{top_minute//60:02d}:{top_minute%60:02d} ({top_10_minutes.values[0]} alertas)"
            ]
        }
        st.dataframe(pd.DataFrame(summary_data), use_container_width=True)

        # Recomenda√ß√µes
        st.markdown("---")
        st.subheader("üí° Recomenda√ß√µes Baseadas nos Padr√µes")

        if concentration_score > 60:
            st.success("‚úÖ **Alta Previsibilidade Detectada:**")
            st.write("‚Ä¢ Implementar janelas de manuten√ß√£o preventiva nos hor√°rios de pico")
            st.write("‚Ä¢ Considerar automa√ß√£o de resposta para padr√µes recorrentes")
            st.write(f"‚Ä¢ Focar aten√ß√£o em: {day_translation[top_3_days.index[0]]} √†s {top_3_hours.index[0]:02d}:00")
        elif concentration_score > 40:
            st.warning("‚ö†Ô∏è **Padr√£o Moderado Detectado:**")
            st.write("‚Ä¢ Monitorar hor√°rios de maior incid√™ncia")
            st.write("‚Ä¢ Avaliar causas subjacentes aos padr√µes identificados")
        else:
            st.info("‚ÑπÔ∏è **Padr√£o Distribu√≠do:**")
            st.write("‚Ä¢ Alertas bem distribu√≠dos - baixa previsibilidade temporal")
            st.write("‚Ä¢ Focar em an√°lise de causa raiz ao inv√©s de padr√µes temporais")

    def show_temporal_patterns(self):
        st.header("‚è∞ Padr√µes Temporais")
        col1, col2 = st.columns(2)
        with col1:
            hourly = self.df['hour'].value_counts().sort_index()
            fig_hour = px.bar(
                x=hourly.index, 
                y=hourly.values,
                title="üìä Distribui√ß√£o por Hora do Dia",
                labels={'x': 'Hora', 'y': 'Quantidade de Alertas'}
            )
            fig_hour.update_layout(showlegend=False)
            st.plotly_chart(fig_hour, use_container_width=True, key='hourly_dist')
            peak_hour = hourly.idxmax()
            quiet_hour = hourly.idxmin()
            st.write(f"üïê **Pico:** {peak_hour:02d}:00 ({hourly[peak_hour]} alertas)")
            st.write(f"üåô **Menor atividade:** {quiet_hour:02d}:00 ({hourly[quiet_hour]} alertas)")
        with col2:
            daily = self.df['day_name'].value_counts()
            days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
            daily_ordered = daily.reindex(days_order).fillna(0)
            fig_day = px.bar(
                x=daily_ordered.index, 
                y=daily_ordered.values,
                title="üìÖ Distribui√ß√£o por Dia da Semana",
                labels={'x': 'Dia', 'y': 'Quantidade de Alertas'}
            )
            fig_day.update_layout(showlegend=False)
            st.plotly_chart(fig_day, use_container_width=True, key='daily_dist')
            busiest_day = daily.idxmax()
            st.write(f"üìà **Dia mais ativo:** {busiest_day} ({daily[busiest_day]} alertas)")
        col1, col2 = st.columns(2)
        with col1:
            business = self.df['is_business_hours'].sum()
            non_business = len(self.df) - business
            st.subheader("üè¢ Hor√°rio Comercial (9h-17h)")
            business_data = pd.DataFrame({
                'Per√≠odo': ['Comercial', 'Fora do hor√°rio'],
                'Quantidade': [business, non_business],
                'Porcentagem': [business/len(self.df)*100, non_business/len(self.df)*100]
            })
            fig_business = px.pie(
                business_data, 
                values='Quantidade', 
                names='Per√≠odo',
                title="Distribui√ß√£o por Hor√°rio"
            )
            st.plotly_chart(fig_business, use_container_width=True, key='business_hours_pie')
        with col2:
            weekend = self.df['is_weekend'].sum()
            weekday = len(self.df) - weekend
            st.subheader("üóìÔ∏è Fins de Semana vs Dias √öteis")
            weekend_data = pd.DataFrame({
                'Per√≠odo': ['Dias √∫teis', 'Fins de semana'],
                'Quantidade': [weekday, weekend],
                'Porcentagem': [weekday/len(self.df)*100, weekend/len(self.df)*100]
            })
            fig_weekend = px.pie(
                weekend_data, 
                values='Quantidade', 
                names='Per√≠odo',
                title="Distribui√ß√£o Semanal"
            )
            st.plotly_chart(fig_weekend, use_container_width=True, key='weekend_pie')

    def show_burst_analysis(self):
        st.header("üí• An√°lise de Rajadas")
        burst_threshold = st.slider("‚è±Ô∏è Threshold para Rajada (horas)", 0.5, 24.0, 2.0, 0.5)
        intervals = self.df[~self.df['is_isolated']]['time_diff_hours'].fillna(999)
        bursts, current_burst = [], []
        for i, interval in enumerate(intervals):
            if interval <= burst_threshold and i > 0:
                if not current_burst:
                    current_burst = [i-1, i]
                else:
                    current_burst.append(i)
            else:
                if len(current_burst) >= 2:
                    bursts.append(current_burst)
                current_burst = []
        if len(current_burst) >= 2:
            bursts.append(current_burst)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("üö® Rajadas Detectadas", len(bursts))
        if bursts:
            burst_sizes = [len(b) for b in bursts]
            with col2:
                st.metric("üìä Tamanho M√©dio", f"{np.mean(burst_sizes):.1f}")
            with col3:
                st.metric("üìà Maior Rajada", f"{max(burst_sizes)} alertas")
            st.subheader("üî• Maiores Rajadas")
            sorted_bursts = sorted(bursts, key=len, reverse=True)[:5]
            burst_data = []
            for i, burst_indices in enumerate(sorted_bursts):
                start_time = self.df.iloc[burst_indices[0]]['created_on']
                end_time = self.df.iloc[burst_indices[-1]]['created_on']
                duration = end_time - start_time
                burst_data.append({
                    'Rajada': f"#{i+1}",
                    'Alertas': len(burst_indices),
                    'In√≠cio': start_time.strftime("%d/%m/%Y %H:%M"),
                    'Fim': end_time.strftime("%d/%m/%Y %H:%M"),
                    'Dura√ß√£o': str(duration)
                })
            st.dataframe(pd.DataFrame(burst_data), use_container_width=True)

    def show_trend_analysis(self):
        st.header("üìà An√°lise de Tend√™ncias")
        daily_counts = self.df.groupby('date').size()
        if len(daily_counts) >= 7:
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=daily_counts.index,
                y=daily_counts.values,
                mode='lines+markers',
                name='Alertas por dia',
                line=dict(color='blue')
            ))
            x_numeric = np.arange(len(daily_counts))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x_numeric, daily_counts.values)
            trend_line = slope * x_numeric + intercept
            fig.add_trace(go.Scatter(
                x=daily_counts.index,
                y=trend_line,
                mode='lines',
                name='Tend√™ncia',
                line=dict(color='red', dash='dash')
            ))
            fig.update_layout(
                title="üìä Evolu√ß√£o Temporal dos Alertas",
                xaxis_title="Data",
                yaxis_title="N√∫mero de Alertas",
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True, key='trend_analysis')
            if slope > 0.01:
                trend = "CRESCENTE üìà"
            elif slope < -0.01:
                trend = "DECRESCENTE üìâ"
            else:
                trend = "EST√ÅVEL ‚û°Ô∏è"
            strength = "FORTE" if abs(r_value) > 0.7 else "MODERADA" if abs(r_value) > 0.3 else "FRACA"
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üéØ Tend√™ncia", trend)
            with col2:
                st.metric("üí™ For√ßa", strength)
            with col3:
                st.metric("üìä Correla√ß√£o", f"{r_value:.4f}")
            with col4:
                st.metric("‚ö° Taxa/dia", f"{slope:.4f}")
        else:
            st.warning("‚ö†Ô∏è Poucos dados para an√°lise de tend√™ncia (m√≠nimo 7 dias)")

    def show_anomaly_detection(self):
        st.header("üö® Detec√ß√£o de Anomalias")
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 4:
            Q1 = intervals.quantile(0.25)
            Q3 = intervals.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            fast_anomalies = intervals[intervals < lower_bound]
            slow_anomalies = intervals[intervals > upper_bound]
            normal_intervals = intervals[(intervals >= lower_bound) & (intervals <= upper_bound)]
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("‚ö° Intervalos Curtos", len(fast_anomalies))
            with col2:
                st.metric("üêå Intervalos Longos", len(slow_anomalies))
            with col3:
                st.metric("‚úÖ Intervalos Normais", len(normal_intervals))
            fig = go.Figure()
            fig.add_trace(go.Box(
                y=intervals,
                name="Intervalos (horas)",
                boxpoints='outliers'
            ))
            fig.update_layout(
                title="üìä Distribui√ß√£o dos Intervalos (Detec√ß√£o de Outliers)",
                yaxis_title="Horas"
            )
            st.plotly_chart(fig, use_container_width=True, key='anomaly_boxplot')
            if len(fast_anomalies) > 0 or len(slow_anomalies) > 0:
                col1, col2 = st.columns(2)
                with col1:
                    if len(fast_anomalies) > 0:
                        st.subheader("‚ö° Intervalos Muito Curtos")
                        st.write(f"Menor intervalo: **{fast_anomalies.min():.2f} horas**")
                        st.write(f"M√©dia dos curtos: **{fast_anomalies.mean():.2f} horas**")
                with col2:
                    if len(slow_anomalies) > 0:
                        st.subheader("üêå Intervalos Muito Longos")
                        st.write(f"Maior intervalo: **{slow_anomalies.max():.2f} horas**")
                        st.write(f"M√©dia dos longos: **{slow_anomalies.mean():.2f} horas**")
        else:
            st.warning("‚ö†Ô∏è Poucos dados para detec√ß√£o de anomalias")

    def show_predictions(self):
        st.header("üîÆ Insights Preditivos")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("‚è∞ Hor√°rios de Maior Probabilidade")
            hourly_prob = self.df['hour'].value_counts(normalize=True).sort_values(ascending=False)
            prob_data = [{'Hor√°rio': f"{hour:02d}:00", 'Probabilidade': f"{prob*100:.1f}%"} for hour, prob in hourly_prob.head(5).items()]
            st.dataframe(pd.DataFrame(prob_data), use_container_width=True)
        with col2:
            st.subheader("üìÖ Dias de Maior Probabilidade")
            daily_prob = self.df['day_name'].value_counts(normalize=True).sort_values(ascending=False)
            day_data = [{'Dia': day, 'Probabilidade': f"{prob*100:.1f}%"} for day, prob in daily_prob.items()]
            st.dataframe(pd.DataFrame(day_data), use_container_width=True)
        st.subheader("‚è±Ô∏è Previs√£o do Pr√≥ximo Alerta")
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 0:
            avg_interval = intervals.mean()
            median_interval = intervals.median()
            last_alert = self.dates.max()
            next_avg = last_alert + timedelta(hours=avg_interval)
            next_median = last_alert + timedelta(hours=median_interval)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üïê √öltimo Alerta", last_alert.strftime("%d/%m %H:%M"))
            with col2:
                st.metric("üìä Pr√≥ximo (M√©dia)", next_avg.strftime("%d/%m %H:%M"))
            with col3:
                st.metric("üìà Pr√≥ximo (Mediana)", next_median.strftime("%d/%m %H:%M"))
            st.info(f"üí° **Baseado em:** Intervalo m√©dio de {avg_interval:.1f}h e mediana de {median_interval:.1f}h")

def main():
    st.title("üö® Analisador de Alertas")
    st.markdown("### An√°lise individual, global e agrupamento inteligente de alertas")
    st.sidebar.header("‚öôÔ∏è Configura√ß√µes")
    
    with st.sidebar.expander("üéõÔ∏è Par√¢metros de Agrupamento", expanded=False):
        max_gap_hours = st.slider(
            "‚è±Ô∏è Gap M√°ximo Entre Alertas (horas)",
            min_value=1,
            max_value=72,
            value=24,
            help="Alertas separados por mais tempo que isso s√£o considerados de grupos diferentes"
        )
        min_group_size = st.slider(
            "üìä Tamanho M√≠nimo do Grupo",
            min_value=2,
            max_value=10,
            value=3,
            help="N√∫mero m√≠nimo de alertas para formar um grupo v√°lido"
        )
        spike_threshold_multiplier = st.slider(
            "üöÄ Multiplicador de Spike",
            min_value=2.0,
            max_value=10.0,
            value=5.0,
            step=0.5,
            help="Dias com mais alertas que m√©dia √ó este valor s√£o considerados spikes isolados"
        )
    
    analysis_mode = st.sidebar.selectbox(
        "üéØ Modo de An√°lise",
        ["üåç An√°lise Global", "üîç An√°lise Individual"],
        help="Escolha entre analisar todos os alertas ou um alerta espec√≠fico"
    )
    uploaded_file = st.sidebar.file_uploader(
        "üìÅ Upload do arquivo CSV",
        type=['csv'],
        help="Fa√ßa upload do arquivo CSV contendo os dados dos alertas"
    )
    
    if uploaded_file is not None:
        analyzer = StreamlitAlertAnalyzer()
        if analyzer.load_data(uploaded_file):
            if analysis_mode == "üåç An√°lise Global":
                st.markdown("---")
                use_multiprocessing = st.sidebar.checkbox(
                    "‚ö° Usar Multiprocessing (Mais R√°pido)", 
                    value=True,
                    help="Processa alertas em paralelo para melhor desempenho"
                )
                if st.sidebar.button("üöÄ Executar An√°lise Global", type="primary"):
                    if analyzer.prepare_global_analysis(use_multiprocessing, max_gap_hours, 
                                                       min_group_size, spike_threshold_multiplier):
                        # MODIFICADO: Adicionada nova aba "Grupos Detalhados"
                        tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
                            "üìä Vis√£o Geral",
                            "üîç Isolados vs Cont√≠nuos",
                            "üî¨ Grupos Detalhados",  # NOVA ABA
                            "üîÅ Recorr√™ncia",
                            "üéØ Agrupamento", 
                            "üë• Perfis dos Clusters",
                            "üí° Recomenda√ß√µes"
                        ])
                        with tab1:
                            analyzer.show_global_overview()
                        with tab2:
                            analyzer.show_isolated_vs_continuous_analysis()
                        with tab3:
                            # NOVA VISUALIZA√á√ÉO DETALHADA DOS GRUPOS
                            analyzer.show_continuous_groups_detailed_view()
                        with tab4:
                            analyzer.analyze_continuous_recurrence_patterns()
                        with tab5:
                            n_clusters = analyzer.perform_clustering_analysis()
                        with tab6:
                            if n_clusters:
                                analyzer.show_cluster_profiles(n_clusters)
                        with tab7:
                            if n_clusters:
                                analyzer.show_cluster_recommendations()
                        st.sidebar.markdown("---")
                        st.sidebar.subheader("üì• Downloads")
                        csv_buffer = io.StringIO()
                        analyzer.df_all_alerts.to_csv(csv_buffer, index=False)
                        st.sidebar.download_button(
                            label="‚¨áÔ∏è Baixar An√°lise Global",
                            data=csv_buffer.getvalue(),
                            file_name=f"analise_global_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.error("‚ùå N√£o foi poss√≠vel processar os dados para an√°lise global")
            else:
                try:
                    id_counts = analyzer.df_original['u_alert_id'].value_counts()

                    id_options = [f"{uid} ({count} ocorr√™ncias)" for uid, count in id_counts.items()]

                    # Cria o selectbox no sidebar
                    selected_option = st.sidebar.selectbox(
                        "üéØ Selecione o Alert ID",
                        id_options,
                        help="Escolha o ID do alerta para an√°lise (ordenado por frequ√™ncia)"
                    )

                    # Extrai o ID puro (antes do par√™ntese)
                    selected_id = selected_option.split(" ")[0]

                    if st.sidebar.button("üöÄ Executar An√°lise Individual", type="primary"):
                        analyzer.max_gap_hours = max_gap_hours
                        analyzer.min_group_size = min_group_size
                        analyzer.spike_threshold_multiplier = spike_threshold_multiplier

                        if analyzer.prepare_individual_analysis(selected_id):
                            st.success(f"üéØ Analisando alert_id: {selected_id} ({len(analyzer.df)} registros)")
                            st.info(f"üìÖ **Per√≠odo analisado:** {analyzer.dates.min()} at√© {analyzer.dates.max()}")

                            tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9 = st.tabs([
                                "üîç Isolados vs Agrupados",
                                "üìä B√°sico", 
                                "‚è∞ Temporais", 
                                "üîÅ Recorr√™ncia",
                                "üî¨ An√°lise Avan√ßada de Recorr√™ncia",
                                "üí• Rajadas", 
                                "üìà Tend√™ncias", 
                                "üö® Anomalias", 
                                "üîÆ Previs√µes"
                            ])

                            with tab1:
                                analyzer.show_individual_alert_analysis()
                            with tab2:
                                analyzer.show_basic_stats()
                            with tab3:
                                analyzer.show_temporal_patterns()
                            with tab4:
                                analyzer.analyze_individual_recurrence_patterns()
                            with tab5:
                                analyzer.show_advanced_recurrence_analysis()
                            with tab6:
                                analyzer.show_burst_analysis()
                            with tab7:
                                analyzer.show_trend_analysis()
                            with tab8:
                                analyzer.show_anomaly_detection()
                            with tab9:
                                analyzer.show_predictions()

                            st.sidebar.markdown("---")
                            st.sidebar.subheader("üì• Download")

                            csv_buffer = io.StringIO()
                            analyzer.df.to_csv(csv_buffer, index=False)
                            st.sidebar.download_button(
                                label="‚¨áÔ∏è Baixar Dados Processados",
                                data=csv_buffer.getvalue(),
                                file_name=f"analise_{selected_id}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.error(f"‚ùå Nenhum registro encontrado para alert_id: {selected_id}")
                except Exception as e:
                    st.error(f"‚ùå Erro ao processar an√°lise individual: {e}")
    else:
        st.info("üëÜ Fa√ßa upload de um arquivo CSV para come√ßar a an√°lise")
        with st.expander("üìñ Instru√ß√µes de Uso"):
            st.markdown("""
            ### Como usar este analisador:
            
            #### üåç **An√°lise Global**
            Analise todos os alertas com 7 abas:
            1. **Vis√£o Geral:** Top alertas e distribui√ß√µes
            2. **Isolados vs Cont√≠nuos:** Compara√ß√£o detalhada com gr√°fico temporal
            3. **üÜï Grupos Detalhados:** Visualiza√ß√£o interativa dos grupos identificados em alertas cont√≠nuos
            4. **Recorr√™ncia:** Padr√µes de hora/dia APENAS de alertas cont√≠nuos
            5. **Agrupamento:** Clustering por comportamento
            6. **Perfis:** Caracter√≠sticas de cada cluster
            7. **Recomenda√ß√µes:** A√ß√µes sugeridas
            
            #### üîç **An√°lise Individual**
            Analise um alerta espec√≠fico em 7 abas detalhadas
            
            ### Principais Melhorias:
            - ‚ú® **Nova aba "Grupos Detalhados"** com visualiza√ß√£o timeline dos grupos
            - üìä Sele√ß√£o interativa de at√© 5 alertas para an√°lise detalhada
            - üìà Gr√°ficos de tamanho e dura√ß√£o de cada grupo
            - ‚è±Ô∏è An√°lise de intervalos entre grupos
            - üìâ Resumo estat√≠stico de todos os grupos
            
            ### Colunas necess√°rias no CSV:
            - `u_alert_id`: Identificador √∫nico do alerta
            - `created_on`: Data e hora da cria√ß√£o do alerta
            """)

if __name__ == "__main__":
    main()