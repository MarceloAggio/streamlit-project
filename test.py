import pandas as pd
import streamlit as st
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import io
import warnings
from multiprocessing import Pool, cpu_count
from functools import partial

warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="Analisador de Alertas - Completo",
    page_icon="ğŸš¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

def identify_alert_groups(alert_data, max_gap_hours=24, min_group_size=3, 
                         spike_threshold_multiplier=5):
    """
    Identifica grupos/sessÃµes de alertas baseado em intervalos de tempo.
    Alertas isolados sÃ£o aqueles que nÃ£o pertencem a nenhum grupo significativo.
    """
    if len(alert_data) == 0:
        return alert_data, []
    
    alert_data = alert_data.sort_values('created_on').reset_index(drop=True)
    alert_data['time_diff_hours'] = alert_data['created_on'].diff().dt.total_seconds() / 3600
    
    # Identificar grupos baseado no gap entre alertas
    alert_data['group_id'] = -1  # -1 = sem grupo (isolado)
    current_group = 0
    group_start_idx = 0
    
    for i in range(len(alert_data)):
        if i == 0:
            continue
            
        gap = alert_data.loc[i, 'time_diff_hours']
        
        # Se o gap for maior que o threshold, finaliza o grupo anterior
        if gap > max_gap_hours:
            # Verifica se o grupo anterior Ã© vÃ¡lido (tamanho mÃ­nimo)
            group_size = i - group_start_idx
            if group_size >= min_group_size:
                alert_data.loc[group_start_idx:i-1, 'group_id'] = current_group
                current_group += 1
            # Inicia novo grupo potencial
            group_start_idx = i
    
    # Processa o Ãºltimo grupo
    group_size = len(alert_data) - group_start_idx
    if group_size >= min_group_size:
        alert_data.loc[group_start_idx:, 'group_id'] = current_group
    
    # Detectar spikes desproporcionais dentro de grupos
    alert_data['date'] = alert_data['created_on'].dt.date
    daily_counts = alert_data.groupby('date').size()
    avg_daily = daily_counts.mean()
    spike_threshold = avg_daily * spike_threshold_multiplier
    
    spike_dates = daily_counts[daily_counts > spike_threshold].index
    
    # Marcar alertas em dias de spike como isolados
    if len(spike_dates) > 0:
        alert_data.loc[alert_data['date'].isin(spike_dates), 'group_id'] = -1
    
    # Classificar cada alerta
    alert_data['is_isolated'] = alert_data['group_id'] == -1
    
    # Criar informaÃ§Ãµes dos grupos
    groups_info = []
    for group_id in alert_data[alert_data['group_id'] >= 0]['group_id'].unique():
        group_data = alert_data[alert_data['group_id'] == group_id]
        groups_info.append({
            'group_id': int(group_id),
            'size': len(group_data),
            'start_time': group_data['created_on'].min(),
            'end_time': group_data['created_on'].max(),
            'duration_hours': (group_data['created_on'].max() - 
                             group_data['created_on'].min()).total_seconds() / 3600
        })
    
    return alert_data, groups_info


def classify_alert_pattern(alert_data, max_gap_hours=24, min_group_size=3, 
                          spike_threshold_multiplier=5):
    """
    Classifica um alerta baseado na identificaÃ§Ã£o de grupos.
    Alertas contÃ­nuos sÃ£o aqueles com mÃºltiplos grupos bem definidos.
    Alertas isolados sÃ£o aqueles sem grupos significativos.
    """
    n = len(alert_data)
    if n == 0:
        return {
            'pattern': 'isolated',
            'reason': 'Sem ocorrÃªncias',
            'occurrences': 0,
            'num_groups': 0,
            'isolated_occurrences': 0,
            'grouped_occurrences': 0,
            'groups_info': []
        }
    
    # Identificar grupos
    alert_data_processed, groups_info = identify_alert_groups(
        alert_data, max_gap_hours, min_group_size, spike_threshold_multiplier
    )
    
    num_groups = len(groups_info)
    isolated_count = alert_data_processed['is_isolated'].sum()
    grouped_count = n - isolated_count
    
    # CritÃ©rios de classificaÃ§Ã£o
    isolated_pct = (isolated_count / n) * 100
    
    # Determinar padrÃ£o
    if num_groups == 0:
        pattern = 'isolated'
        reason = f'Nenhum grupo identificado ({n} ocorrÃªncias isoladas)'
    elif num_groups == 1 and isolated_pct > 50:
        pattern = 'isolated'
        reason = f'Apenas 1 grupo pequeno com {isolated_pct:.0f}% de alertas isolados'
    elif isolated_pct > 70:
        pattern = 'isolated'
        reason = f'{isolated_pct:.0f}% de alertas isolados ({isolated_count}/{n})'
    elif num_groups >= 2:
        pattern = 'continuous'
        reason = f'{num_groups} grupos contÃ­nuos identificados ({grouped_count} alertas agrupados)'
    elif num_groups == 1 and grouped_count >= min_group_size * 2:
        pattern = 'continuous'
        reason = f'1 grupo contÃ­nuo grande ({grouped_count} alertas)'
    else:
        pattern = 'isolated'
        reason = f'PadrÃ£o inconsistente: {num_groups} grupo(s), {isolated_pct:.0f}% isolados'
    
    return {
        'pattern': pattern,
        'reason': reason,
        'occurrences': n,
        'num_groups': num_groups,
        'isolated_occurrences': int(isolated_count),
        'grouped_occurrences': int(grouped_count),
        'groups_info': groups_info
    }

# ============================================================
# FunÃ§Ãµes de processamento
# ============================================================
def process_single_alert(alert_id, df_original, max_gap_hours=24, min_group_size=3, 
                        spike_threshold_multiplier=5):
    try:
        df_alert = df_original[df_original['u_alert_id'] == alert_id].copy()
        if len(df_alert) < 1:
            return None
        
        # ClassificaÃ§Ã£o de padrÃ£o baseada em grupos
        pattern_info = classify_alert_pattern(df_alert, max_gap_hours, min_group_size, 
                                             spike_threshold_multiplier)
        
        df_alert['hour'] = df_alert['created_on'].dt.hour
        df_alert['day_of_week'] = df_alert['created_on'].dt.dayofweek
        df_alert['is_weekend'] = df_alert['day_of_week'].isin([5, 6])
        df_alert['is_business_hours'] = (df_alert['hour'] >= 9) & (df_alert['hour'] <= 17)
        df_alert = df_alert.sort_values('created_on')
        intervals_hours = df_alert['created_on'].diff().dt.total_seconds() / 3600
        intervals_hours = intervals_hours.dropna()
        
        period_days = (df_alert['created_on'].max() - df_alert['created_on'].min()).days + 1
        
        metrics = {
            'alert_id': alert_id,
            'pattern_type': pattern_info['pattern'],
            'pattern_reason': pattern_info['reason'],
            'total_ocorrencias': pattern_info['occurrences'],
            'num_grupos': pattern_info['num_groups'],
            'alertas_isolados': pattern_info['isolated_occurrences'],
            'alertas_agrupados': pattern_info['grouped_occurrences'],
            'pct_isolados': (pattern_info['isolated_occurrences'] / pattern_info['occurrences'] * 100) 
                           if pattern_info['occurrences'] > 0 else 0,
            'periodo_dias': period_days,
            'freq_dia': len(df_alert) / period_days if period_days > 0 else 0,
            'freq_semana': (len(df_alert) / period_days * 7) if period_days > 0 else 0,
            'freq_mes': (len(df_alert) / period_days * 30) if period_days > 0 else 0,
            'intervalo_medio_h': intervals_hours.mean() if len(intervals_hours) > 0 else None,
            'intervalo_mediano_h': intervals_hours.median() if len(intervals_hours) > 0 else None,
            'intervalo_std_h': intervals_hours.std() if len(intervals_hours) > 0 else None,
            'intervalo_min_h': intervals_hours.min() if len(intervals_hours) > 0 else None,
            'intervalo_max_h': intervals_hours.max() if len(intervals_hours) > 0 else None,
            'hora_pico': df_alert['hour'].mode().iloc[0] if len(df_alert['hour'].mode()) > 0 else 12,
            'pct_fins_semana': df_alert['is_weekend'].mean() * 100,
            'pct_horario_comercial': df_alert['is_business_hours'].mean() * 100,
            'variabilidade_intervalo': intervals_hours.std() / intervals_hours.mean() if len(intervals_hours) > 0 and intervals_hours.mean() > 0 else 0,
            'primeiro_alerta': df_alert['created_on'].min(),
            'ultimo_alerta': df_alert['created_on'].max()
        }
        return metrics
    except Exception:
        return None

def process_alert_chunk(alert_ids, df_original, max_gap_hours=24, min_group_size=3, 
                       spike_threshold_multiplier=5):
    return [metrics for alert_id in alert_ids 
            if (metrics := process_single_alert(alert_id, df_original, max_gap_hours, 
                                               min_group_size, spike_threshold_multiplier))]

class StreamlitAlertAnalyzer:
    def __init__(self):
        self.df_original = None
        self.df_all_alerts = None
        self.df = None
        self.dates = None
        self.alert_id = None
        self.max_gap_hours = 24
        self.min_group_size = 3
        self.spike_threshold_multiplier = 5

    def load_data(self, uploaded_file):
        try:
            df_raw = pd.read_csv(uploaded_file)
            st.success(f"âœ… Arquivo carregado com {len(df_raw)} registros")
            with st.expander("ğŸ“‹ InformaÃ§Ãµes do Dataset"):
                st.write(f"**Colunas:** {list(df_raw.columns)}")
                st.write(f"**Shape:** {df_raw.shape}")
                st.dataframe(df_raw.head())
            if 'created_on' not in df_raw.columns or 'u_alert_id' not in df_raw.columns:
                st.error("âŒ Colunas 'created_on' e 'u_alert_id' sÃ£o obrigatÃ³rias!")
                return False
            df_raw['created_on'] = pd.to_datetime(df_raw['created_on'])
            df_raw = df_raw.dropna(subset=['created_on'])
            df_raw = df_raw.sort_values(['u_alert_id', 'created_on']).reset_index(drop=True)
            self.df_original = df_raw
            st.sidebar.write(f"**IDs disponÃ­veis:** {len(df_raw['u_alert_id'].unique())}")
            return True
        except Exception as e:
            st.error(f"âŒ Erro ao carregar dados: {e}")
            return False

    def prepare_individual_analysis(self, alert_id):
        df_filtered = self.df_original[self.df_original['u_alert_id'] == alert_id].copy()
        if len(df_filtered) == 0:
            return False

        df_filtered['date'] = df_filtered['created_on'].dt.date
        df_filtered['hour'] = df_filtered['created_on'].dt.hour
        df_filtered['day_of_week'] = df_filtered['created_on'].dt.dayofweek
        df_filtered['day_name'] = df_filtered['created_on'].dt.day_name()
        df_filtered['month'] = df_filtered['created_on'].dt.month
        df_filtered['month_name'] = df_filtered['created_on'].dt.month_name()
        df_filtered['is_weekend'] = df_filtered['day_of_week'].isin([5, 6])
        df_filtered['is_business_hours'] = (df_filtered['hour'] >= 9) & (df_filtered['hour'] <= 17)
        df_filtered['time_diff_hours'] = df_filtered['created_on'].diff().dt.total_seconds() / 3600
        df_filtered['time_diff_days'] = df_filtered['created_on'].diff().dt.total_seconds() / 86400

        # Identificar grupos de alertas
        df_filtered, groups_info = identify_alert_groups(
            df_filtered, 
            self.max_gap_hours, 
            self.min_group_size,
            self.spike_threshold_multiplier
        )

        self.df = df_filtered
        self.dates = df_filtered['created_on']
        self.alert_id = alert_id
        self.groups_info = groups_info
        return True

    def prepare_global_analysis(self, use_multiprocessing=True, max_gap_hours=24, 
                               min_group_size=3, spike_threshold_multiplier=5):
        st.header("ğŸŒ AnÃ¡lise Global de Todos os Alertas")
        self.max_gap_hours = max_gap_hours
        self.min_group_size = min_group_size
        self.spike_threshold_multiplier = spike_threshold_multiplier
        
        unique_ids = self.df_original['u_alert_id'].unique()
        total_ids = len(unique_ids)
        st.info(f"ğŸ“Š Processando {total_ids} Alert IDs...")
        alert_metrics = []
        
        if use_multiprocessing:
            n_processes = min(cpu_count(), total_ids)
            st.write(f"ğŸš€ Usando {n_processes} processos paralelos")
            chunk_size = max(1, total_ids // n_processes)
            id_chunks = [unique_ids[i:i + chunk_size] for i in range(0, total_ids, chunk_size)]
            progress_bar = st.progress(0)
            status_text = st.empty()
            process_func = partial(process_alert_chunk, 
                                  df_original=self.df_original,
                                  max_gap_hours=max_gap_hours,
                                  min_group_size=min_group_size,
                                  spike_threshold_multiplier=spike_threshold_multiplier)
            try:
                with Pool(processes=n_processes) as pool:
                    results = pool.map(process_func, id_chunks)
                    for result in results:
                        alert_metrics.extend(result)
                    progress_bar.progress(1.0)
                    status_text.success(f"âœ… Processamento concluÃ­do! {len(alert_metrics)} alertas analisados")
            except Exception as e:
                st.error(f"âŒ Erro no multiprocessing: {e}")
                st.warning("âš ï¸ Tentando processamento sequencial...")
                use_multiprocessing = False
                alert_metrics = []
        
        if not use_multiprocessing or len(alert_metrics) == 0:
            alert_metrics = []
            progress_bar = st.progress(0)
            for i, alert_id in enumerate(unique_ids):
                progress_bar.progress((i + 1) / total_ids)
                metrics = process_single_alert(alert_id, self.df_original, 
                                              max_gap_hours, min_group_size, 
                                              spike_threshold_multiplier)
                if metrics:
                    alert_metrics.append(metrics)
        
        if 'progress_bar' in locals():
            progress_bar.empty()
        
        self.df_all_alerts = pd.DataFrame(alert_metrics)
        
        # EstatÃ­sticas de padrÃµes
        isolated_count = len(self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'isolated'])
        continuous_count = len(self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous'])
        
        st.subheader("ğŸ“Š EstatÃ­sticas Globais")
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        with col1:
            st.metric("ğŸ”¢ Total de Alert IDs", len(unique_ids))
        with col2:
            st.metric("ğŸ“ˆ IDs com Dados", len(self.df_all_alerts))
        with col3:
            st.metric("ğŸš¨ Total de Alertas", self.df_original.shape[0])
        with col4:
            period_total = (self.df_original['created_on'].max() - self.df_original['created_on'].min()).days + 1
            st.metric("ğŸ“… PerÃ­odo (dias)", period_total)
        with col5:
            st.metric("ğŸ”´ Alertas Isolados", isolated_count)
        with col6:
            st.metric("ğŸŸ¢ Alertas ContÃ­nuos", continuous_count)
        
        return len(self.df_all_alerts) > 0

    def show_isolated_vs_continuous_analysis(self):
        st.header("ğŸ” AnÃ¡lise de Alertas Isolados vs ContÃ­nuos (Baseado em Grupos)")

        # Garantir que nÃ£o tenha duplicados
        self.df_all_alerts = self.df_all_alerts.drop_duplicates(subset=['alert_id'])

        # Separar alertas
        df_isolated = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'isolated']
        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']

        # VisualizaÃ§Ã£o geral
        col1, col2 = st.columns(2)
        with col1:
            # GrÃ¡fico de pizza
            pattern_dist = self.df_all_alerts['pattern_type'].value_counts()
            fig_pie = px.pie(
                values=pattern_dist.values,
                names=pattern_dist.index,
                title="ğŸ“Š DistribuiÃ§Ã£o de PadrÃµes de Alerta",
                color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
            )
            st.plotly_chart(fig_pie, use_container_width=True, key='pattern_pie')

        with col2:
            # EstatÃ­sticas comparativas
            st.subheader("ğŸ“ˆ ComparaÃ§Ã£o de MÃ©tricas")
            comparison_data = pd.DataFrame({
                'MÃ©trica': ['Qtd Alertas', 'MÃ©dia OcorrÃªncias', 'MÃ©dia Grupos', 
                            'MÃ©dia % Isolados', 'MÃ©dia Freq/Dia'],
                'Isolados': [
                    len(df_isolated),
                    df_isolated['total_ocorrencias'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['num_grupos'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['pct_isolados'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['freq_dia'].mean() if len(df_isolated) > 0 else 0
                ],
                'ContÃ­nuos': [
                    len(df_continuous),
                    df_continuous['total_ocorrencias'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['num_grupos'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['pct_isolados'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['freq_dia'].mean() if len(df_continuous) > 0 else 0
                ]
            })
            comparison_data = comparison_data.round(2)
            st.dataframe(comparison_data, use_container_width=True)

        # Tabs para detalhes
        tab1, tab2, tab3 = st.tabs(["ğŸ”´ Alertas Isolados", "ğŸŸ¢ Alertas ContÃ­nuos", "ğŸ“Š AnÃ¡lise Comparativa"])

        # ISOLADOS
        with tab1:
            st.subheader(f"ğŸ”´ Alertas Isolados ({len(df_isolated)} alertas)")

            if len(df_isolated) > 0:
                fig_iso = px.scatter(
                    df_isolated,
                    x='primeiro_alerta',
                    y='total_ocorrencias',
                    size='alertas_isolados',
                    color='pct_isolados',
                    title="â³ OcorrÃªncias de Alertas Isolados no Tempo",
                    hover_data=['alert_id', 'pattern_reason', 'num_grupos'],
                    labels={'pct_isolados': '% Isolados'}
                )
                st.plotly_chart(fig_iso, use_container_width=True, key='isolated_scatter')

                # RazÃµes para isolamento
                st.write("**ğŸ“ RazÃµes para ClassificaÃ§Ã£o como Isolado:**")
                reason_counts = df_isolated['pattern_reason'].value_counts()
                for reason, count in reason_counts.items():
                    st.write(f"â€¢ {reason}: {count} alertas")

                # Top alertas isolados
                st.write("**ğŸ” Top 10 Alertas Isolados (por % de alertas isolados):**")
                top_isolated = df_isolated.nlargest(10, 'pct_isolados')[
                    ['alert_id', 'total_ocorrencias', 'alertas_isolados', 'num_grupos', 'pct_isolados', 'pattern_reason']
                ]
                top_isolated.columns = ['Alert ID', 'Total OcorrÃªncias', 'Alertas Isolados', 'NÂº Grupos', '% Isolados', 'RazÃ£o']
                top_isolated['% Isolados'] = top_isolated['% Isolados'].round(1).astype(str) + '%'
                st.dataframe(top_isolated, use_container_width=True)

                with st.expander("ğŸ“‹ Ver todos os alertas isolados"):
                    isolated_list = df_isolated[['alert_id', 'total_ocorrencias', 'alertas_isolados',
                                                'num_grupos', 'pct_isolados', 'pattern_reason']].copy()
                    isolated_list.columns = ['Alert ID', 'Total', 'Isolados', 'Grupos', '% Isolados', 'RazÃ£o']
                    isolated_list['% Isolados'] = isolated_list['% Isolados'].round(1).astype(str) + '%'
                    st.dataframe(isolated_list, use_container_width=True)
            else:
                st.info("Nenhum alerta isolado encontrado com os critÃ©rios atuais.")

        # CONTÃNUOS
        with tab2:
            st.subheader(f"ğŸŸ¢ Alertas ContÃ­nuos ({len(df_continuous)} alertas)")

            if len(df_continuous) > 0:
                # Top alertas contÃ­nuos
                st.write("**ğŸ” Top 10 Alertas ContÃ­nuos (maior nÃºmero de grupos):**")
                top_continuous = df_continuous.nlargest(10, 'num_grupos')[
                    ['alert_id', 'total_ocorrencias', 'num_grupos', 'alertas_agrupados', 'freq_dia']
                ]
                top_continuous.columns = ['Alert ID', 'Total OcorrÃªncias', 'NÂº Grupos', 'Alertas Agrupados', 'Freq/Dia']
                st.dataframe(top_continuous, use_container_width=True)

                # DistribuiÃ§Ã£o de grupos
                col1, col2 = st.columns(2)
                with col1:
                    fig_groups = px.histogram(
                        df_continuous, 
                        x='num_grupos',
                        title="ğŸ“Š DistribuiÃ§Ã£o de NÃºmero de Grupos",
                        labels={'num_grupos': 'NÃºmero de Grupos', 'count': 'Quantidade'}
                    )
                    st.plotly_chart(fig_groups, use_container_width=True, key='continuous_groups_hist')

                with col2:
                    fig_pct = px.histogram(
                        df_continuous,
                        x='pct_isolados',
                        title="ğŸ“Š DistribuiÃ§Ã£o de % de Alertas Isolados",
                        labels={'pct_isolados': '% Alertas Isolados', 'count': 'Quantidade'}
                    )
                    st.plotly_chart(fig_pct, use_container_width=True, key='continuous_pct_hist')

                with st.expander("ğŸ“‹ Ver todos os alertas contÃ­nuos"):
                    continuous_list = df_continuous[['alert_id', 'total_ocorrencias', 'num_grupos',
                                                    'alertas_agrupados', 'alertas_isolados', 'pct_isolados']].copy()
                    continuous_list.columns = ['Alert ID', 'Total', 'Grupos', 'Agrupados', 'Isolados', '% Isolados']
                    continuous_list['% Isolados'] = continuous_list['% Isolados'].round(1).astype(str) + '%'
                    st.dataframe(continuous_list, use_container_width=True)
            else:
                st.info("Nenhum alerta contÃ­nuo encontrado com os critÃ©rios atuais.")

        # ANÃLISE COMPARATIVA
        with tab3:
            st.subheader("ğŸ“Š AnÃ¡lise Comparativa Detalhada")

            # Scatter plot comparativo
            fig_scatter = px.scatter(
                self.df_all_alerts,
                x='total_ocorrencias',
                y='intervalo_medio_h',
                color='pattern_type',
                title="ğŸ¯ OcorrÃªncias vs Intervalo MÃ©dio",
                labels={
                    'total_ocorrencias': 'Total de OcorrÃªncias',
                    'intervalo_medio_h': 'Intervalo MÃ©dio (horas)',
                    'pattern_type': 'Tipo de PadrÃ£o'
                },
                hover_data=['alert_id'],
                color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
            )
            st.plotly_chart(fig_scatter, use_container_width=True, key='comparative_scatter')

            # Box plots comparativos
            col1, col2 = st.columns(2)
            with col1:
                fig_box_occ = px.box(
                    self.df_all_alerts,
                    x='pattern_type',
                    y='total_ocorrencias',
                    title="ğŸ“¦ DistribuiÃ§Ã£o de OcorrÃªncias",
                    color='pattern_type',
                    color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
                )
                st.plotly_chart(fig_box_occ, use_container_width=True, key='box_occurrences')

            with col2:
                fig_box_freq = px.box(
                    self.df_all_alerts,
                    x='pattern_type',
                    y='freq_dia',
                    title="ğŸ“¦ DistribuiÃ§Ã£o de FrequÃªncia DiÃ¡ria",
                    color='pattern_type',
                    color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
                )
                st.plotly_chart(fig_box_freq, use_container_width=True, key='box_frequency')

            # RecomendaÃ§Ãµes
            st.subheader("ğŸ’¡ RecomendaÃ§Ãµes de Tratamento")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**ğŸ”´ Para Alertas Isolados:**")
                st.write("â€¢ Considerar desativaÃ§Ã£o ou revisÃ£o de configuraÃ§Ã£o")
                st.write("â€¢ Verificar se sÃ£o falsos positivos")
                st.write("â€¢ Analisar contexto especÃ­fico das ocorrÃªncias")
                st.write("â€¢ Avaliar consolidaÃ§Ã£o com outros alertas similares")

            with col2:
                st.write("**ğŸŸ¢ Para Alertas ContÃ­nuos:**")
                st.write("â€¢ Priorizar automaÃ§Ã£o de resposta")
                st.write("â€¢ Implementar supressÃ£o inteligente")
                st.write("â€¢ Criar runbooks especÃ­ficos")
                st.write("â€¢ Considerar ajuste de thresholds")

    def show_global_overview(self, filter_isolated=False):
        st.subheader("ğŸ“ˆ VisÃ£o Geral dos Alertas")
        
        df_to_analyze = self.df_all_alerts
        if filter_isolated:
            df_to_analyze = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
            st.info(f"ğŸ” Mostrando apenas alertas contÃ­nuos ({len(df_to_analyze)} de {len(self.df_all_alerts)})")
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**ğŸ”¥ Top 10 Alertas Mais Frequentes**")
            top_frequent = df_to_analyze.nlargest(10, 'total_ocorrencias')[['alert_id', 'total_ocorrencias', 'freq_dia', 'pattern_type']]
            top_frequent.columns = ['Alert ID', 'Total OcorrÃªncias', 'FrequÃªncia/Dia', 'Tipo']
            st.dataframe(top_frequent, use_container_width=True)
        with col2:
            st.write("**âš¡ Top 10 Alertas Mais RÃ¡pidos (Menor Intervalo)**")
            df_with_intervals = df_to_analyze.dropna(subset=['intervalo_medio_h'])
            if len(df_with_intervals) > 0:
                top_fast = df_with_intervals.nsmallest(10, 'intervalo_medio_h')[['alert_id', 'intervalo_medio_h', 'total_ocorrencias', 'pattern_type']]
                top_fast.columns = ['Alert ID', 'Intervalo MÃ©dio (h)', 'Total OcorrÃªncias', 'Tipo']
                st.dataframe(top_fast, use_container_width=True)
            else:
                st.info("Sem dados de intervalo disponÃ­veis")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            fig_freq = px.histogram(df_to_analyze, x='freq_dia', title="ğŸ“Š DistribuiÃ§Ã£o de FrequÃªncia (alertas/dia)",
                                   labels={'freq_dia': 'Alertas por Dia', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_freq, use_container_width=True)
        with col2:
            fig_int = px.histogram(df_to_analyze, x='freq_semana', title="ğŸ“Š DistribuiÃ§Ã£o de FrequÃªncia (alertas/semana)",
                                  labels={'freq_semana': 'Alertas por semana', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_int, use_container_width=True)
        with col3:
            fig_int = px.histogram(df_to_analyze, x='freq_mes', title="ğŸ“Š DistribuiÃ§Ã£o de FrequÃªncia (alertas/mes)",
                                  labels={'freq_mes': 'Alertas por mes', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_int, use_container_width=True)
        with col4:
            df_with_intervals = df_to_analyze.dropna(subset=['intervalo_medio_h'])
            if len(df_with_intervals) > 0:
                fig_int = px.histogram(df_with_intervals, x='intervalo_medio_h', title="â±ï¸ DistribuiÃ§Ã£o de Intervalos MÃ©dios",
                                      labels={'intervalo_medio_h': 'Intervalo MÃ©dio (horas)', 'count': 'Quantidade de Alert IDs'})
                st.plotly_chart(fig_int, use_container_width=True)

    def perform_clustering_analysis(self, use_only_continuous=True):
        st.subheader("ğŸ¯ Agrupamento de Alertas por Perfil de Comportamento")
        
        df_for_clustering = self.df_all_alerts
        if use_only_continuous:
            df_for_clustering = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous'].copy()
            st.info(f"ğŸ” Usando apenas alertas contÃ­nuos para clustering ({len(df_for_clustering)} alertas)")
        
        if len(df_for_clustering) < 2:
            st.warning("âš ï¸ Dados insuficientes para clustering")
            return None
        
        features = [
            'freq_dia', 'intervalo_medio_h', 'intervalo_std_h',
            'hora_pico', 'pct_fins_semana', 'pct_horario_comercial', 'variabilidade_intervalo'
        ]
        X = df_for_clustering[features].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        st.write("**ğŸ” Determinando NÃºmero Ã“timo de Clusters...**")
        max_clusters = min(10, len(X) - 1)
        silhouette_scores = []
        inertias = []
        
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)
            silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))
            inertias.append(kmeans.inertia_)
        
        optimal_k = range(2, max_clusters + 1)[np.argmax(silhouette_scores)]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ğŸ¯ NÃºmero Ã“timo de Clusters", optimal_k)
        with col2:
            st.metric("ğŸ“Š Silhouette Score", f"{max(silhouette_scores):.3f}")
        
        kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        clusters = kmeans_final.fit_predict(X_scaled)
        
        # Atribuir clusters apenas aos alertas usados no clustering
        df_for_clustering['cluster'] = clusters
        
        # Atualizar o dataframe principal apenas com os Ã­ndices corretos
        self.df_all_alerts['cluster'] = np.nan
        self.df_all_alerts.loc[df_for_clustering.index, 'cluster'] = df_for_clustering['cluster']
        
        col1, col2 = st.columns(2)
        with col1:
            fig_scatter = px.scatter(
                df_for_clustering,
                x='freq_dia',
                y='intervalo_medio_h',
                color='cluster',
                size='total_ocorrencias',
                hover_data=['alert_id'],
                title="ğŸ¨ Clusters: FrequÃªncia vs Intervalo MÃ©dio"
            )
            st.plotly_chart(fig_scatter, use_container_width=True, key='cluster_scatter')
        with col2:
            cluster_dist = df_for_clustering['cluster'].value_counts().sort_index()
            fig_dist = px.bar(
                x=cluster_dist.index,
                y=cluster_dist.values,
                title="ğŸ“Š DistribuiÃ§Ã£o de Alertas por Cluster",
                labels={'x': 'Cluster', 'y': 'Quantidade de Alert IDs'}
            )
            st.plotly_chart(fig_dist, use_container_width=True, key='cluster_dist')
        return optimal_k

    def show_cluster_profiles(self, n_clusters):
        st.subheader("ğŸ‘¥ Perfis dos Clusters")
        cluster_profiles = self.df_all_alerts.groupby('cluster').agg({
            'total_ocorrencias': ['mean', 'std', 'count'],
            'freq_dia': ['mean', 'std'],
            'intervalo_medio_h': ['mean', 'std'],
            'hora_pico': 'mean',
            'pct_fins_semana': 'mean',
            'pct_horario_comercial': 'mean',
            'variabilidade_intervalo': 'mean'
        }).round(2)
        cluster_tabs = st.tabs([f"Cluster {i}" for i in range(n_clusters)])
        for i in range(n_clusters):
            with cluster_tabs[i]:
                cluster_data = self.df_all_alerts[self.df_all_alerts['cluster'] == i]
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ğŸ“Š Quantidade de Alertas", len(cluster_data))
                with col2:
                    avg_freq = cluster_data['freq_dia'].mean()
                    st.metric("ğŸ“ˆ Freq. MÃ©dia/Dia", f"{avg_freq:.2f}")
                with col3:
                    avg_interval = cluster_data['intervalo_medio_h'].mean()
                    st.metric("â±ï¸ Intervalo MÃ©dio (h)", f"{avg_interval:.2f}")
                with col4:
                    avg_hour = cluster_data['hora_pico'].mean()
                    st.metric("ğŸ• Hora Pico MÃ©dia", f"{avg_hour:.0f}:00")
                st.write("**ğŸ¯ CaracterÃ­sticas do Cluster:**")
                weekend_pct = cluster_data['pct_fins_semana'].mean()
                business_pct = cluster_data['pct_horario_comercial'].mean()
                variability = cluster_data['variabilidade_intervalo'].mean()
                characteristics = []
                if avg_freq > self.df_all_alerts['freq_dia'].median():
                    characteristics.append("ğŸ”¥ **Alta frequÃªncia**")
                else:
                    characteristics.append("ğŸŒ **Baixa frequÃªncia**")
                if avg_interval < self.df_all_alerts['intervalo_medio_h'].median():
                    characteristics.append("âš¡ **Intervalos curtos**")
                else:
                    characteristics.append("â³ **Intervalos longos**")
                if weekend_pct > 30:
                    characteristics.append("ğŸ—“ï¸ **Ativo nos fins de semana**")
                if business_pct > 70:
                    characteristics.append("ğŸ¢ **Predominantemente em horÃ¡rio comercial**")
                elif business_pct < 30:
                    characteristics.append("ğŸŒ™ **Predominantemente fora do horÃ¡rio comercial**")
                if variability > self.df_all_alerts['variabilidade_intervalo'].median():
                    characteristics.append("ğŸ“Š **PadrÃ£o irregular**")
                else:
                    characteristics.append("ğŸ“ˆ **PadrÃ£o regular**")
                for char in characteristics:
                    st.write(f"â€¢ {char}")
                with st.expander(f"ğŸ“‹ Alertas no Cluster {i}"):
                    cluster_alerts = cluster_data[['alert_id', 'total_ocorrencias', 'freq_dia', 'intervalo_medio_h']].copy()
                    cluster_alerts.columns = ['Alert ID', 'Total OcorrÃªncias', 'Freq/Dia', 'Intervalo MÃ©dio (h)']
                    st.dataframe(cluster_alerts, use_container_width=True, key=f'cluster_table_{i}')

    def show_cluster_recommendations(self):
        st.subheader("ğŸ’¡ RecomendaÃ§Ãµes por Cluster")
        for cluster_id in sorted(self.df_all_alerts['cluster'].unique()):
            cluster_data = self.df_all_alerts[self.df_all_alerts['cluster'] == cluster_id]
            avg_freq = cluster_data['freq_dia'].mean()
            avg_interval = cluster_data['intervalo_medio_h'].mean()
            weekend_pct = cluster_data['pct_fins_semana'].mean()
            business_pct = cluster_data['pct_horario_comercial'].mean()
            with st.expander(f"ğŸ¯ RecomendaÃ§Ãµes para Cluster {cluster_id} ({len(cluster_data)} alertas)"):
                recommendations = []
                if avg_freq > 5:
                    recommendations.append("ğŸš¨ **Prioridade Alta**: Alertas muito frequentes - investigar causa raiz")
                    recommendations.append("ğŸ”§ **AÃ§Ã£o**: Considerar automaÃ§Ã£o de resposta ou ajuste de thresholds")
                if avg_interval < 1:
                    recommendations.append("âš¡ **Rajadas detectadas**: PossÃ­vel tempestade de alertas")
                    recommendations.append("ğŸ›¡ï¸ **AÃ§Ã£o**: Implementar rate limiting ou supressÃ£o inteligente")
                if weekend_pct > 50:
                    recommendations.append("ğŸ—“ï¸ **PadrÃ£o de fim de semana**: Alertas ativos nos fins de semana")
                    recommendations.append("ğŸ‘¥ **AÃ§Ã£o**: Verificar cobertura de plantÃ£o")
                if business_pct < 30:
                    recommendations.append("ğŸŒ™ **PadrÃ£o noturno**: Principalmente fora do horÃ¡rio comercial")
                    recommendations.append("ğŸ”„ **AÃ§Ã£o**: Considerar processos automatizados noturnos")
                if avg_freq < 0.5:
                    recommendations.append("ğŸ“‰ **Baixa frequÃªncia**: Alertas esporÃ¡dicos")
                    recommendations.append("ğŸ“Š **AÃ§Ã£o**: Revisar relevÃ¢ncia e configuraÃ§Ã£o do alerta")
                for rec in recommendations:
                    st.write(f"â€¢ {rec}")
                if not recommendations:
                    st.write("â€¢ âœ… **PadrÃ£o normal**: Nenhuma aÃ§Ã£o especÃ­fica recomendada")

    def show_basic_stats(self):
        st.header("ğŸ“Š EstatÃ­sticas BÃ¡sicas")
        total = len(self.df)
        period_days = (self.dates.max() - self.dates.min()).days + 1
        avg_per_day = total / period_days
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ”¥ Total de OcorrÃªncias", total)
        with col2:
            st.metric("ğŸ“… PerÃ­odo (dias)", period_days)
        with col3:
            st.metric("ğŸ“ˆ MÃ©dia/dia", f"{avg_per_day:.2f}")
        with col4:
            last_alert = self.dates.max().strftime("%d/%m %H:%M")
            st.metric("ğŸ• Ãšltimo Alerta", last_alert)
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 0:
            st.subheader("â±ï¸ Intervalos Entre Alertas")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("MÃ©dia (h)", f"{intervals.mean():.2f}")
            with col2:
                st.metric("Mediana (h)", f"{intervals.median():.2f}")
            with col3:
                st.metric("MÃ­nimo (h)", f"{intervals.min():.2f}")
            with col4:
                st.metric("MÃ¡ximo (h)", f"{intervals.max():.2f}")

    def show_individual_alert_analysis(self):
        st.header(f"ğŸ“Œ AnÃ¡lise Individual do Alert ID: {self.alert_id}")
    
        if self.df is None or len(self.df) == 0:
            st.info("Nenhum dado disponÃ­vel para este alerta.")
            return
    
        # Separar alertas isolados e agrupados
        df_isolated = self.df[self.df['is_isolated']]
        df_grouped = self.df[~self.df['is_isolated']]
    
        # EstatÃ­sticas gerais
        st.subheader("ğŸ“Š EstatÃ­sticas Gerais do Alert ID")
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("Total OcorrÃªncias", len(self.df))
        with col2:
            st.metric("ğŸ”´ Isolados", len(df_isolated))
        with col3:
            st.metric("ğŸŸ¢ Agrupados", len(df_grouped))
        with col4:
            st.metric("ğŸ“¦ NÂº de Grupos", len(self.groups_info))
        with col5:
            pct_isolated = (len(df_isolated) / len(self.df) * 100) if len(self.df) > 0 else 0
            st.metric("% Isolados", f"{pct_isolated:.1f}%")
    
        # InformaÃ§Ãµes dos grupos
        if len(self.groups_info) > 0:
            st.subheader("ğŸ“¦ InformaÃ§Ãµes dos Grupos")
            groups_df = pd.DataFrame(self.groups_info)
            groups_df['start_time'] = pd.to_datetime(groups_df['start_time']).dt.strftime('%Y-%m-%d %H:%M')
            groups_df['end_time'] = pd.to_datetime(groups_df['end_time']).dt.strftime('%Y-%m-%d %H:%M')
            groups_df['duration_hours'] = groups_df['duration_hours'].round(2)
            groups_df.columns = ['ID Grupo', 'Tamanho', 'InÃ­cio', 'Fim', 'DuraÃ§Ã£o (h)']
            st.dataframe(groups_df, use_container_width=True)
    
        tab1, tab2, tab3 = st.tabs(["ğŸ”´ OcorrÃªncias Isoladas", "ğŸŸ¢ OcorrÃªncias Agrupadas", "ğŸ“Š VisualizaÃ§Ã£o Temporal"])
    
        with tab1:
            st.subheader(f"ğŸ”´ OcorrÃªncias Isoladas ({len(df_isolated)})")
            if len(df_isolated) > 0:
                isolated_display = df_isolated[['created_on', 'hour', 'day_name', 'time_diff_hours', 'date']].copy()
                isolated_display['created_on'] = isolated_display['created_on'].dt.strftime('%Y-%m-%d %H:%M:%S')
                isolated_display.columns = ['Data/Hora', 'Hora', 'Dia da Semana', 'Intervalo (h)', 'Data']
                st.dataframe(isolated_display, use_container_width=True)
                st.write(f"**Percentual:** {len(df_isolated)/len(self.df)*100:.2f}% das ocorrÃªncias sÃ£o isoladas")
                
                # Detectar spikes (dias com muitos alertas)
                daily_counts = df_isolated.groupby('date').size().sort_values(ascending=False)
                if len(daily_counts) > 0:
                    st.write("**ğŸ“ˆ Dias com Mais Alertas Isolados:**")
                    top_days = daily_counts.head(5)
                    for date, count in top_days.items():
                        st.write(f"â€¢ {date}: {count} alertas")
            else:
                st.info("Nenhuma ocorrÃªncia isolada detectada neste alerta.")
    
        with tab2:
            st.subheader(f"ğŸŸ¢ OcorrÃªncias Agrupadas ({len(df_grouped)})")
            if len(df_grouped) > 0:
                grouped_display = df_grouped[['created_on', 'hour', 'day_name', 'time_diff_hours', 'group_id']].copy()
                grouped_display['created_on'] = grouped_display['created_on'].dt.strftime('%Y-%m-%d %H:%M:%S')
                grouped_display.columns = ['Data/Hora', 'Hora', 'Dia da Semana', 'Intervalo (h)', 'Grupo']
                st.dataframe(grouped_display, use_container_width=True)
                st.write(f"**Percentual:** {len(df_grouped)/len(self.df)*100:.2f}% das ocorrÃªncias estÃ£o agrupadas")
            else:
                st.info("Nenhuma ocorrÃªncia agrupada detectada neste alerta.")
        
        with tab3:
            st.subheader("ğŸ“Š VisualizaÃ§Ã£o Temporal dos Alertas")
            
            # GrÃ¡fico de timeline com grupos coloridos
            fig = go.Figure()
            
            # Adicionar alertas isolados
            if len(df_isolated) > 0:
                fig.add_trace(go.Scatter(
                    x=df_isolated['created_on'],
                    y=[1] * len(df_isolated),
                    mode='markers',
                    name='Isolados',
                    marker=dict(size=10, color='red', symbol='x'),
                    hovertemplate='%{x}<br>Isolado<extra></extra>'
                ))
            
            # Adicionar alertas agrupados por grupo
            for group_info in self.groups_info:
                group_id = group_info['group_id']
                group_data = df_grouped[df_grouped['group_id'] == group_id]
                fig.add_trace(go.Scatter(
                    x=group_data['created_on'],
                    y=[1] * len(group_data),
                    mode='markers',
                    name=f'Grupo {group_id}',
                    marker=dict(size=10),
                    hovertemplate='%{x}<br>Grupo ' + str(group_id) + '<extra></extra>'
                ))
            
            fig.update_layout(
                title="Timeline de Alertas (Isolados vs Agrupados)",
                xaxis_title="Data/Hora",
                yaxis=dict(showticklabels=False, title=""),
                height=400,
                hovermode='closest'
            )
            st.plotly_chart(fig, use_container_width=True)
            st.plotly_chart(fig, use_container_width=True)

    def show_temporal_patterns(self):
        st.header("â° PadrÃµes Temporais")
        col1, col2 = st.columns(2)
        with col1:
            hourly = self.df['hour'].value_counts().sort_index()
            fig_hour = px.bar(
                x=hourly.index, 
                y=hourly.values,
                title="ğŸ“Š DistribuiÃ§Ã£o por Hora do Dia",
                labels={'x': 'Hora', 'y': 'Quantidade de Alertas'}
            )
            fig_hour.update_layout(showlegend=False)
            st.plotly_chart(fig_hour, use_container_width=True, key='hourly_dist')
            peak_hour = hourly.idxmax()
            quiet_hour = hourly.idxmin()
            st.write(f"ğŸ• **Pico:** {peak_hour:02d}:00 ({hourly[peak_hour]} alertas)")
            st.write(f"ğŸŒ™ **Menor atividade:** {quiet_hour:02d}:00 ({hourly[quiet_hour]} alertas)")
        with col2:
            daily = self.df['day_name'].value_counts()
            days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
            daily_ordered = daily.reindex(days_order).fillna(0)
            fig_day = px.bar(
                x=daily_ordered.index, 
                y=daily_ordered.values,
                title="ğŸ“… DistribuiÃ§Ã£o por Dia da Semana",
                labels={'x': 'Dia', 'y': 'Quantidade de Alertas'}
            )
            fig_day.update_layout(showlegend=False)
            st.plotly_chart(fig_day, use_container_width=True, key='daily_dist')
            busiest_day = daily.idxmax()
            st.write(f"ğŸ“ˆ **Dia mais ativo:** {busiest_day} ({daily[busiest_day]} alertas)")
        col1, col2 = st.columns(2)
        with col1:
            business = self.df['is_business_hours'].sum()
            non_business = len(self.df) - business
            st.subheader("ğŸ¢ HorÃ¡rio Comercial (9h-17h)")
            business_data = pd.DataFrame({
                'PerÃ­odo': ['Comercial', 'Fora do horÃ¡rio'],
                'Quantidade': [business, non_business],
                'Porcentagem': [business/len(self.df)*100, non_business/len(self.df)*100]
            })
            fig_business = px.pie(
                business_data, 
                values='Quantidade', 
                names='PerÃ­odo',
                title="DistribuiÃ§Ã£o por HorÃ¡rio"
            )
            st.plotly_chart(fig_business, use_container_width=True, key='business_hours_pie')
        with col2:
            weekend = self.df['is_weekend'].sum()
            weekday = len(self.df) - weekend
            st.subheader("ğŸ—“ï¸ Fins de Semana vs Dias Ãšteis")
            weekend_data = pd.DataFrame({
                'PerÃ­odo': ['Dias Ãºteis', 'Fins de semana'],
                'Quantidade': [weekday, weekend],
                'Porcentagem': [weekday/len(self.df)*100, weekend/len(self.df)*100]
            })
            fig_weekend = px.pie(
                weekend_data, 
                values='Quantidade', 
                names='PerÃ­odo',
                title="DistribuiÃ§Ã£o Semanal"
            )
            st.plotly_chart(fig_weekend, use_container_width=True, key='weekend_pie')

    def show_burst_analysis(self):
        st.header("ğŸ’¥ AnÃ¡lise de Rajadas")
        burst_threshold = st.slider("â±ï¸ Threshold para Rajada (horas)", 0.5, 24.0, 2.0, 0.5)
        intervals = self.df[~self.df['is_isolated']]['time_diff_hours'].fillna(999)
        bursts, current_burst = [], []
        for i, interval in enumerate(intervals):
            if interval <= burst_threshold and i > 0:
                if not current_burst:
                    current_burst = [i-1, i]
                else:
                    current_burst.append(i)
            else:
                if len(current_burst) >= 2:
                    bursts.append(current_burst)
                current_burst = []
        if len(current_burst) >= 2:
            bursts.append(current_burst)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸš¨ Rajadas Detectadas", len(bursts))
        if bursts:
            burst_sizes = [len(b) for b in bursts]
            with col2:
                st.metric("ğŸ“Š Tamanho MÃ©dio", f"{np.mean(burst_sizes):.1f}")
            with col3:
                st.metric("ğŸ“ˆ Maior Rajada", f"{max(burst_sizes)} alertas")
            st.subheader("ğŸ”¥ Maiores Rajadas")
            sorted_bursts = sorted(bursts, key=len, reverse=True)[:5]
            burst_data = []
            for i, burst_indices in enumerate(sorted_bursts):
                start_time = self.df.iloc[burst_indices[0]]['created_on']
                end_time = self.df.iloc[burst_indices[-1]]['created_on']
                duration = end_time - start_time
                burst_data.append({
                    'Rajada': f"#{i+1}",
                    'Alertas': len(burst_indices),
                    'InÃ­cio': start_time.strftime("%d/%m/%Y %H:%M"),
                    'Fim': end_time.strftime("%d/%m/%Y %H:%M"),
                    'DuraÃ§Ã£o': str(duration)
                })
            st.dataframe(pd.DataFrame(burst_data), use_container_width=True)

    def show_trend_analysis(self):
        st.header("ğŸ“ˆ AnÃ¡lise de TendÃªncias")
        daily_counts = self.df.groupby('date').size()
        if len(daily_counts) >= 7:
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=daily_counts.index,
                y=daily_counts.values,
                mode='lines+markers',
                name='Alertas por dia',
                line=dict(color='blue')
            ))
            x_numeric = np.arange(len(daily_counts))
            slope, intercept, r_value, p_value, std_err = stats.linregress(x_numeric, daily_counts.values)
            trend_line = slope * x_numeric + intercept
            fig.add_trace(go.Scatter(
                x=daily_counts.index,
                y=trend_line,
                mode='lines',
                name='TendÃªncia',
                line=dict(color='red', dash='dash')
            ))
            fig.update_layout(
                title="ğŸ“Š EvoluÃ§Ã£o Temporal dos Alertas",
                xaxis_title="Data",
                yaxis_title="NÃºmero de Alertas",
                hovermode='x'
            )
            st.plotly_chart(fig, use_container_width=True, key='trend_analysis')
            if slope > 0.01:
                trend = "CRESCENTE ğŸ“ˆ"
            elif slope < -0.01:
                trend = "DECRESCENTE ğŸ“‰"
            else:
                trend = "ESTÃVEL â¡ï¸"
            strength = "FORTE" if abs(r_value) > 0.7 else "MODERADA" if abs(r_value) > 0.3 else "FRACA"
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ¯ TendÃªncia", trend)
            with col2:
                st.metric("ğŸ’ª ForÃ§a", strength)
            with col3:
                st.metric("ğŸ“Š CorrelaÃ§Ã£o", f"{r_value:.4f}")
            with col4:
                st.metric("âš¡ Taxa/dia", f"{slope:.4f}")
        else:
            st.warning("âš ï¸ Poucos dados para anÃ¡lise de tendÃªncia (mÃ­nimo 7 dias)")

    def show_anomaly_detection(self):
        st.header("ğŸš¨ DetecÃ§Ã£o de Anomalias")
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 4:
            Q1 = intervals.quantile(0.25)
            Q3 = intervals.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            fast_anomalies = intervals[intervals < lower_bound]
            slow_anomalies = intervals[intervals > upper_bound]
            normal_intervals = intervals[(intervals >= lower_bound) & (intervals <= upper_bound)]
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("âš¡ Intervalos Curtos", len(fast_anomalies))
            with col2:
                st.metric("ğŸŒ Intervalos Longos", len(slow_anomalies))
            with col3:
                st.metric("âœ… Intervalos Normais", len(normal_intervals))
            fig = go.Figure()
            fig.add_trace(go.Box(
                y=intervals,
                name="Intervalos (horas)",
                boxpoints='outliers'
            ))
            fig.update_layout(
                title="ğŸ“Š DistribuiÃ§Ã£o dos Intervalos (DetecÃ§Ã£o de Outliers)",
                yaxis_title="Horas"
            )
            st.plotly_chart(fig, use_container_width=True, key='anomaly_boxplot')
            if len(fast_anomalies) > 0 or len(slow_anomalies) > 0:
                col1, col2 = st.columns(2)
                with col1:
                    if len(fast_anomalies) > 0:
                        st.subheader("âš¡ Intervalos Muito Curtos")
                        st.write(f"Menor intervalo: **{fast_anomalies.min():.2f} horas**")
                        st.write(f"MÃ©dia dos curtos: **{fast_anomalies.mean():.2f} horas**")
                with col2:
                    if len(slow_anomalies) > 0:
                        st.subheader("ğŸŒ Intervalos Muito Longos")
                        st.write(f"Maior intervalo: **{slow_anomalies.max():.2f} horas**")
                        st.write(f"MÃ©dia dos longos: **{slow_anomalies.mean():.2f} horas**")
        else:
            st.warning("âš ï¸ Poucos dados para detecÃ§Ã£o de anomalias")

    def show_predictions(self):
        st.header("ğŸ”® Insights Preditivos")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("â° HorÃ¡rios de Maior Probabilidade")
            hourly_prob = self.df['hour'].value_counts(normalize=True).sort_values(ascending=False)
            prob_data = [{'HorÃ¡rio': f"{hour:02d}:00", 'Probabilidade': f"{prob*100:.1f}%"} for hour, prob in hourly_prob.head(5).items()]
            st.dataframe(pd.DataFrame(prob_data), use_container_width=True)
        with col2:
            st.subheader("ğŸ“… Dias de Maior Probabilidade")
            daily_prob = self.df['day_name'].value_counts(normalize=True).sort_values(ascending=False)
            day_data = [{'Dia': day, 'Probabilidade': f"{prob*100:.1f}%"} for day, prob in daily_prob.items()]
            st.dataframe(pd.DataFrame(day_data), use_container_width=True)
        st.subheader("â±ï¸ PrevisÃ£o do PrÃ³ximo Alerta")
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 0:
            avg_interval = intervals.mean()
            median_interval = intervals.median()
            last_alert = self.dates.max()
            next_avg = last_alert + timedelta(hours=avg_interval)
            next_median = last_alert + timedelta(hours=median_interval)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ğŸ• Ãšltimo Alerta", last_alert.strftime("%d/%m %H:%M"))
            with col2:
                st.metric("ğŸ“Š PrÃ³ximo (MÃ©dia)", next_avg.strftime("%d/%m %H:%M"))
            with col3:
                st.metric("ğŸ“ˆ PrÃ³ximo (Mediana)", next_median.strftime("%d/%m %H:%M"))
            st.info(f"ğŸ’¡ **Baseado em:** Intervalo mÃ©dio de {avg_interval:.1f}h e mediana de {median_interval:.1f}h")

def main():
    st.title("ğŸš¨ Analisador de Alertas - VersÃ£o Completa")
    st.markdown("### AnÃ¡lise individual, global e agrupamento inteligente de alertas")
    st.sidebar.header("âš™ï¸ ConfiguraÃ§Ãµes")
    
    # ParÃ¢metros de agrupamento
    with st.sidebar.expander("ğŸ›ï¸ ParÃ¢metros de Agrupamento", expanded=False):
        max_gap_hours = st.slider(
            "â±ï¸ Gap MÃ¡ximo Entre Alertas (horas)",
            min_value=1,
            max_value=72,
            value=24,
            help="Alertas separados por mais tempo que isso sÃ£o considerados de grupos diferentes"
        )
        min_group_size = st.slider(
            "ğŸ“Š Tamanho MÃ­nimo do Grupo",
            min_value=2,
            max_value=10,
            value=3,
            help="NÃºmero mÃ­nimo de alertas para formar um grupo vÃ¡lido"
        )
        spike_threshold_multiplier = st.slider(
            "ğŸš€ Multiplicador de Spike",
            min_value=2.0,
            max_value=10.0,
            value=5.0,
            step=0.5,
            help="Dias com mais alertas que mÃ©dia Ã— este valor sÃ£o considerados spikes isolados"
        )
    
    analysis_mode = st.sidebar.selectbox(
        "ğŸ¯ Modo de AnÃ¡lise",
        ["ğŸŒ AnÃ¡lise Global", "ğŸ” AnÃ¡lise Individual"],
        help="Escolha entre analisar todos os alertas ou um alerta especÃ­fico"
    )
    uploaded_file = st.sidebar.file_uploader(
        "ğŸ“ Upload do arquivo CSV",
        type=['csv'],
        help="FaÃ§a upload do arquivo CSV contendo os dados dos alertas"
    )
    if uploaded_file is not None:
        analyzer = StreamlitAlertAnalyzer()
        if analyzer.load_data(uploaded_file):
            if analysis_mode == "ğŸŒ AnÃ¡lise Global":
                st.markdown("---")
                use_multiprocessing = st.sidebar.checkbox(
                    "âš¡ Usar Multiprocessing (Mais RÃ¡pido)", 
                    value=True,
                    help="Processa alertas em paralelo para melhor desempenho"
                )
                if st.sidebar.button("ğŸš€ Executar AnÃ¡lise Global", type="primary"):
                    if analyzer.prepare_global_analysis(use_multiprocessing, max_gap_hours, 
                                                       min_group_size, spike_threshold_multiplier):
                        tab1, tab2, tab3, tab4, tab5 = st.tabs([
                            "ğŸ“Š VisÃ£o Geral",
                            "ğŸ” Isolados vs ContÃ­nuos",
                            "ğŸ¯ Agrupamento", 
                            "ğŸ‘¥ Perfis dos Clusters",
                            "ğŸ’¡ RecomendaÃ§Ãµes"
                        ])
                        with tab1:
                            analyzer.show_global_overview()
                        with tab2:
                            analyzer.show_isolated_vs_continuous_analysis()
                        with tab3:
                            n_clusters = analyzer.perform_clustering_analysis()
                        with tab4:
                            if n_clusters:
                                analyzer.show_cluster_profiles(n_clusters)
                        with tab5:
                            if n_clusters:
                                analyzer.show_cluster_recommendations()
                        st.sidebar.markdown("---")
                        st.sidebar.subheader("ğŸ“¥ Downloads")
                        csv_buffer = io.StringIO()
                        analyzer.df_all_alerts.to_csv(csv_buffer, index=False)
                        st.sidebar.download_button(
                            label="â¬‡ï¸ Baixar AnÃ¡lise Global",
                            data=csv_buffer.getvalue(),
                            file_name=f"analise_global_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.error("âŒ NÃ£o foi possÃ­vel processar os dados para anÃ¡lise global")
            else:
                try:
                    unique_ids = analyzer.df_original['u_alert_id'].unique()
                    selected_id = st.sidebar.selectbox(
                        "ğŸ¯ Selecione o Alert ID",
                        unique_ids,
                        help="Escolha o ID do alerta para anÃ¡lise"
                    )
                    if st.sidebar.button("ğŸš€ Executar AnÃ¡lise Individual", type="primary"):
                        analyzer.max_gap_hours = max_gap_hours
                        analyzer.min_group_size = min_group_size
                        analyzer.spike_threshold_multiplier = spike_threshold_multiplier
                        
                        if analyzer.prepare_individual_analysis(selected_id):
                            st.success(f"ğŸ¯ Analisando alert_id: {selected_id} ({len(analyzer.df)} registros)")
                            st.info(f"ğŸ“… **PerÃ­odo analisado:** {analyzer.dates.min()} atÃ© {analyzer.dates.max()}")
                            tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
                                "ğŸ” Isolados vs Agrupados",
                                "ğŸ“Š BÃ¡sico", 
                                "â° Temporais", 
                                "ğŸ’¥ Rajadas", 
                                "ğŸ“ˆ TendÃªncias", 
                                "ğŸš¨ Anomalias", 
                                "ğŸ”® PrevisÃµes"
                            ])
                            with tab1:
                                analyzer.show_individual_alert_analysis()
                            with tab2:
                                analyzer.show_basic_stats()
                            with tab3:
                                analyzer.show_temporal_patterns()
                            with tab4:
                                analyzer.show_burst_analysis()
                            with tab5:
                                analyzer.show_trend_analysis()
                            with tab6:
                                analyzer.show_anomaly_detection()
                            with tab7:
                                analyzer.show_predictions()
                            st.sidebar.markdown("---")
                            st.sidebar.subheader("ğŸ“¥ Download")
                            csv_buffer = io.StringIO()
                            analyzer.df.to_csv(csv_buffer, index=False)
                            st.sidebar.download_button(
                                label="â¬‡ï¸ Baixar Dados Processados",
                                data=csv_buffer.getvalue(),
                                file_name=f"analise_{selected_id}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.error(f"âŒ Nenhum registro encontrado para alert_id: {selected_id}")
                except Exception as e:
                    st.error(f"âŒ Erro ao processar anÃ¡lise individual: {e}")
    else:
        st.info("ğŸ‘† FaÃ§a upload de um arquivo CSV para comeÃ§ar a anÃ¡lise")
        with st.expander("ğŸ“– InstruÃ§Ãµes de Uso"):
            st.markdown("""
            ### Como usar este analisador:
            
            #### ğŸ›ï¸ **ParÃ¢metros de Agrupamento** (ConfigurÃ¡vel na Sidebar)
            
            **â±ï¸ Gap MÃ¡ximo Entre Alertas:**
            - Define o intervalo mÃ¡ximo (em horas) entre alertas para considerÃ¡-los do mesmo grupo
            - PadrÃ£o: 24 horas
            - Alertas separados por mais tempo sÃ£o considerados de grupos diferentes
            
            **ğŸ“Š Tamanho MÃ­nimo do Grupo:**
            - NÃºmero mÃ­nimo de alertas necessÃ¡rios para formar um grupo vÃ¡lido
            - PadrÃ£o: 3 alertas
            - Grupos menores sÃ£o desconsiderados e seus alertas marcados como isolados
            
            **ğŸš€ Multiplicador de Spike:**
            - Define quando um dia tem alertas "desproporcionais"
            - PadrÃ£o: 5x a mÃ©dia diÃ¡ria
            - Dias com alertas acima deste threshold sÃ£o marcados como spikes isolados
            
            #### ğŸŒ **AnÃ¡lise Global**
            1. **ğŸ“ Upload do arquivo:** Carregue um arquivo CSV com os dados dos alertas
            2. **ğŸ¯ Selecione "AnÃ¡lise Global"** no modo de anÃ¡lise
            3. **ğŸ›ï¸ Ajuste os parÃ¢metros** de agrupamento se necessÃ¡rio
            4. **âš¡ Ative Multiprocessing** para processamento mais rÃ¡pido (recomendado)
            5. **ğŸš€ Clique em "Executar AnÃ¡lise Global"**
            6. **ğŸ“Š Explore os resultados** nas diferentes abas:
               - **VisÃ£o Geral:** EstatÃ­sticas gerais e top alertas
               - **Isolados vs ContÃ­nuos:** ComparaÃ§Ã£o detalhada de padrÃµes baseados em grupos
               - **Agrupamento:** Clustering por comportamento
               - **Perfis dos Clusters:** CaracterÃ­sticas de cada grupo
               - **RecomendaÃ§Ãµes:** SugestÃµes de aÃ§Ã£o
            
            #### ğŸ” **AnÃ¡lise Individual**
            1. **ğŸ“ Upload do arquivo:** Carregue um arquivo CSV com os dados dos alertas
            2. **ğŸ¯ Selecione "AnÃ¡lise Individual"** no modo de anÃ¡lise
            3. **ğŸ›ï¸ Ajuste os parÃ¢metros** de agrupamento se necessÃ¡rio
            4. **ğŸ¯ Escolha um Alert ID** especÃ­fico
            5. **ğŸš€ Clique em "Executar AnÃ¡lise Individual"**
            6. **ğŸ“Š Navegue pelas abas** para ver diferentes anÃ¡lises:
               - **Isolados vs Agrupados:** VisualizaÃ§Ã£o dos grupos identificados
               - **BÃ¡sico:** EstatÃ­sticas fundamentais
               - **Temporais:** PadrÃµes de horÃ¡rio e dia
               - **Rajadas:** DetecÃ§Ã£o de bursts
               - **TendÃªncias:** EvoluÃ§Ã£o temporal
               - **Anomalias:** Outliers detectados
               - **PrevisÃµes:** Insights preditivos
            
            ### Colunas necessÃ¡rias no CSV:
            - `u_alert_id`: Identificador Ãºnico do alerta
            - `created_on`: Data e hora da criaÃ§Ã£o do alerta
            
            ### ğŸ†• **Nova LÃ³gica de ClassificaÃ§Ã£o:**
            
            **ğŸ“¦ Baseada em Grupos:**
            - Alertas sÃ£o agrupados baseado no gap de tempo entre eles
            - Grupos pequenos (< tamanho mÃ­nimo) sÃ£o desconsiderados
            - Dias com spikes desproporcionais sÃ£o isolados automaticamente
            
            **ğŸ”´ Alertas Isolados:**
            - Sem grupos vÃ¡lidos identificados
            - Maioria das ocorrÃªncias sÃ£o esparsas
            - Incluem dias com explosÃ£o anormal de alertas
            
            **ğŸŸ¢ Alertas ContÃ­nuos:**
            - 2+ grupos bem definidos identificados
            - PadrÃ£o recorrente e consistente
            - Maioria das ocorrÃªncias estÃ£o agrupadas
            
            ### ğŸš€ **Multiprocessing!**
            - **âš¡ Processamento Paralelo:** Usa mÃºltiplos nÃºcleos da CPU
            - **ğŸ“ˆ Muito mais rÃ¡pido:** Ideal para grandes volumes de dados
            - **ğŸ”§ AutomÃ¡tico:** Detecta nÃºmero ideal de processos
            - **ğŸ’¾ Fallback seguro:** Volta para modo sequencial se houver problemas
            """)

if __name__ == "__main__":
    main()