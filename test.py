import pandas as pd
import streamlit as st
import numpy as np
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from scipy import stats, signal
from scipy.fft import fft, fftfreq
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.ensemble import IsolationForest
import io
import warnings
from multiprocessing import Pool, cpu_count
import holidays
from functools import partial
from collections import defaultdict, Counter
import math

warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="Analisador de Alertas - Completo",
    page_icon="ğŸš¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AdvancedRecurrenceAnalyzer:
    """Analisador avanÃ§ado de padrÃµes de reincidÃªncia com todas as anÃ¡lises integradas"""
    
    def __init__(self, df, alert_id):
        self.df = df.copy() if df is not None else None
        self.alert_id = alert_id
        self.cache = {}
        
    def _cache_result(self, key, func):
        """Cache de resultados para otimizaÃ§Ã£o"""
        if key not in self.cache:
            self.cache[key] = func()
        return self.cache[key]
    
    def _prepare_data(self):
        """PreparaÃ§Ã£o otimizada dos dados"""
        if self.df is None or len(self.df) < 3:
            return None
            
        df = self.df.sort_values('created_on').copy()
        
        # Vetorizar operaÃ§Ãµes de timestamp
        df['timestamp'] = df['created_on'].astype('int64') // 10**9
        df['time_diff_seconds'] = df['timestamp'].diff()
        df['time_diff_hours'] = df['time_diff_seconds'] / 3600
        df['time_diff_days'] = df['time_diff_seconds'] / 86400
        
        # Extrair componentes temporais de uma vez
        if 'hour' not in df.columns:
            df['hour'] = df['created_on'].dt.hour
        if 'day_of_week' not in df.columns:
            df['day_of_week'] = df['created_on'].dt.dayofweek
        if 'day_of_month' not in df.columns:
            df['day_of_month'] = df['created_on'].dt.day
        if 'week_of_year' not in df.columns:
            df['week_of_year'] = df['created_on'].dt.isocalendar().week
        if 'month' not in df.columns:
            df['month'] = df['created_on'].dt.month
        if 'day_name' not in df.columns:
            df['day_name'] = df['created_on'].dt.day_name()
        
        return df
    
    def analyze(self):
        """MÃ©todo principal de anÃ¡lise - VERSÃƒO COMPLETA"""
        st.header("ğŸ”„ AnÃ¡lise AvanÃ§ada de ReincidÃªncia Temporal")
        
        df = self._prepare_data()
        if df is None:
            st.warning("âš ï¸ Dados insuficientes (mÃ­nimo 3 ocorrÃªncias).")
            return
        
        st.info(f"ğŸ“Š Analisando **{len(df)}** ocorrÃªncias do Short CI: **{self.alert_id}**")
        
        intervals_hours = df['time_diff_hours'].dropna().values
        if len(intervals_hours) < 2:
            st.warning("âš ï¸ Intervalos insuficientes.")
            return
        
        # Executar apenas anÃ¡lises essenciais para determinar reincidÃªncia
        results = {}
        results['basic_stats'] = self._analyze_basic_statistics(intervals_hours)
        results['regularity'] = self._analyze_regularity(intervals_hours)
        results['periodicity'] = self._analyze_periodicity(intervals_hours)
        results['autocorr'] = self._analyze_autocorrelation(intervals_hours)
        results['temporal'] = self._analyze_temporal_patterns(df)
        results['clusters'] = self._analyze_clusters(df, intervals_hours)
        results['bursts'] = self._detect_bursts(intervals_hours)
        results['seasonality'] = self._analyze_seasonality(df)
        results['predictability'] = self._calculate_predictability(intervals_hours)
        results['stability'] = self._analyze_stability(intervals_hours, df)
        
        # NOVAS ANÃLISES AVANÃ‡ADAS
        results['contextual'] = self._analyze_contextual_dependencies(df)
        results['vulnerability'] = self._identify_vulnerability_windows(df, intervals_hours)
        results['maturity'] = self._analyze_pattern_maturity(df, intervals_hours)
        results['prediction_confidence'] = self._calculate_prediction_confidence(intervals_hours)
        results['markov'] = self._analyze_markov_chains(intervals_hours)
        results['randomness'] = self._advanced_randomness_tests(intervals_hours)
        
        # ClassificaÃ§Ã£o final consolidada
        self._final_classification(results, df, intervals_hours)
    
    # ============================================================
    # ANÃLISES BÃSICAS (mantidas do cÃ³digo original)
    # ============================================================
    
    def _analyze_basic_statistics(self, intervals):
        """EstatÃ­sticas bÃ¡sicas otimizadas"""
        st.subheader("ğŸ“Š 1. EstatÃ­sticas de Intervalos")
        
        stats_dict = {
            'mean': np.mean(intervals),
            'median': np.median(intervals),
            'std': np.std(intervals),
            'min': np.min(intervals),
            'max': np.max(intervals),
            'cv': np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float('inf'),
            'q25': np.percentile(intervals, 25),
            'q75': np.percentile(intervals, 75),
            'iqr': np.percentile(intervals, 75) - np.percentile(intervals, 25)
        }
        
        col1, col2, col3, col4, col5 = st.columns(5)
        col1.metric("â±ï¸ MÃ©dia", f"{stats_dict['mean']:.1f}h")
        col2.metric("ğŸ“Š Mediana", f"{stats_dict['median']:.1f}h")
        col3.metric("ğŸ“ˆ Desvio", f"{stats_dict['std']:.1f}h")
        col4.metric("âš¡ MÃ­nimo", f"{stats_dict['min']:.1f}h")
        col5.metric("ğŸŒ MÃ¡ximo", f"{stats_dict['max']:.1f}h")
        
        return stats_dict
    
    def _analyze_regularity(self, intervals):
        """AnÃ¡lise de regularidade com testes estatÃ­sticos"""
        st.subheader("ğŸ¯ 2. Regularidade e Aleatoriedade")
        
        cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float('inf')
        
        if cv < 0.15:
            pattern_type = "ğŸŸ¢ ALTAMENTE REGULAR"
            pattern_color = "green"
            regularity_score = 95
        elif cv < 0.35:
            pattern_type = "ğŸŸ¢ REGULAR"
            pattern_color = "lightgreen"
            regularity_score = 80
        elif cv < 0.65:
            pattern_type = "ğŸŸ¡ SEMI-REGULAR"
            pattern_color = "yellow"
            regularity_score = 60
        elif cv < 1.0:
            pattern_type = "ğŸŸ  IRREGULAR"
            pattern_color = "orange"
            regularity_score = 40
        else:
            pattern_type = "ğŸ”´ ALTAMENTE IRREGULAR"
            pattern_color = "red"
            regularity_score = 20
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown(f"**ClassificaÃ§Ã£o:** {pattern_type}")
            st.write(f"**CV:** {cv:.2%}")
            
            # Teste de Shapiro-Wilk para normalidade
            if len(intervals) >= 3:
                _, p_value = stats.shapiro(intervals)
                if p_value > 0.05:
                    st.info("ğŸ“Š **Normalidade:** DistribuiÃ§Ã£o aproximadamente normal")
                else:
                    st.warning("ğŸ“Š **Normalidade:** DistribuiÃ§Ã£o nÃ£o-normal")
        
        with col2:
            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=regularity_score,
                title={'text': "Regularidade"},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'color': pattern_color},
                    'steps': [
                        {'range': [0, 40], 'color': "lightgray"},
                        {'range': [40, 70], 'color': "lightyellow"},
                        {'range': [70, 100], 'color': "lightgreen"}
                    ]
                }
            ))
            fig.update_layout(height=250)
            st.plotly_chart(fig, use_container_width=True, key='reg_gauge')
        
        return {'cv': cv, 'score': regularity_score, 'type': pattern_type}
    
    def _analyze_periodicity(self, intervals):
        """AnÃ¡lise de periodicidade com FFT"""
        st.subheader("ğŸ” 3. Periodicidade (FFT)")
        
        if len(intervals) < 10:
            st.info("ğŸ“Š MÃ­nimo de 10 intervalos necessÃ¡rios")
            return {}
        
        intervals_norm = (intervals - np.mean(intervals)) / np.std(intervals)
        n_padded = 2**int(np.ceil(np.log2(len(intervals_norm))))
        intervals_padded = np.pad(intervals_norm, (0, n_padded - len(intervals_norm)), 'constant')
        
        fft_vals = fft(intervals_padded)
        freqs = fftfreq(n_padded, d=1)
        
        positive_idx = freqs > 0
        freqs_pos = freqs[positive_idx]
        fft_mag = np.abs(fft_vals[positive_idx])
        
        threshold = np.mean(fft_mag) + 2 * np.std(fft_mag)
        peaks_idx = fft_mag > threshold
        
        dominant_periods = []
        if np.any(peaks_idx):
            dominant_freqs = freqs_pos[peaks_idx]
            dominant_periods = 1 / dominant_freqs
            dominant_periods = dominant_periods[dominant_periods < len(intervals)][:3]
            
            st.success("ğŸ¯ **Periodicidades Detectadas:**")
            for period in dominant_periods:
                est_time = period * np.mean(intervals)
                time_str = f"{est_time:.1f}h" if est_time < 24 else f"{est_time/24:.1f} dias"
                st.write(f"â€¢ PerÃ­odo: **{period:.1f}** ocorrÃªncias (~{time_str})")
        else:
            st.info("ğŸ“Š Nenhuma periodicidade forte detectada")
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=1/freqs_pos[:len(freqs_pos)//4],
            y=fft_mag[:len(freqs_pos)//4],
            mode='lines',
            fill='tozeroy'
        ))
        fig.update_layout(
            title="Espectro de FrequÃªncia",
            xaxis_title="PerÃ­odo",
            yaxis_title="Magnitude",
            height=300,
            xaxis_type="log"
        )
        st.plotly_chart(fig, use_container_width=True, key='fft')
        
        return {'periods': dominant_periods, 'has_periodicity': len(dominant_periods) > 0}
    
    def _analyze_autocorrelation(self, intervals):
        """AnÃ¡lise de autocorrelaÃ§Ã£o"""
        st.subheader("ğŸ“ˆ 4. AutocorrelaÃ§Ã£o")
        
        if len(intervals) < 5:
            return {}
        
        intervals_norm = (intervals - np.mean(intervals)) / np.std(intervals)
        autocorr = signal.correlate(intervals_norm, intervals_norm, mode='full')
        autocorr = autocorr[len(autocorr)//2:]
        autocorr = autocorr / autocorr[0]
        
        lags = np.arange(len(autocorr))
        threshold = 2 / np.sqrt(len(intervals))
        
        significant_peaks = [(i, autocorr[i]) for i in range(1, min(len(autocorr), 20)) 
                           if autocorr[i] > threshold]
        
        if significant_peaks:
            st.success("âœ… **AutocorrelaÃ§Ã£o Significativa:**")
            for lag, corr in significant_peaks[:3]:
                st.write(f"â€¢ Lag {lag}: {corr:.2f}")
        else:
            st.info("ğŸ“Š Sem autocorrelaÃ§Ã£o significativa")
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=lags[:min(30, len(lags))],
            y=autocorr[:min(30, len(autocorr))],
            mode='lines+markers'
        ))
        fig.add_hline(y=threshold, line_dash="dash", line_color="red")
        fig.add_hline(y=-threshold, line_dash="dash", line_color="red")
        fig.update_layout(title="AutocorrelaÃ§Ã£o", height=300)
        st.plotly_chart(fig, use_container_width=True, key='autocorr')
        
        return {'peaks': significant_peaks, 'has_autocorr': len(significant_peaks) > 0}
    
    def _analyze_temporal_patterns(self, df):
        """AnÃ¡lise de padrÃµes temporais"""
        st.subheader("â° 5. PadrÃµes Temporais")
        
        hourly = df.groupby('hour').size()
        hourly = hourly.reindex(range(24), fill_value=0)
        
        daily = df.groupby('day_of_week').size()
        daily = daily.reindex(range(7), fill_value=0)
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = go.Figure(go.Bar(
                x=list(range(24)),
                y=hourly.values,
                marker_color=['red' if v > hourly.mean() + hourly.std() else 'lightblue' 
                            for v in hourly.values]
            ))
            fig.update_layout(title="Por Hora", xaxis_title="Hora", height=250)
            st.plotly_chart(fig, use_container_width=True, key='hourly')
            
            peak_hours = hourly[hourly > hourly.mean() + hourly.std()].index.tolist()
            if peak_hours:
                st.success(f"ğŸ• **Picos:** {', '.join([f'{h:02d}:00' for h in peak_hours])}")
        
        with col2:
            days_map = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'SÃ¡b', 'Dom']
            fig = go.Figure(go.Bar(
                x=days_map,
                y=daily.values,
                marker_color=['red' if v > daily.mean() + daily.std() else 'lightgreen' 
                            for v in daily.values]
            ))
            fig.update_layout(title="Por Dia", xaxis_title="Dia", height=250)
            st.plotly_chart(fig, use_container_width=True, key='daily')
            
            peak_days = daily[daily > daily.mean() + daily.std()].index.tolist()
            if peak_days:
                st.success(f"ğŸ“… **Picos:** {', '.join([days_map[d] for d in peak_days])}")
        
        hourly_pct = (hourly / hourly.sum() * 100) if hourly.sum() > 0 else pd.Series()
        daily_pct = (daily / daily.sum() * 100) if daily.sum() > 0 else pd.Series()
        
        hourly_conc = hourly_pct.nlargest(3).sum() if len(hourly_pct) > 0 else 0
        daily_conc = daily_pct.nlargest(3).sum() if len(daily_pct) > 0 else 0
        
        return {
            'hourly_concentration': hourly_conc,
            'daily_concentration': daily_conc,
            'peak_hours': peak_hours,
            'peak_days': peak_days
        }
    
    def _analyze_clusters(self, df, intervals):
        """DetecÃ§Ã£o de clusters temporais"""
        st.subheader("ğŸ¯ 6. Clusters Temporais")
        
        if len(df) < 10:
            st.info("MÃ­nimo de 10 ocorrÃªncias necessÃ¡rio")
            return {}
        
        first_ts = df['timestamp'].min()
        time_features = ((df['timestamp'] - first_ts) / 3600).values.reshape(-1, 1)
        
        eps = np.median(intervals) * 2
        dbscan = DBSCAN(eps=eps, min_samples=3)
        clusters = dbscan.fit_predict(time_features)
        
        n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
        n_noise = list(clusters).count(-1)
        
        col1, col2, col3 = st.columns(3)
        col1.metric("ğŸ¯ Clusters", n_clusters)
        col2.metric("ğŸ“Š Em Clusters", len(clusters) - n_noise)
        col3.metric("ğŸ”´ Isolados", n_noise)
        
        if n_clusters > 0:
            st.success(f"âœ… **{n_clusters} clusters** identificados")
        
        return {'n_clusters': n_clusters, 'n_noise': n_noise}
    
    def _detect_bursts(self, intervals):
        """DetecÃ§Ã£o de bursts"""
        st.subheader("ğŸ’¥ 7. DetecÃ§Ã£o de Bursts")
        
        burst_threshold = np.percentile(intervals, 25)
        
        is_burst = intervals < burst_threshold
        burst_changes = np.diff(np.concatenate(([False], is_burst, [False])))
        burst_starts = np.where(burst_changes == 1)[0]
        burst_ends = np.where(burst_changes == -1)[0]
        
        burst_sequences = [(start, end) for start, end in zip(burst_starts, burst_ends) 
                          if end - start >= 3]
        
        col1, col2 = st.columns(2)
        col1.metric("ğŸ’¥ Bursts", len(burst_sequences))
        
        if burst_sequences:
            avg_size = np.mean([end - start for start, end in burst_sequences])
            col2.metric("ğŸ“Š Tamanho MÃ©dio", f"{avg_size:.1f}")
            st.warning(f"âš ï¸ **{len(burst_sequences)} bursts** detectados")
        else:
            st.success("âœ… Sem padrÃ£o de rajadas")
        
        return {'n_bursts': len(burst_sequences), 'has_bursts': len(burst_sequences) > 0}
    
    def _analyze_seasonality(self, df):
        """AnÃ¡lise de sazonalidade"""
        st.subheader("ğŸŒ¡ï¸ 8. Sazonalidade")
        
        date_range = (df['created_on'].max() - df['created_on'].min()).days
        
        if date_range < 30:
            st.info("ğŸ“Š PerÃ­odo curto para anÃ¡lise sazonal")
            return {}
        
        weekly = df.groupby('week_of_year').size()
        
        if len(weekly) >= 4:
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=weekly.index,
                y=weekly.values,
                mode='lines+markers',
                fill='tozeroy'
            ))
            fig.update_layout(title="EvoluÃ§Ã£o Semanal", height=250)
            st.plotly_chart(fig, use_container_width=True, key='weekly')
            
            if len(weekly) > 3:
                slope, _, _, p_value, _ = stats.linregress(weekly.index.values, weekly.values)
                if p_value < 0.05:
                    if slope > 0:
                        st.warning("ğŸ“ˆ **TendÃªncia crescente**")
                        return {'trend': 'increasing', 'slope': slope}
                    else:
                        st.success("ğŸ“‰ **TendÃªncia decrescente**")
                        return {'trend': 'decreasing', 'slope': slope}
        
        return {'trend': 'stable'}
    
    def _detect_changepoints(self, intervals):
        """DetecÃ§Ã£o de pontos de mudanÃ§a"""
        st.subheader("ğŸ”€ 9. Pontos de MudanÃ§a")
        
        if len(intervals) < 20:
            st.info("MÃ­nimo de 20 intervalos necessÃ¡rio")
            return {}
        
        cumsum = np.cumsum(intervals - np.mean(intervals))
        
        window = 5
        changes = []
        for i in range(window, len(cumsum) - window):
            before = np.mean(intervals[max(0, i-window):i])
            after = np.mean(intervals[i:min(len(intervals), i+window)])
            if abs(before - after) > np.std(intervals):
                changes.append(i)
        
        filtered = []
        for cp in changes:
            if not filtered or cp - filtered[-1] > 5:
                filtered.append(cp)
        
        if filtered:
            st.warning(f"âš ï¸ **{len(filtered)} pontos de mudanÃ§a** detectados")
            
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=list(range(len(cumsum))), y=cumsum, mode='lines'))
            for cp in filtered:
                fig.add_vline(x=cp, line_dash="dash", line_color="red")
            fig.update_layout(title="CUSUM", height=250)
            st.plotly_chart(fig, use_container_width=True, key='cusum')
        else:
            st.success("âœ… Comportamento estÃ¡vel")
        
        return {'changepoints': filtered, 'has_changes': len(filtered) > 0}
    
    def _detect_anomalies(self, intervals):
        """DetecÃ§Ã£o de anomalias"""
        st.subheader("ğŸš¨ 10. DetecÃ§Ã£o de Anomalias")
        
        z_scores = np.abs(stats.zscore(intervals))
        z_anomalies = np.sum(z_scores > 3)
        
        q1, q3 = np.percentile(intervals, [25, 75])
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        iqr_anomalies = np.sum((intervals < lower) | (intervals > upper))
        
        iso_anomalies = 0
        if len(intervals) >= 10:
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            predictions = iso_forest.fit_predict(intervals.reshape(-1, 1))
            iso_anomalies = np.sum(predictions == -1)
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Z-Score", f"{z_anomalies}")
        col2.metric("IQR", f"{iqr_anomalies}")
        col3.metric("Iso. Forest", f"{iso_anomalies}")
        
        total_anomalies = max(z_anomalies, iqr_anomalies, iso_anomalies)
        anomaly_rate = total_anomalies / len(intervals) * 100
        
        if anomaly_rate > 10:
            st.warning(f"âš ï¸ **{anomaly_rate:.1f}%** de anomalias detectadas")
        else:
            st.success("âœ… Baixa taxa de anomalias")
        
        return {'anomaly_rate': anomaly_rate, 'total_anomalies': total_anomalies}
    
    def _analyze_trend(self, df):
        """AnÃ¡lise de tendÃªncia temporal"""
        df_sorted = df.sort_values('created_on')
        
        df_sorted['week'] = df_sorted['created_on'].dt.to_period('W')
        weekly_counts = df_sorted.groupby('week').size()
        
        if len(weekly_counts) < 3:
            return {'has_trend': False}
        
        x = np.arange(len(weekly_counts))
        y = weekly_counts.values
        
        slope, _, _, p_value, _ = stats.linregress(x, y)
        
        has_trend = p_value < 0.05
        trend_type = 'increasing' if slope > 0 else 'decreasing'
        
        return {
            'has_trend': has_trend,
            'trend_type': trend_type if has_trend else 'stable',
            'slope': slope,
            'p_value': p_value
        }
    
    def _calculate_predictability(self, intervals):
        """Calcula score de previsibilidade"""
        if len(intervals) < 5:
            return {'score': 0}
        
        n_bins = min(10, len(intervals) // 3)
        hist, _ = np.histogram(intervals, bins=n_bins)
        probs = hist[hist > 0] / hist.sum()
        entropy = -np.sum(probs * np.log2(probs))
        max_entropy = np.log2(n_bins)
        norm_entropy = entropy / max_entropy if max_entropy > 0 else 1
        
        predictability_score = (1 - norm_entropy) * 100
        
        return {'score': predictability_score, 'entropy': norm_entropy}
    
    def _analyze_stability(self, intervals, df):
        """AnÃ¡lise de estabilidade do padrÃ£o"""
        if len(intervals) < 10:
            return {'is_stable': True}
        
        mid = len(intervals) // 2
        first_half = intervals[:mid]
        second_half = intervals[mid:]
        
        _, p_value = stats.ttest_ind(first_half, second_half)
        
        is_stable = p_value > 0.05
        
        mean_diff = abs(np.mean(second_half) - np.mean(first_half))
        drift_pct = (mean_diff / np.mean(first_half)) * 100 if np.mean(first_half) > 0 else 0
        
        return {
            'is_stable': is_stable,
            'drift_pct': drift_pct,
            'p_value': p_value
        }
    
    def _mine_patterns(self, intervals):
        """Mining de padrÃµes sequenciais"""
        if len(intervals) < 10:
            return {}
        
        q1, q2, q3 = np.percentile(intervals, [25, 50, 75])
        

    def analyze_silent(self):
        """MÃ©todo de anÃ¡lise SILENCIOSO para processamento em lote - retorna apenas resultados"""
        df = self._prepare_data()
        if df is None:
            return None
        
        intervals_hours = df['time_diff_hours'].dropna().values
        if len(intervals_hours) < 2:
            return None
        
        # Executar anÃ¡lises sem interface
        results = {}
        results['basic_stats'] = self._analyze_basic_statistics_silent(intervals_hours)
        results['regularity'] = self._analyze_regularity_silent(intervals_hours)
        results['periodicity'] = self._analyze_periodicity_silent(intervals_hours)
        results['autocorr'] = self._analyze_autocorrelation_silent(intervals_hours)
        results['predictability'] = self._calculate_predictability_silent(intervals_hours)
        results['markov'] = self._analyze_markov_chains_silent(intervals_hours)
        results['randomness'] = self._advanced_randomness_tests_silent(intervals_hours)
        
        # ClassificaÃ§Ã£o final
        final_score, classification = self._calculate_final_score(results)
        
        return {
            'short_ci': self.alert_id,
            'total_occurrences': len(df),
            'score': final_score,
            'classification': classification,
            'mean_interval_hours': results['basic_stats']['mean'],
            'median_interval_hours': results['basic_stats']['median'],
            'cv': results['basic_stats']['cv'],
            'regularity_score': results['regularity']['regularity_score'],
            'periodicity_detected': results['periodicity']['has_strong_periodicity'],
            'dominant_period_hours': results['periodicity'].get('dominant_period_hours'),
            'predictability_score': results['predictability']['predictability_score'],
            'next_occurrence_prediction_hours': results['predictability']['next_expected_hours']
        }
    
    def _calculate_final_score(self, results):
        """Calcula o score final de reincidÃªncia (0-100) - usado para anÃ¡lise em lote"""
        scores = {
            'regularity': results['regularity']['regularity_score'] * 0.20,
            'periodicity': (100 if results['periodicity']['has_strong_periodicity'] else 
                          (50 if results['periodicity'].get('has_moderate_periodicity', False) else 0)) * 0.20,
            'predictability': results['predictability']['predictability_score'] * 0.15,
            'determinism': (100 - results['randomness']['overall_randomness_score']) * 0.15,
            'autocorrelation': (results['autocorr']['max_autocorr'] * 100) * 0.10,
            'stability': results.get('stability', {}).get('stability_score', 50) * 0.10,
            'markov': results['markov']['markov_score'] * 0.10
        }
        
        final_score = sum(scores.values())
        
        if final_score >= 70:
            classification = "ğŸ”´ REINCIDENTE CRÃTICO (P1)"
        elif final_score >= 50:
            classification = "ğŸŸ  PARCIALMENTE REINCIDENTE (P2)"
        elif final_score >= 30:
            classification = "ğŸŸ¡ PADRÃƒO DETECTÃVEL (P3)"
        else:
            classification = "ğŸŸ¢ NÃƒO REINCIDENTE (P4)"
        
        return round(final_score, 2), classification
    
    # MÃ©todos silenciosos (versÃµes sem interface)
    def _analyze_basic_statistics_silent(self, intervals):
        return {
            'mean': np.mean(intervals),
            'median': np.median(intervals),
            'std': np.std(intervals),
            'cv': np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float('inf')
        }
    
    def _analyze_regularity_silent(self, intervals):
        cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float('inf')
        if cv < 0.15:
            regularity_score = 95
        elif cv < 0.35:
            regularity_score = 80
        elif cv < 0.65:
            regularity_score = 60
        elif cv < 1.0:
            regularity_score = 40
        else:
            regularity_score = 20
        return {'cv': cv, 'regularity_score': regularity_score}
    
    def _analyze_periodicity_silent(self, intervals):
        if len(intervals) < 4:
            return {'has_strong_periodicity': False, 'has_moderate_periodicity': False, 'dominant_period_hours': None}
        try:
            N = len(intervals)
            yf = fft(intervals)
            xf = fftfreq(N, d=1)[:N//2]
            power = 2.0/N * np.abs(yf[:N//2])
            if len(power) > 1:
                peak_idx = np.argmax(power[1:]) + 1
                dominant_freq = xf[peak_idx]
                dominant_period = 1/dominant_freq if dominant_freq != 0 else None
                mean_interval = np.mean(intervals)
                dominant_period_hours = dominant_period * mean_interval if dominant_period else None
                peak_power = power[peak_idx]
                mean_power = np.mean(power)
                strength_ratio = peak_power / mean_power if mean_power > 0 else 0
                has_strong = strength_ratio > 3.0
                has_moderate = 1.5 < strength_ratio <= 3.0
                return {
                    'has_strong_periodicity': has_strong,
                    'has_moderate_periodicity': has_moderate,
                    'dominant_period_hours': dominant_period_hours
                }
        except:
            pass
        return {'has_strong_periodicity': False, 'has_moderate_periodicity': False, 'dominant_period_hours': None}
    
    def _analyze_autocorrelation_silent(self, intervals):
        if len(intervals) < 5:
            return {'max_autocorr': 0}
        try:
            max_lag = min(len(intervals) // 2, 20)
            autocorr_values = []
            for lag in range(1, max_lag + 1):
                if lag < len(intervals):
                    corr = np.corrcoef(intervals[:-lag], intervals[lag:])[0, 1]
                    if not np.isnan(corr):
                        autocorr_values.append(abs(corr))
            return {'max_autocorr': max(autocorr_values) if autocorr_values else 0}
        except:
            return {'max_autocorr': 0}
    
    def _calculate_predictability_silent(self, intervals):
        cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else float('inf')
        if cv < 0.2:
            predictability = 95
        elif cv < 0.4:
            predictability = 80
        elif cv < 0.7:
            predictability = 60
        elif cv < 1.2:
            predictability = 40
        else:
            predictability = 20
        mean_interval = np.mean(intervals)
        return {'predictability_score': predictability, 'next_expected_hours': mean_interval}
    
    def _analyze_markov_chains_silent(self, intervals):
        if len(intervals) < 5:
            return {'markov_score': 0}
        try:
            bins = np.percentile(intervals, [0, 33, 67, 100])
            states = np.digitize(intervals, bins[1:-1])
            n_states = 3
            transition_matrix = np.zeros((n_states, n_states))
            for i in range(len(states) - 1):
                current = states[i]
                next_state = states[i + 1]
                transition_matrix[current, next_state] += 1
            row_sums = transition_matrix.sum(axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1
            transition_matrix = transition_matrix / row_sums
            max_probs = transition_matrix.max(axis=1)
            markov_score = np.mean(max_probs) * 100
            return {'markov_score': markov_score}
        except:
            return {'markov_score': 0}
    
    def _advanced_randomness_tests_silent(self, intervals):
        if len(intervals) < 5:
            return {'overall_randomness_score': 50}
        try:
            median = np.median(intervals)
            runs = np.diff(intervals > median).sum() + 1
            expected_runs = len(intervals) / 2
            runs_score = min(abs(runs - expected_runs) / expected_runs * 100, 100)
            overall_randomness = runs_score
            return {'overall_randomness_score': overall_randomness}
        except:
            return {'overall_randomness_score': 50}

        def categorize(val):
            if val <= q1:
                return 'A'
            elif val <= q2:
                return 'B'
            elif val <= q3:
                return 'C'
            else:
                return 'D'
        
        sequence = ''.join([categorize(i) for i in intervals])
        
        patterns = defaultdict(int)
        for n in [2, 3]:
            for i in range(len(sequence) - n + 1):
                ngram = sequence[i:i+n]
                patterns[ngram] += 1
        
        frequent = {k: v for k, v in patterns.items() if v >= 3}
        
        return {'frequent_patterns': frequent, 'has_patterns': len(frequent) > 0}
    
    # ============================================================
    # NOVAS ANÃLISES AVANÃ‡ADAS
    # ============================================================
    
    def _analyze_contextual_dependencies(self, df):
        """AnÃ¡lise de dependÃªncias contextuais"""
        st.subheader("ğŸŒ 11. DependÃªncias Contextuais")
        
        try:
            br_holidays = holidays.Brazil(years=df['created_on'].dt.year.unique())
            df['is_holiday'] = df['created_on'].dt.date.apply(lambda x: x in br_holidays)
        except:
            df['is_holiday'] = False
            st.info("âš ï¸ Biblioteca holidays nÃ£o disponÃ­vel - anÃ¡lise de feriados desabilitada")
        
        business_days = df[~df['is_weekend'] & ~df['is_holiday']]
        weekend_days = df[df['is_weekend']]
        holiday_days = df[df['is_holiday']]
        
        col1, col2, col3 = st.columns(3)
        col1.metric("ğŸ“Š Dias Ãšteis", f"{len(business_days)/len(df)*100:.1f}%")
        col2.metric("ğŸ‰ Fins de Semana", f"{len(weekend_days)/len(df)*100:.1f}%")
        col3.metric("ğŸŠ Feriados", f"{len(holiday_days)/len(df)*100:.1f}%")
        
        if len(holiday_days) > 0:
            st.warning(f"âš ï¸ {len(holiday_days)} alertas em feriados detectados")
        
        return {
            'holiday_correlation': len(holiday_days) / len(df) if len(df) > 0 else 0,
            'weekend_correlation': len(weekend_days) / len(df) if len(df) > 0 else 0
        }
    
    def _identify_vulnerability_windows(self, df, intervals):
        """Identifica janelas temporais de alta vulnerabilidade"""
        st.subheader("ğŸ¯ 12. Janelas de Vulnerabilidade")
        
        # Criar grade horÃ¡ria/semanal
        vulnerability_matrix = df.groupby(['day_of_week', 'hour']).size().reset_index(name='count')
        vulnerability_matrix['risk_score'] = (
            vulnerability_matrix['count'] / vulnerability_matrix['count'].max() * 100
        )
        
        # Identificar top 5 janelas mais crÃ­ticas
        top_windows = vulnerability_matrix.nlargest(5, 'risk_score')
        
        day_map = {0: 'Seg', 1: 'Ter', 2: 'Qua', 3: 'Qui', 4: 'Sex', 5: 'SÃ¡b', 6: 'Dom'}
        
        st.write("**ğŸ”´ Top 5 Janelas CrÃ­ticas:**")
        for idx, row in top_windows.iterrows():
            day = day_map[row['day_of_week']]
            hour = int(row['hour'])
            risk = row['risk_score']
            st.write(f"â€¢ **{day} {hour:02d}:00** - Score: {risk:.1f} ({row['count']} alertas)")
        
        return {'top_windows': top_windows.to_dict('records')}
    
    def _analyze_pattern_maturity(self, df, intervals):
        """Analisa maturidade e evoluÃ§Ã£o do padrÃ£o"""
        st.subheader("ğŸ“ˆ 13. Maturidade do PadrÃ£o")
        
        # Dividir em perÃ­odos
        n_periods = 4
        period_size = len(intervals) // n_periods
        
        if period_size < 2:
            st.info("PerÃ­odo insuficiente para anÃ¡lise de maturidade")
            return {}
        
        periods_stats = []
        for i in range(n_periods):
            start = i * period_size
            end = (i + 1) * period_size if i < n_periods - 1 else len(intervals)
            period_intervals = intervals[start:end]
            
            periods_stats.append({
                'period': i + 1,
                'mean': np.mean(period_intervals),
                'cv': np.std(period_intervals) / np.mean(period_intervals) if np.mean(period_intervals) > 0 else 0
            })
        
        periods_df = pd.DataFrame(periods_stats)
        
        # Visualizar evoluÃ§Ã£o
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=periods_df['period'],
            y=periods_df['cv'],
            mode='lines+markers',
            name='CV (Variabilidade)',
            line=dict(color='red', width=3)
        ))
        fig.update_layout(
            title="EvoluÃ§Ã£o da Variabilidade ao Longo do Tempo",
            xaxis_title="PerÃ­odo",
            yaxis_title="CV (Coeficiente de VariaÃ§Ã£o)",
            height=300
        )
        st.plotly_chart(fig, use_container_width=True, key='maturity')
        
        # TendÃªncia de maturidade
        slope = np.polyfit(periods_df['period'], periods_df['cv'], 1)[0]
        
        if slope < -0.05:
            st.success("âœ… **PadrÃ£o amadurecendo**: Variabilidade decrescente")
            maturity = "maturing"
        elif slope > 0.05:
            st.warning("âš ï¸ **PadrÃ£o degradando**: Variabilidade crescente")
            maturity = "degrading"
        else:
            st.info("ğŸ“Š **PadrÃ£o estÃ¡vel**: Variabilidade constante")
            maturity = "stable"
        
        return {'maturity': maturity, 'slope': slope}
    
    def _calculate_prediction_confidence(self, intervals):
        """Calcula confianÃ§a estatÃ­stica da prediÃ§Ã£o"""
        if len(intervals) < 10:
            return {'confidence': 'low', 'score': 0}
        
        # Calcular mÃºltiplas mÃ©tricas de confianÃ§a
        cv = np.std(intervals) / np.mean(intervals)
        n_samples = len(intervals)
        
        # Score baseado em:
        # 1. Regularidade (CV baixo = alta confianÃ§a)
        # 2. Quantidade de dados (mais dados = mais confianÃ§a)
        # 3. TendÃªncia estacionÃ¡ria
        
        regularity_score = max(0, 100 - cv * 100)
        sample_score = min(100, (n_samples / 50) * 100)
        
        # Teste de estacionariedade simples (variÃ¢ncia constante)
        mid = len(intervals) // 2
        var1 = np.var(intervals[:mid])
        var2 = np.var(intervals[mid:])
        var_ratio = min(var1, var2) / max(var1, var2) if max(var1, var2) > 0 else 0
        stationarity_score = var_ratio * 100
        
        confidence_score = (regularity_score * 0.5 + 
                           sample_score * 0.3 + 
                           stationarity_score * 0.2)
        
        if confidence_score > 70:
            confidence = 'high'
        elif confidence_score > 40:
            confidence = 'medium'
        else:
            confidence = 'low'
        
        return {'confidence': confidence, 'score': confidence_score}
    
    def _analyze_multivariate_patterns(self, df):
        """AnÃ¡lise de padrÃµes multivariados"""
        st.subheader("ğŸ”¬ 14. PadrÃµes Multivariados")
        
        # Criar matriz de correlaÃ§Ã£o entre features temporais
        features = df[['hour', 'day_of_week', 'is_weekend', 'is_business_hours']].copy()
        features['alert'] = 1
        
        correlation_matrix = features.corr()
        
        fig = go.Figure(data=go.Heatmap(
            z=correlation_matrix.values,
            x=correlation_matrix.columns,
            y=correlation_matrix.columns,
            colorscale='RdBu',
            zmid=0
        ))
        fig.update_layout(title="Matriz de CorrelaÃ§Ã£o", height=400)
        st.plotly_chart(fig, use_container_width=True, key='multivariate')
        
        # Identificar correlaÃ§Ãµes fortes
        strong_corr = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr = correlation_matrix.iloc[i, j]
                if abs(corr) > 0.5:
                    strong_corr.append({
                        'var1': correlation_matrix.columns[i],
                        'var2': correlation_matrix.columns[j],
                        'correlation': corr
                    })
        
        if strong_corr:
            st.success(f"âœ… {len(strong_corr)} correlaÃ§Ãµes fortes detectadas")
            for item in strong_corr:
                st.write(f"â€¢ **{item['var1']}** â†” **{item['var2']}**: {item['correlation']:.2f}")
        
        return {'strong_correlations': strong_corr}
    
    def _analyze_markov_chains(self, intervals):
        """AnÃ¡lise de Cadeias de Markov para padrÃµes de transiÃ§Ã£o"""
        st.subheader("ğŸ”— 15. AnÃ¡lise de Cadeias de Markov")
        
        if len(intervals) < 20:
            st.info("MÃ­nimo de 20 intervalos necessÃ¡rio")
            return {}
        
        # Discretizar intervalos em estados
        q25, q50, q75 = np.percentile(intervals, [25, 50, 75])
        
        def interval_to_state(val):
            if val <= q25:
                return 'Muito Curto'
            elif val <= q50:
                return 'Curto'
            elif val <= q75:
                return 'Normal'
            else:
                return 'Longo'
        
        states = [interval_to_state(i) for i in intervals]
        state_labels = ['Muito Curto', 'Curto', 'Normal', 'Longo']
        
        # Construir matriz de transiÃ§Ã£o
        n_states = len(state_labels)
        transition_matrix = np.zeros((n_states, n_states))
        state_to_idx = {state: idx for idx, state in enumerate(state_labels)}
        
        for i in range(len(states) - 1):
            from_state = state_to_idx[states[i]]
            to_state = state_to_idx[states[i + 1]]
            transition_matrix[from_state, to_state] += 1
        
        # Normalizar para probabilidades
        row_sums = transition_matrix.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1
        transition_probs = transition_matrix / row_sums
        
        # Visualizar matriz de transiÃ§Ã£o
        fig = go.Figure(data=go.Heatmap(
            z=transition_probs,
            x=state_labels,
            y=state_labels,
            text=np.round(transition_probs, 2),
            texttemplate='%{text:.2f}',
            textfont={"size": 12},
            colorscale='Blues',
            colorbar=dict(title="Probabilidade")
        ))
        
        fig.update_layout(
            title="Matriz de TransiÃ§Ã£o de Estados (Markov)",
            xaxis_title="Estado Seguinte",
            yaxis_title="Estado Atual",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True, key='markov_matrix')
        
        # Calcular distribuiÃ§Ã£o estacionÃ¡ria
        try:
            eigenvalues, eigenvectors = np.linalg.eig(transition_probs.T)
            stationary_idx = np.argmin(np.abs(eigenvalues - 1))
            stationary_dist = np.real(eigenvectors[:, stationary_idx])
            stationary_dist = stationary_dist / stationary_dist.sum()
            
            st.write("**ğŸ“Š DistribuiÃ§Ã£o EstacionÃ¡ria (Longo Prazo):**")
            col1, col2, col3, col4 = st.columns(4)
            cols = [col1, col2, col3, col4]
            for idx, (state, prob) in enumerate(zip(state_labels, stationary_dist)):
                cols[idx].metric(state, f"{prob*100:.1f}%")
            
        except:
            st.warning("âš ï¸ NÃ£o foi possÃ­vel calcular distribuiÃ§Ã£o estacionÃ¡ria")
            stationary_dist = None
        
        # Identificar transiÃ§Ãµes mais provÃ¡veis
        st.write("**ğŸ”¥ Top 5 TransiÃ§Ãµes Mais ProvÃ¡veis:**")
        transitions = []
        for i in range(n_states):
            for j in range(n_states):
                if transition_probs[i, j] > 0:
                    transitions.append({
                        'from': state_labels[i],
                        'to': state_labels[j],
                        'prob': transition_probs[i, j]
                    })
        
        transitions_sorted = sorted(transitions, key=lambda x: x['prob'], reverse=True)[:5]
        for trans in transitions_sorted:
            st.write(f"â€¢ **{trans['from']}** â†’ **{trans['to']}**: {trans['prob']*100:.1f}%")
        
        # Calcular entropia da cadeia
        entropy = 0
        for i in range(n_states):
            for j in range(n_states):
                if transition_probs[i, j] > 0:
                    entropy += transition_probs[i, j] * np.log2(transition_probs[i, j])
        entropy = -entropy / n_states
        
        max_entropy = np.log2(n_states)
        predictability = (1 - entropy / max_entropy) * 100 if max_entropy > 0 else 0
        
        st.metric("ğŸ¯ Previsibilidade Markoviana", f"{predictability:.1f}%")
        
        if predictability > 60:
            st.success("âœ… **Alto padrÃ£o markoviano**: Estado atual prevÃª bem o prÃ³ximo")
        elif predictability > 30:
            st.info("ğŸ“Š **PadrÃ£o markoviano moderado**")
        else:
            st.warning("âš ï¸ **Baixo padrÃ£o markoviano**: TransiÃ§Ãµes mais aleatÃ³rias")
        
        return {
            'transition_matrix': transition_probs.tolist(),
            'stationary_distribution': stationary_dist.tolist() if stationary_dist is not None else None,
            'markov_predictability': predictability,
            'top_transitions': transitions_sorted
        }
    
    def _advanced_randomness_tests(self, intervals):
        """Bateria completa de testes de aleatoriedade"""
        st.subheader("ğŸ² 16. Testes AvanÃ§ados de Aleatoriedade")
        
        if len(intervals) < 10:
            st.info("MÃ­nimo de 10 intervalos necessÃ¡rio")
            return {}
        
        results = {}
        
        # ========================================
        # 1. RUNS TEST (Wald-Wolfowitz)
        # ========================================
        st.write("**1ï¸âƒ£ Runs Test (SequÃªncias)**")
        
        median = np.median(intervals)
        runs = []
        current_run = []
        
        for val in intervals:
            if val > median:
                if current_run and current_run[0] <= median:
                    runs.append(len(current_run))
                    current_run = [val]
                else:
                    current_run.append(val)
            else:
                if current_run and current_run[0] > median:
                    runs.append(len(current_run))
                    current_run = [val]
                else:
                    current_run.append(val)
        
        if current_run:
            runs.append(len(current_run))
        
        n_runs = len(runs)
        n_above = np.sum(intervals > median)
        n_below = len(intervals) - n_above
        
        # EstatÃ­stica do teste
        expected_runs = (2 * n_above * n_below) / len(intervals) + 1
        var_runs = (2 * n_above * n_below * (2 * n_above * n_below - len(intervals))) / \
                   (len(intervals)**2 * (len(intervals) - 1))
        
        if var_runs > 0:
            z_score = (n_runs - expected_runs) / np.sqrt(var_runs)
            p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Runs Observados", n_runs)
            col2.metric("Runs Esperados", f"{expected_runs:.1f}")
            col3.metric("P-valor", f"{p_value:.4f}")
            
            if p_value > 0.05:
                st.success("âœ… **NÃ£o rejeita aleatoriedade** (sequÃªncias normais)")
                results['runs_test'] = 'random'
            else:
                st.warning("âš ï¸ **Rejeita aleatoriedade** (padrÃ£o nas sequÃªncias)")
                results['runs_test'] = 'non-random'
        
        # ========================================
        # 2. PERMUTATION ENTROPY TEST
        # ========================================
        st.write("**2ï¸âƒ£ Permutation Entropy (Complexidade)**")
        
        def permutation_entropy(series, order=3, delay=1):
            """Calcula entropia de permutaÃ§Ã£o"""
            n = len(series)
            permutations = []
            
            for i in range(n - delay * (order - 1)):
                pattern = []
                for j in range(order):
                    pattern.append(series[i + j * delay])
                
                # Converter para permutaÃ§Ã£o
                sorted_idx = np.argsort(pattern)
                perm = tuple(sorted_idx)
                permutations.append(perm)
            
            # Calcular entropia
            from collections import Counter
            perm_counts = Counter(permutations)
            probs = np.array(list(perm_counts.values())) / len(permutations)
            entropy = -np.sum(probs * np.log2(probs))
            
            # Normalizar
            max_entropy = np.log2(math.factorial(order))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            return normalized_entropy
        
        if len(intervals) >= 10:
            perm_entropy = permutation_entropy(intervals, order=3)
            complexity_score = perm_entropy * 100
            
            col1, col2 = st.columns(2)
            col1.metric("Entropia de PermutaÃ§Ã£o", f"{perm_entropy:.3f}")
            col2.metric("Score de Complexidade", f"{complexity_score:.1f}%")
            
            if complexity_score > 70:
                st.success("âœ… **Alta complexidade**: PadrÃ£o mais aleatÃ³rio")
                results['permutation_entropy'] = 'high'
            elif complexity_score > 40:
                st.info("ğŸ“Š **Complexidade moderada**")
                results['permutation_entropy'] = 'medium'
            else:
                st.warning("âš ï¸ **Baixa complexidade**: PadrÃ£o mais determinÃ­stico")
                results['permutation_entropy'] = 'low'
        
        # ========================================
        # 3. APPROXIMATE ENTROPY (ApEn)
        # ========================================
        st.write("**3ï¸âƒ£ Approximate Entropy (Regularidade)**")
        
        def approximate_entropy(series, m=2, r=None):
            """Calcula ApEn"""
            n = len(series)
            if r is None:
                r = 0.2 * np.std(series)
            
            def _phi(m):
                patterns = np.array([series[i:i+m] for i in range(n - m + 1)])
                counts = np.zeros(len(patterns))
                
                for i, pattern in enumerate(patterns):
                    distances = np.max(np.abs(patterns - pattern), axis=1)
                    counts[i] = np.sum(distances <= r)
                
                phi = np.sum(np.log(counts / (n - m + 1))) / (n - m + 1)
                return phi
            
            return _phi(m) - _phi(m + 1)
        
        if len(intervals) >= 20:
            apen = approximate_entropy(intervals)
            
            st.metric("ApEn Score", f"{apen:.4f}")
            
            if apen > 1.0:
                st.success("âœ… **Alta irregularidade**: Mais aleatÃ³rio")
                results['approximate_entropy'] = 'high'
            elif apen > 0.5:
                st.info("ğŸ“Š **Irregularidade moderada**")
                results['approximate_entropy'] = 'medium'
            else:
                st.warning("âš ï¸ **Baixa irregularidade**: Mais previsÃ­vel")
                results['approximate_entropy'] = 'low'
        
        # ========================================
        # 4. SERIAL CORRELATION TEST (Ljung-Box)
        # ========================================
        st.write("**4ï¸âƒ£ Serial Correlation (Ljung-Box)**")
        
        from scipy.stats import chi2
        
        def ljung_box_test(series, lags=10):
            """Teste de Ljung-Box para autocorrelaÃ§Ã£o"""
            n = len(series)
            acf_values = []
            
            for lag in range(1, min(lags + 1, n)):
                corr = np.corrcoef(series[:-lag], series[lag:])[0, 1]
                acf_values.append(corr)
            
            acf_values = np.array(acf_values)
            lb_stat = n * (n + 2) * np.sum(acf_values**2 / (n - np.arange(1, len(acf_values) + 1)))
            p_value = 1 - chi2.cdf(lb_stat, len(acf_values))
            
            return lb_stat, p_value
        
        if len(intervals) >= 15:
            lb_stat, lb_pval = ljung_box_test(intervals, lags=min(10, len(intervals)//2))
            
            col1, col2 = st.columns(2)
            col1.metric("Ljung-Box Statistic", f"{lb_stat:.2f}")
            col2.metric("P-valor", f"{lb_pval:.4f}")
            
            if lb_pval > 0.05:
                st.success("âœ… **Sem autocorrelaÃ§Ã£o significativa**: Comportamento independente")
                results['ljung_box'] = 'independent'
            else:
                st.warning("âš ï¸ **AutocorrelaÃ§Ã£o detectada**: Valores dependentes")
                results['ljung_box'] = 'dependent'
        
        # ========================================
        # 5. HURST EXPONENT (PersistÃªncia)
        # ========================================
        st.write("**5ï¸âƒ£ Hurst Exponent (MemÃ³ria de Longo Prazo)**")
        
        def hurst_exponent(series):
            """Calcula o expoente de Hurst"""
            n = len(series)
            if n < 20:
                return None
            
            lags = range(2, min(n//2, 20))
            tau = []
            
            for lag in lags:
                # Particionar sÃ©rie
                n_partitions = n // lag
                partitions = [series[i*lag:(i+1)*lag] for i in range(n_partitions)]
                
                # Calcular R/S para cada partiÃ§Ã£o
                rs_values = []
                for partition in partitions:
                    if len(partition) == 0:
                        continue
                    mean = np.mean(partition)
                    cumsum = np.cumsum(partition - mean)
                    R = np.max(cumsum) - np.min(cumsum)
                    S = np.std(partition)
                    if S > 0:
                        rs_values.append(R / S)
                
                if rs_values:
                    tau.append(np.mean(rs_values))
            
            # RegressÃ£o log-log
            if len(tau) > 2:
                log_lags = np.log(list(lags[:len(tau)]))
                log_tau = np.log(tau)
                hurst = np.polyfit(log_lags, log_tau, 1)[0]
                return hurst
            return None
        
        if len(intervals) >= 20:
            hurst = hurst_exponent(intervals)
            
            if hurst is not None:
                st.metric("Hurst Exponent", f"{hurst:.3f}")
                
                col1, col2, col3 = st.columns(3)
                
                if hurst < 0.45:
                    col1.success("ğŸ“‰ **Anti-persistente** (H < 0.5)")
                    st.write("â€¢ ReversÃ£o Ã  mÃ©dia: valores altos seguidos de baixos")
                    results['hurst'] = 'anti-persistent'
                elif hurst > 0.55:
                    col3.warning("ğŸ“ˆ **Persistente** (H > 0.5)")
                    st.write("â€¢ TendÃªncia: valores altos seguidos de altos")
                    results['hurst'] = 'persistent'
                else:
                    col2.info("ğŸ² **Random Walk** (H â‰ˆ 0.5)")
                    st.write("â€¢ Movimento browniano: comportamento aleatÃ³rio")
                    results['hurst'] = 'random'
        
        # ========================================
        # RESUMO FINAL DOS TESTES
        # ========================================
        st.markdown("---")
        st.write("**ğŸ“Š Resumo da Bateria de Testes:**")
        
        randomness_score = 0
        max_tests = 5
        
        if results.get('runs_test') == 'random':
            randomness_score += 1
        if results.get('permutation_entropy') == 'high':
            randomness_score += 1
        if results.get('approximate_entropy') == 'high':
            randomness_score += 1
        if results.get('ljung_box') == 'independent':
            randomness_score += 1
        if results.get('hurst') == 'random':
            randomness_score += 1
        
        randomness_pct = (randomness_score / max_tests) * 100
        
        col1, col2 = st.columns(2)
        col1.metric("Score de Aleatoriedade", f"{randomness_pct:.0f}%")
        col2.metric("Testes Aprovados", f"{randomness_score}/{max_tests}")
        
        if randomness_pct >= 60:
            st.success("âœ… **COMPORTAMENTO ALEATÃ“RIO**: PadrÃ£o nÃ£o determinÃ­stico")
            final_classification = 'random'
        elif randomness_pct >= 40:
            st.info("ğŸ“Š **COMPORTAMENTO MISTO**: Parcialmente previsÃ­vel")
            final_classification = 'mixed'
        else:
            st.warning("âš ï¸ **COMPORTAMENTO DETERMINÃSTICO**: Alto padrÃ£o estruturado")
            final_classification = 'deterministic'
        
        results['randomness_score'] = randomness_pct
        results['final_classification'] = final_classification
        
        return results
    
    # ============================================================
    # CLASSIFICAÃ‡ÃƒO FINAL COMPLETA
    # ============================================================
    
    def _final_classification(self, results, df, intervals):
        """ClassificaÃ§Ã£o final - APENAS critÃ©rios essenciais para determinar REINCIDÃŠNCIA"""
        st.markdown("---")
        st.header("ğŸ¯ CLASSIFICAÃ‡ÃƒO FINAL DE REINCIDÃŠNCIA")
        
        score = 0
        max_score = 100  # Score simplificado
        criteria = []
        
        # 1. REGULARIDADE (20 pontos) - O MAIS IMPORTANTE
        cv = results['basic_stats']['cv']
        if cv < 0.15:
            score += 20
            criteria.append(("âœ… Altamente regular (CV < 0.15)", 20))
        elif cv < 0.35:
            score += 15
            criteria.append(("âœ… Regular (CV < 0.35)", 15))
        elif cv < 0.65:
            score += 8
            criteria.append(("ğŸŸ¡ Semi-regular", 8))
        else:
            criteria.append(("âŒ Irregular", 0))
        
        # 2. PERIODICIDADE (20 pontos) - MUITO IMPORTANTE
        if results.get('periodicity', {}).get('has_periodicity', False):
            score += 20
            criteria.append(("âœ… Periodicidade detectada", 20))
        else:
            criteria.append(("âŒ Sem periodicidade", 0))
        
        # 3. PREVISIBILIDADE (15 pontos)
        pred_score = results.get('predictability', {}).get('score', 0)
        if pred_score > 70:
            score += 15
            criteria.append(("âœ… Altamente previsÃ­vel", 15))
        elif pred_score > 50:
            score += 10
            criteria.append(("ğŸŸ¡ Moderadamente previsÃ­vel", 10))
        elif pred_score > 30:
            score += 5
            criteria.append(("ğŸŸ¡ Parcialmente previsÃ­vel", 5))
        else:
            criteria.append(("âŒ ImprevisÃ­vel", 0))
        
        # 4. DETERMINISMO vs ALEATORIEDADE (15 pontos)
        randomness = results.get('randomness', {}).get('randomness_score', 50)
        determinism_score = 100 - randomness
        
        if determinism_score > 70:
            score += 15
            criteria.append(("âœ… Comportamento determinÃ­stico", 15))
        elif determinism_score > 50:
            score += 10
            criteria.append(("ğŸŸ¡ Comportamento semi-determinÃ­stico", 10))
        elif determinism_score > 30:
            score += 5
            criteria.append(("ğŸŸ¡ Comportamento misto", 5))
        else:
            criteria.append(("âŒ Comportamento aleatÃ³rio", 0))
        
        # 5. AUTOCORRELAÃ‡ÃƒO (10 pontos)
        if results.get('autocorr', {}).get('has_autocorr', False):
            score += 10
            criteria.append(("âœ… AutocorrelaÃ§Ã£o significativa", 10))
        else:
            criteria.append(("âŒ Sem autocorrelaÃ§Ã£o", 0))
        
        # 6. ESTABILIDADE (10 pontos)
        if results.get('stability', {}).get('is_stable', True):
            score += 10
            criteria.append(("âœ… PadrÃ£o estÃ¡vel no tempo", 10))
        else:
            criteria.append(("âš ï¸ PadrÃ£o instÃ¡vel", 0))
        
        # 7. PADRÃƒO MARKOVIANO (10 pontos)
        markov_pred = results.get('markov', {}).get('markov_predictability', 0)
        if markov_pred > 60:
            score += 10
            criteria.append(("âœ… Forte padrÃ£o markoviano", 10))
        elif markov_pred > 40:
            score += 6
            criteria.append(("ğŸŸ¡ PadrÃ£o markoviano moderado", 6))
        elif markov_pred > 20:
            score += 3
            criteria.append(("ğŸŸ¡ PadrÃ£o markoviano fraco", 3))
        else:
            criteria.append(("âŒ Sem padrÃ£o markoviano", 0))
        
        # Score jÃ¡ estÃ¡ em 0-100
        final_score = score
        
        # Determinar classificaÃ§Ã£o
        if final_score >= 70:
            classification = "ğŸ”´ ALERTA REINCIDENTE"
            level = "CRÃTICO"
            color = "red"
            recommendation = "**AÃ§Ã£o Imediata:** Criar automaÃ§Ã£o, runbook e investigar causa raiz"
            priority = "P1"
        elif final_score >= 50:
            classification = "ğŸŸ  PARCIALMENTE REINCIDENTE"
            level = "ALTO"
            color = "orange"
            recommendation = "**AÃ§Ã£o Recomendada:** Monitorar evoluÃ§Ã£o e considerar automaÃ§Ã£o"
            priority = "P2"
        elif final_score >= 30:
            classification = "ğŸŸ¡ PADRÃƒO DETECTÃVEL"
            level = "MÃ‰DIO"
            color = "yellow"
            recommendation = "**AÃ§Ã£o Sugerida:** Documentar padrÃ£o e revisar thresholds"
            priority = "P3"
        else:
            classification = "ğŸŸ¢ NÃƒO REINCIDENTE"
            level = "BAIXO"
            color = "green"
            recommendation = "**AÃ§Ã£o:** AnÃ¡lise caso a caso - possÃ­vel alarme falso"
            priority = "P4"
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown(f"### {classification}")
            st.markdown(f"**NÃ­vel:** {level} | **Prioridade:** {priority}")
            st.metric("Score de ReincidÃªncia", f"{final_score:.0f}/100", delta=level)
            
            st.markdown("#### ğŸ“Š CritÃ©rios Essenciais Avaliados")
            for criterion, points in criteria:
                st.write(f"â€¢ {criterion} ({points} pts)")
            
            st.info(recommendation)
            
            st.markdown("#### ğŸ“ˆ MÃ©tricas Principais")
            col_a, col_b, col_c, col_d = st.columns(4)
            col_a.metric("CV (Regularidade)", f"{cv:.2f}")
            col_b.metric("Previsibilidade", f"{pred_score:.0f}%")
            col_c.metric("Determinismo", f"{determinism_score:.0f}%")
            col_d.metric("Markov", f"{markov_pred:.0f}%")
        
        with col2:
            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=final_score,
                title={'text': "Score Final", 'font': {'size': 20}},
                gauge={
                    'axis': {'range': [0, 100]},
                    'bar': {'color': color},
                    'steps': [
                        {'range': [0, 30], 'color': "lightgray"},
                        {'range': [30, 50], 'color': "lightyellow"},
                        {'range': [50, 70], 'color': "orange"},
                        {'range': [70, 100], 'color': "red"}
                    ],
                    'threshold': {
                        'line': {'color': "black", 'width': 3},
                        'thickness': 0.75,
                        'value': 70
                    }
                }
            ))
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True, key='final_gauge')
        
        # ============================================================
        # PLANO DE AÃ‡ÃƒO RECOMENDADO
        # ============================================================
        
        st.markdown("---")
        st.subheader("ğŸ“‹ Plano de AÃ§Ã£o Recomendado")
        
        actions = []
        
        if final_score >= 70:
            actions.extend([
                ("ğŸš¨ URGENTE", "Criar automaÃ§Ã£o imediata", "P0", "24h"),
                ("ğŸ“– CRÃTICO", "Documentar runbook completo", "P0", "48h"),
                ("ğŸ” CRÃTICO", "AnÃ¡lise de causa raiz", "P0", "72h"),
                ("âš™ï¸ ALTO", "Implementar supressÃ£o inteligente", "P1", "1 semana")
            ])
        elif final_score >= 50:
            actions.extend([
                ("ğŸ“Š ALTO", "Monitoramento contÃ­nuo", "P1", "ContÃ­nuo"),
                ("ğŸ“– MÃ‰DIO", "Criar documentaÃ§Ã£o bÃ¡sica", "P2", "1 semana"),
                ("ğŸ”§ MÃ‰DIO", "Avaliar ajuste de thresholds", "P2", "2 semanas")
            ])
        else:
            actions.extend([
                ("ğŸ” MÃ‰DIO", "AnÃ¡lise caso a caso", "P3", "1 mÃªs"),
                ("ğŸ“Š BAIXO", "RevisÃ£o de configuraÃ§Ã£o", "P3", "Trimestral")
            ])
        
        # Tabela de aÃ§Ãµes
        actions_df = pd.DataFrame(actions, columns=['Prioridade', 'AÃ§Ã£o', 'NÃ­vel', 'Prazo'])
        st.table(actions_df)
        
        # ============================================================
        # PREDIÃ‡ÃƒO COM CONFIANÃ‡A
        # ============================================================
        
        if final_score >= 50:
            st.markdown("---")
            st.subheader("ğŸ”® PrediÃ§Ã£o de PrÃ³xima OcorrÃªncia")
            
            last_alert = df['created_on'].max()
            mean_interval = np.mean(intervals)
            std_interval = np.std(intervals)
            
            pred_time = last_alert + pd.Timedelta(hours=mean_interval)
            conf_interval = 1.96 * std_interval
            
            pred_confidence = results.get('prediction_confidence', {})
            confidence_label = pred_confidence.get('confidence', 'medium')
            confidence_score = pred_confidence.get('score', 0)
            
            confidence_emoji = {
                'high': 'ğŸŸ¢', 
                'medium': 'ğŸŸ¡', 
                'low': 'ğŸ”´'
            }
            
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("PrediÃ§Ã£o", pred_time.strftime('%d/%m %H:%M'))
            col2.metric("Intervalo", f"{mean_interval:.1f}h")
            col3.metric("ConfianÃ§a (95%)", f"Â± {conf_interval:.1f}h")
            col4.metric("NÃ­vel ConfianÃ§a", 
                       f"{confidence_emoji.get(confidence_label, 'ğŸŸ¡')} {confidence_label.upper()}")
            
            st.info(f"ğŸ“Š **Score de ConfianÃ§a Preditiva:** {confidence_score:.1f}%")
        
        # ============================================================
        # EXPORTAR RELATÃ“RIO COMPLETO
        # ============================================================
        
        st.markdown("---")
        export_data = {
            'short_ci': self.alert_id,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'classificacao': classification,
            'nivel': level,
            'prioridade': priority,
            'score': final_score,
            'cv': cv,
            'periodicidade': results.get('periodicity', {}).get('has_periodicity', False),
            'autocorrelacao': results.get('autocorr', {}).get('has_autocorr', False),
            'previsibilidade': pred_score,
            'markov_pred': markov_pred,
            'randomness_score': randomness,
            'determinism_score': determinism_score,
            'estabilidade': results.get('stability', {}).get('is_stable', True),
            'recomendacao': recommendation
        }
        
        export_df = pd.DataFrame([export_data])
        csv = export_df.to_csv(index=False)
        
        st.download_button(
            label="â¬‡ï¸ Exportar RelatÃ³rio Completo (CSV)",
            data=csv,
            file_name=f"reincidencia_{self.alert_id}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv",
            use_container_width=True
        )


# ============================================================
# FUNÃ‡Ã•ES AUXILIARES DE AGRUPAMENTO (mantidas do cÃ³digo original)
# ============================================================

def identify_alert_groups(alert_data, max_gap_hours=24, min_group_size=3, 
                         spike_threshold_multiplier=5):
    if len(alert_data) == 0:
        return alert_data, []
    
    alert_data = alert_data.sort_values('created_on').reset_index(drop=True)
    alert_data['time_diff_hours'] = alert_data['created_on'].diff().dt.total_seconds() / 3600
    alert_data['date'] = alert_data['created_on'].dt.date
    unique_dates = alert_data['date'].nunique()
    
    if unique_dates == 1:
        alert_data['group_id'] = -1
        alert_data['is_isolated'] = True
        return alert_data, []
    
    alert_data['group_id'] = -1
    current_group = 0
    group_start_idx = 0
    
    for i in range(len(alert_data)):
        if i == 0:
            continue
            
        gap = alert_data.loc[i, 'time_diff_hours']
        
        if gap > max_gap_hours:
            group_size = i - group_start_idx
            if group_size >= min_group_size:
                alert_data.loc[group_start_idx:i-1, 'group_id'] = current_group
                current_group += 1
            group_start_idx = i
    
    group_size = len(alert_data) - group_start_idx
    if group_size >= min_group_size:
        alert_data.loc[group_start_idx:, 'group_id'] = current_group
    
    daily_counts = alert_data.groupby('date').size()
    avg_daily = daily_counts.mean()
    spike_threshold = avg_daily * spike_threshold_multiplier
    spike_dates = daily_counts[daily_counts > spike_threshold].index
    
    if len(spike_dates) > 0:
        alert_data.loc[alert_data['date'].isin(spike_dates), 'group_id'] = -1
    
    alert_data['is_isolated'] = alert_data['group_id'] == -1
    
    groups_info = []
    for group_id in alert_data[alert_data['group_id'] >= 0]['group_id'].unique():
        group_data = alert_data[alert_data['group_id'] == group_id]
        groups_info.append({
            'group_id': int(group_id),
            'size': len(group_data),
            'start_time': group_data['created_on'].min(),
            'end_time': group_data['created_on'].max(),
            'duration_hours': (group_data['created_on'].max() - 
                             group_data['created_on'].min()).total_seconds() / 3600
        })
    
    return alert_data, groups_info


def classify_alert_pattern(alert_data, max_gap_hours=24, min_group_size=3, 
                          spike_threshold_multiplier=5):
    n = len(alert_data)
    if n == 0:
        return {
            'pattern': 'isolated',
            'reason': 'Sem ocorrÃªncias',
            'occurrences': 0,
            'num_groups': 0,
            'isolated_occurrences': 0,
            'grouped_occurrences': 0,
            'groups_info': [],
            'unique_days': 0
        }
    
    unique_days = alert_data['created_on'].dt.date.nunique()
    
    if unique_days == 1:
        return {
            'pattern': 'isolated',
            'reason': f'Todos os {n} alertas ocorreram em um Ãºnico dia',
            'occurrences': n,
            'num_groups': 0,
            'isolated_occurrences': n,
            'grouped_occurrences': 0,
            'groups_info': [],
            'unique_days': 1
        }
    
    alert_data_processed, groups_info = identify_alert_groups(
        alert_data, max_gap_hours, min_group_size, spike_threshold_multiplier
    )
    
    num_groups = len(groups_info)
    isolated_count = alert_data_processed['is_isolated'].sum()
    grouped_count = n - isolated_count
    isolated_pct = (isolated_count / n) * 100
    
    if num_groups == 0:
        pattern = 'isolated'
        reason = f'Nenhum grupo identificado ({n} ocorrÃªncias isoladas)'
    elif num_groups == 1 and isolated_pct > 50:
        pattern = 'isolated'
        reason = f'Apenas 1 grupo pequeno com {isolated_pct:.0f}% de alertas isolados'
    elif isolated_pct > 70:
        pattern = 'isolated'
        reason = f'{isolated_pct:.0f}% de alertas isolados ({isolated_count}/{n})'
    elif num_groups >= 2:
        pattern = 'continuous'
        reason = f'{num_groups} grupos contÃ­nuos identificados ({grouped_count} alertas agrupados)'
    elif num_groups == 1 and grouped_count >= min_group_size * 2:
        pattern = 'continuous'
        reason = f'1 grupo contÃ­nuo grande ({grouped_count} alertas)'
    else:
        pattern = 'isolated'
        reason = f'PadrÃ£o inconsistente: {num_groups} grupo(s), {isolated_pct:.0f}% isolados'
    
    return {
        'pattern': pattern,
        'reason': reason,
        'occurrences': n,
        'num_groups': num_groups,
        'isolated_occurrences': int(isolated_count),
        'grouped_occurrences': int(grouped_count),
        'groups_info': groups_info,
        'unique_days': unique_days
    }


def process_single_alert(alert_id, df_original, max_gap_hours=24, min_group_size=3, 
                        spike_threshold_multiplier=5):
    try:
        df_alert = df_original[df_original['short_ci'] == alert_id].copy()
        if len(df_alert) < 1:
            return None
        
        pattern_info = classify_alert_pattern(df_alert, max_gap_hours, min_group_size, 
                                             spike_threshold_multiplier)
        
        df_alert['hour'] = df_alert['created_on'].dt.hour
        df_alert['day_of_week'] = df_alert['created_on'].dt.dayofweek
        df_alert['is_weekend'] = df_alert['day_of_week'].isin([5, 6])
        df_alert['is_business_hours'] = (df_alert['hour'] >= 9) & (df_alert['hour'] <= 17)
        df_alert = df_alert.sort_values('created_on')
        intervals_hours = df_alert['created_on'].diff().dt.total_seconds() / 3600
        intervals_hours = intervals_hours.dropna()
        
        period_days = (df_alert['created_on'].max() - df_alert['created_on'].min()).days + 1
        
        metrics = {
            'alert_id': alert_id,
            'pattern_type': pattern_info['pattern'],
            'pattern_reason': pattern_info['reason'],
            'total_ocorrencias': pattern_info['occurrences'],
            'num_grupos': pattern_info['num_groups'],
            'alertas_isolados': pattern_info['isolated_occurrences'],
            'alertas_agrupados': pattern_info['grouped_occurrences'],
            'pct_isolados': (pattern_info['isolated_occurrences'] / pattern_info['occurrences'] * 100) 
                           if pattern_info['occurrences'] > 0 else 0,
            'unique_days': pattern_info['unique_days'],
            'periodo_dias': period_days,
            'freq_dia': len(df_alert) / period_days if period_days > 0 else 0,
            'freq_semana': (len(df_alert) / period_days * 7) if period_days > 0 else 0,
            'freq_mes': (len(df_alert) / period_days * 30) if period_days > 0 else 0,
            'intervalo_medio_h': intervals_hours.mean() if len(intervals_hours) > 0 else None,
            'intervalo_mediano_h': intervals_hours.median() if len(intervals_hours) > 0 else None,
            'intervalo_std_h': intervals_hours.std() if len(intervals_hours) > 0 else None,
            'intervalo_min_h': intervals_hours.min() if len(intervals_hours) > 0 else None,
            'intervalo_max_h': intervals_hours.max() if len(intervals_hours) > 0 else None,
            'hora_pico': df_alert['hour'].mode().iloc[0] if len(df_alert['hour'].mode()) > 0 else 12,
            'pct_fins_semana': df_alert['is_weekend'].mean() * 100,
            'pct_horario_comercial': df_alert['is_business_hours'].mean() * 100,
            'variabilidade_intervalo': intervals_hours.std() / intervals_hours.mean() if len(intervals_hours) > 0 and intervals_hours.mean() > 0 else 0,
            'primeiro_alerta': df_alert['created_on'].min(),
            'ultimo_alerta': df_alert['created_on'].max()
        }
        return metrics
    except Exception:
        return None


def process_alert_chunk(alert_ids, df_original, max_gap_hours=24, min_group_size=3, 
                       spike_threshold_multiplier=5):
    return [metrics for alert_id in alert_ids 
            if (metrics := process_single_alert(alert_id, df_original, max_gap_hours, 
                                               min_group_size, spike_threshold_multiplier))]


# ============================================================
# CLASSE PRINCIPAL (mantida do cÃ³digo original - continuaÃ§Ã£o)
# ============================================================

class StreamlitAlertAnalyzer:
    def __init__(self):
        self.df_original = None
        self.df_all_alerts = None
        self.df = None
        self.dates = None
        self.alert_id = None
        self.max_gap_hours = 24
        self.min_group_size = 3
        self.spike_threshold_multiplier = 5

    def load_data(self, uploaded_file):
        try:
            df_raw = pd.read_csv(uploaded_file)
            st.success(f"âœ… Arquivo carregado com {len(df_raw)} registros")
            with st.expander("ğŸ“‹ InformaÃ§Ãµes do Dataset"):
                st.write(f"**Colunas:** {list(df_raw.columns)}")
                st.write(f"**Shape:** {df_raw.shape}")
                st.dataframe(df_raw.head())
            if 'created_on' not in df_raw.columns or 'short_ci' not in df_raw.columns:
                st.error("âŒ Colunas 'created_on' e 'short_ci' sÃ£o obrigatÃ³rias!")
                return False
            df_raw['created_on'] = pd.to_datetime(df_raw['created_on'])
            df_raw = df_raw.dropna(subset=['created_on'])
            df_raw = df_raw.sort_values(['short_ci', 'created_on']).reset_index(drop=True)
            self.df_original = df_raw
            st.sidebar.write(f"**IDs disponÃ­veis:** {len(df_raw['short_ci'].unique())}")
            return True
        except Exception as e:
            st.error(f"âŒ Erro ao carregar dados: {e}")
            return False

    def prepare_individual_analysis(self, alert_id):
        df_filtered = self.df_original[self.df_original['short_ci'] == alert_id].copy()
        if len(df_filtered) == 0:
            return False

        df_filtered['date'] = df_filtered['created_on'].dt.date
        df_filtered['hour'] = df_filtered['created_on'].dt.hour
        df_filtered['day_of_week'] = df_filtered['created_on'].dt.dayofweek
        df_filtered['day_name'] = df_filtered['created_on'].dt.day_name()
        df_filtered['is_weekend'] = df_filtered['day_of_week'].isin([5, 6])
        df_filtered['is_business_hours'] = (df_filtered['hour'] >= 9) & (df_filtered['hour'] <= 17)
        df_filtered['time_diff_hours'] = df_filtered['created_on'].diff().dt.total_seconds() / 3600

        df_filtered, groups_info = identify_alert_groups(
            df_filtered, 
            self.max_gap_hours, 
            self.min_group_size,
            self.spike_threshold_multiplier
        )

        self.df = df_filtered
        self.dates = df_filtered['created_on']
        self.alert_id = alert_id
        self.groups_info = groups_info
        return True

    def prepare_global_analysis(self, use_multiprocessing=True, max_gap_hours=24, 
                               min_group_size=3, spike_threshold_multiplier=5):
        st.header("ğŸŒ AnÃ¡lise Global de Todos os Alertas")
        
        # Definir self.df para uso posterior
        self.df = self.df_original.copy()
        
        self.max_gap_hours = max_gap_hours
        self.min_group_size = min_group_size
        self.spike_threshold_multiplier = spike_threshold_multiplier
        
        unique_ids = self.df_original['short_ci'].unique()
        total_ids = len(unique_ids)
        st.info(f"ğŸ“Š Processando {total_ids} Alert IDs...")
        alert_metrics = []
        
        if use_multiprocessing:
            n_processes = min(cpu_count(), total_ids)
            st.write(f"ğŸš€ Usando {n_processes} processos paralelos")
            chunk_size = max(1, total_ids // n_processes)
            id_chunks = [unique_ids[i:i + chunk_size] for i in range(0, total_ids, chunk_size)]
            progress_bar = st.progress(0)
            status_text = st.empty()
            process_func = partial(process_alert_chunk, 
                                  df_original=self.df_original,
                                  max_gap_hours=max_gap_hours,
                                  min_group_size=min_group_size,
                                  spike_threshold_multiplier=spike_threshold_multiplier)
            try:
                with Pool(processes=n_processes) as pool:
                    results = pool.map(process_func, id_chunks)
                    for result in results:
                        alert_metrics.extend(result)
                    progress_bar.progress(1.0)
                    status_text.success(f"âœ… Processamento concluÃ­do! {len(alert_metrics)} alertas analisados")
            except Exception as e:
                st.error(f"âŒ Erro no multiprocessing: {e}")
                st.warning("âš ï¸ Tentando processamento sequencial...")
                use_multiprocessing = False
                alert_metrics = []
        
        if not use_multiprocessing or len(alert_metrics) == 0:
            alert_metrics = []
            progress_bar = st.progress(0)
            for i, alert_id in enumerate(unique_ids):
                progress_bar.progress((i + 1) / total_ids)
                metrics = process_single_alert(alert_id, self.df_original, 
                                              max_gap_hours, min_group_size, 
                                              spike_threshold_multiplier)
                if metrics:
                    alert_metrics.append(metrics)
        
        if 'progress_bar' in locals():
            progress_bar.empty()
        
        self.df_all_alerts = pd.DataFrame(alert_metrics)
        
        isolated_count = len(self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'isolated'])
        continuous_count = len(self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous'])
        single_day_count = len(self.df_all_alerts[self.df_all_alerts['unique_days'] == 1])
        
        st.subheader("ğŸ“Š EstatÃ­sticas Globais")
        col1, col2, col3, col4, col5, col6, col7 = st.columns(7)
        with col1:
            st.metric("ğŸ”¢ Total de Alert IDs", len(unique_ids))
        with col2:
            st.metric("ğŸ“ˆ IDs com Dados", len(self.df_all_alerts))
        with col3:
            st.metric("ğŸš¨ Total de Alertas", self.df_original.shape[0])
        with col4:
            period_total = (self.df_original['created_on'].max() - self.df_original['created_on'].min()).days + 1
            st.metric("ğŸ“… PerÃ­odo (dias)", period_total)
        with col5:
            st.metric("ğŸ”´ Alertas Isolados", isolated_count)
        with col6:
            st.metric("ğŸŸ¢ Alertas ContÃ­nuos", continuous_count)
        with col7:
            st.metric("ğŸ“† Alertas de 1 Dia", single_day_count)
        
        return len(self.df_all_alerts) > 0

    def analyze_temporal_recurrence_patterns(self):
        """AnÃ¡lise avanÃ§ada de recorrÃªncia usando a nova classe COMPLETA"""
        analyzer = AdvancedRecurrenceAnalyzer(self.df, self.alert_id)
        analyzer.analyze()

    # ============================================================
    # MÃ‰TODOS RESTANTES DA CLASSE (mantidos do cÃ³digo original)
    # ============================================================

    def show_isolated_vs_continuous_analysis(self):
        st.header("ğŸ” AnÃ¡lise de Alertas Isolados vs ContÃ­nuos (Baseado em Grupos)")

        self.df_all_alerts = self.df_all_alerts.drop_duplicates(subset=['alert_id'])

        df_isolated = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'isolated']
        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
        df_single_day = self.df_all_alerts[self.df_all_alerts['unique_days'] == 1]

        col1, col2 = st.columns(2)
        with col1:
            pattern_dist = self.df_all_alerts['pattern_type'].value_counts()
            fig_pie = px.pie(
                values=pattern_dist.values,
                names=pattern_dist.index,
                title="ğŸ“Š DistribuiÃ§Ã£o de PadrÃµes de Alerta",
                color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
            )
            st.plotly_chart(fig_pie, use_container_width=True, key='pattern_pie')

        with col2:
            st.subheader("ğŸ“ˆ ComparaÃ§Ã£o de MÃ©tricas")
            comparison_data = pd.DataFrame({
                'MÃ©trica': ['Qtd Alertas', 'MÃ©dia OcorrÃªncias', 'MÃ©dia Grupos', 
                            'MÃ©dia % Isolados', 'MÃ©dia Freq/Dia', 'Alertas 1 Dia'],
                'Isolados': [
                    len(df_isolated),
                    df_isolated['total_ocorrencias'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['num_grupos'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['pct_isolados'].mean() if len(df_isolated) > 0 else 0,
                    df_isolated['freq_dia'].mean() if len(df_isolated) > 0 else 0,
                    len(df_single_day)
                ],
                'ContÃ­nuos': [
                    len(df_continuous),
                    df_continuous['total_ocorrencias'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['num_grupos'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['pct_isolados'].mean() if len(df_continuous) > 0 else 0,
                    df_continuous['freq_dia'].mean() if len(df_continuous) > 0 else 0,
                    0
                ]
            })
            comparison_data = comparison_data.round(2)
            st.dataframe(comparison_data, use_container_width=True)

        st.subheader("ğŸ“ˆ EvoluÃ§Ã£o Temporal: Isolados vs Agrupados")
        
        df_with_dates = self.df_all_alerts.copy()
        df_with_dates['date'] = pd.to_datetime(df_with_dates['primeiro_alerta']).dt.date
        
        daily_isolated = df_isolated.copy()
        daily_isolated['date'] = pd.to_datetime(daily_isolated['primeiro_alerta']).dt.date
        isolated_counts = daily_isolated.groupby('date').size()
        
        daily_continuous = df_continuous.copy()
        daily_continuous['date'] = pd.to_datetime(daily_continuous['primeiro_alerta']).dt.date
        continuous_counts = daily_continuous.groupby('date').size()
        
        all_dates = pd.date_range(
            start=self.df_all_alerts['primeiro_alerta'].min(),
            end=self.df_all_alerts['ultimo_alerta'].max(),
            freq='D'
        ).date
        
        line_data = pd.DataFrame({'date': all_dates})
        line_data['Isolados'] = line_data['date'].map(isolated_counts).fillna(0)
        line_data['ContÃ­nuos'] = line_data['date'].map(continuous_counts).fillna(0)
        
        fig_lines = go.Figure()
        
        fig_lines.add_trace(go.Scatter(
            x=line_data['date'],
            y=line_data['Isolados'],
            mode='lines+markers',
            name='Isolados',
            line=dict(color='#ff4444', width=2),
            marker=dict(size=6),
            hovertemplate='%{x}<br>Isolados: %{y}<extra></extra>'
        ))
        
        fig_lines.add_trace(go.Scatter(
            x=line_data['date'],
            y=line_data['ContÃ­nuos'],
            mode='lines+markers',
            name='ContÃ­nuos',
            line=dict(color='#44ff44', width=2),
            marker=dict(size=6),
            hovertemplate='%{x}<br>ContÃ­nuos: %{y}<extra></extra>'
        ))
        
        fig_lines.update_layout(
            title="Quantidade de Alertas por Dia (Isolados vs ContÃ­nuos)",
            xaxis_title="Data",
            yaxis_title="Quantidade de Alertas",
            hovermode='x unified',
            height=400
        )
        
        st.plotly_chart(fig_lines, use_container_width=True, key='isolated_vs_continuous_lines')

        tab1, tab2, tab3 = st.tabs(["ğŸ”´ Alertas Isolados", "ğŸŸ¢ Alertas ContÃ­nuos", "ğŸ“Š AnÃ¡lise Comparativa"])

        with tab1:
            st.subheader(f"ğŸ”´ Alertas Isolados ({len(df_isolated)} alertas)")

            if len(df_isolated) > 0:
                if len(df_single_day) > 0:
                    st.info(f"ğŸ“† **{len(df_single_day)} alertas** ({len(df_single_day)/len(df_isolated)*100:.1f}%) ocorreram em apenas 1 dia")
                
                fig_iso = px.scatter(
                    df_isolated,
                    x='primeiro_alerta',
                    y='total_ocorrencias',
                    size='alertas_isolados',
                    color='pct_isolados',
                    title="â³ OcorrÃªncias de Alertas Isolados no Tempo",
                    hover_data=['alert_id', 'pattern_reason', 'num_grupos', 'unique_days'],
                    labels={'pct_isolados': '% Isolados', 'unique_days': 'Dias Ãšnicos'}
                )
                st.plotly_chart(fig_iso, use_container_width=True, key='isolated_scatter')

                st.write("**ğŸ“ RazÃµes para ClassificaÃ§Ã£o como Isolado:**")
                reason_counts = df_isolated['pattern_reason'].value_counts()
                for reason, count in reason_counts.items():
                    st.write(f"â€¢ {reason}: {count} alertas")

                st.write("**ğŸ” Top 10 Alertas Isolados (por % de alertas isolados):**")
                top_isolated = df_isolated.nlargest(10, 'pct_isolados')[
                    ['alert_id', 'total_ocorrencias', 'alertas_isolados', 'num_grupos', 'pct_isolados', 'unique_days', 'pattern_reason']
                ]
                top_isolated.columns = ['Alert ID', 'Total OcorrÃªncias', 'Alertas Isolados', 'NÂº Grupos', '% Isolados', 'Dias Ãšnicos', 'RazÃ£o']
                top_isolated['% Isolados'] = top_isolated['% Isolados'].round(1).astype(str) + '%'
                st.dataframe(top_isolated, use_container_width=True)

                with st.expander("ğŸ“‹ Ver todos os alertas isolados"):
                    isolated_list = df_isolated[['alert_id', 'total_ocorrencias', 'alertas_isolados',
                                                'num_grupos', 'pct_isolados', 'unique_days', 'pattern_reason']].copy()
                    isolated_list.columns = ['Alert ID', 'Total', 'Isolados', 'Grupos', '% Isolados', 'Dias Ãšnicos', 'RazÃ£o']
                    isolated_list['% Isolados'] = isolated_list['% Isolados'].round(1).astype(str) + '%'
                    st.dataframe(isolated_list, use_container_width=True)
            else:
                st.info("Nenhum alerta isolado encontrado com os critÃ©rios atuais.")

        with tab2:
            st.subheader(f"ğŸŸ¢ Alertas ContÃ­nuos ({len(df_continuous)} alertas)")

            if len(df_continuous) > 0:
                st.write("**ğŸ” Top 10 Alertas ContÃ­nuos (maior nÃºmero de grupos):**")
                top_continuous = df_continuous.nlargest(10, 'num_grupos')[
                    ['alert_id', 'total_ocorrencias', 'num_grupos', 'alertas_agrupados', 'freq_dia', 'unique_days']
                ]
                top_continuous.columns = ['Alert ID', 'Total OcorrÃªncias', 'NÂº Grupos', 'Alertas Agrupados', 'Freq/Dia', 'Dias Ãšnicos']
                st.dataframe(top_continuous, use_container_width=True)

                col1, col2 = st.columns(2)
                with col1:
                    fig_groups = px.histogram(
                        df_continuous, 
                        x='num_grupos',
                        title="ğŸ“Š DistribuiÃ§Ã£o de NÃºmero de Grupos",
                        labels={'num_grupos': 'NÃºmero de Grupos', 'count': 'Quantidade'}
                    )
                    st.plotly_chart(fig_groups, use_container_width=True, key='continuous_groups_hist')
                with col2:
                    fig_pct = px.histogram(
                        df_continuous,
                        x='pct_isolados',
                        title="ğŸ“Š DistribuiÃ§Ã£o de % de Alertas Isolados",
                        labels={'pct_isolados': '% Alertas Isolados', 'count': 'Quantidade'}
                    )
                    st.plotly_chart(fig_pct, use_container_width=True, key='continuous_pct_hist')

                with st.expander("ğŸ“‹ Ver todos os alertas contÃ­nuos"):
                    continuous_list = df_continuous[['alert_id', 'total_ocorrencias', 'num_grupos',
                                                    'alertas_agrupados', 'alertas_isolados', 'pct_isolados', 'unique_days']].copy()
                    continuous_list.columns = ['Alert ID', 'Total', 'Grupos', 'Agrupados', 'Isolados', '% Isolados', 'Dias Ãšnicos']
                    continuous_list['% Isolados'] = continuous_list['% Isolados'].round(1).astype(str) + '%'
                    st.dataframe(continuous_list, use_container_width=True)
            else:
                st.info("Nenhum alerta contÃ­nuo encontrado com os critÃ©rios atuais.")

        with tab3:
            st.subheader("ğŸ“Š AnÃ¡lise Comparativa Detalhada")

            fig_scatter = px.scatter(
                self.df_all_alerts,
                x='total_ocorrencias',
                y='intervalo_medio_h',
                color='pattern_type',
                title="ğŸ¯ OcorrÃªncias vs Intervalo MÃ©dio",
                labels={
                    'total_ocorrencias': 'Total de OcorrÃªncias',
                    'intervalo_medio_h': 'Intervalo MÃ©dio (horas)',
                    'pattern_type': 'Tipo de PadrÃ£o'
                },
                hover_data=['alert_id', 'unique_days'],
                color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
            )
            st.plotly_chart(fig_scatter, use_container_width=True, key='comparative_scatter')

            col1, col2 = st.columns(2)
            with col1:
                fig_box_occ = px.box(
                    self.df_all_alerts,
                    x='pattern_type',
                    y='total_ocorrencias',
                    title="ğŸ“¦ DistribuiÃ§Ã£o de OcorrÃªncias",
                    color='pattern_type',
                    color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
                )
                st.plotly_chart(fig_box_occ, use_container_width=True, key='box_occurrences')

            with col2:
                fig_box_freq = px.box(
                    self.df_all_alerts,
                    x='pattern_type',
                    y='freq_dia',
                    title="ğŸ“¦ DistribuiÃ§Ã£o de FrequÃªncia DiÃ¡ria",
                    color='pattern_type',
                    color_discrete_map={'isolated': '#ff4444', 'continuous': '#44ff44'}
                )
                st.plotly_chart(fig_box_freq, use_container_width=True, key='box_frequency')

            st.subheader("ğŸ’¡ RecomendaÃ§Ãµes de Tratamento")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**ğŸ”´ Para Alertas Isolados:**")
                st.write("â€¢ Considerar desativaÃ§Ã£o ou revisÃ£o de configuraÃ§Ã£o")
                st.write("â€¢ Verificar se sÃ£o falsos positivos")
                st.write("â€¢ Analisar contexto especÃ­fico das ocorrÃªncias")
                st.write("â€¢ Avaliar consolidaÃ§Ã£o com outros alertas similares")
                st.write("â€¢ Alertas de 1 dia podem ser eventos Ãºnicos sem recorrÃªncia")

            with col2:
                st.write("**ğŸŸ¢ Para Alertas ContÃ­nuos:**")
                st.write("â€¢ Priorizar automaÃ§Ã£o de resposta")
                st.write("â€¢ Implementar supressÃ£o inteligente")
                st.write("â€¢ Criar runbooks especÃ­ficos")
                st.write("â€¢ Considerar ajuste de thresholds")

    def show_continuous_groups_detailed_view(self):
        st.header("ğŸ” VisualizaÃ§Ã£o Detalhada dos Grupos - Alertas ContÃ­nuos")
        
        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']
        
        if len(df_continuous) == 0:
            st.warning("âš ï¸ Nenhum alerta contÃ­nuo encontrado para visualizaÃ§Ã£o de grupos.")
            return
        
        st.info(f"ğŸ“Š Analisando grupos detalhados de **{len(df_continuous)}** alertas contÃ­nuos")
        
        selected_alerts = st.multiselect(
            "ğŸ¯ Selecione alertas para visualizar grupos em detalhes (mÃ¡x. 5):",
            options=df_continuous.nlargest(20, 'num_grupos')['alert_id'].tolist(),
            default=df_continuous.nlargest(3, 'num_grupos')['alert_id'].tolist()[:3],
            help="Mostrando os 20 alertas com mais grupos. Selecione atÃ© 5 para anÃ¡lise detalhada."
        )
        
        if len(selected_alerts) > 5:
            st.warning("âš ï¸ MÃ¡ximo de 5 alertas por vez. Mostrando apenas os 5 primeiros selecionados.")
            selected_alerts = selected_alerts[:5]
        
        if not selected_alerts:
            st.info("ğŸ‘† Selecione pelo menos um alerta acima para ver os detalhes dos grupos")
            return
        
        for alert_id in selected_alerts:
            st.markdown("---")
            alert_info = df_continuous[df_continuous['alert_id'] == alert_id].iloc[0]
            
            with st.expander(f"ğŸ“Š **Alert ID: {alert_id}** - {alert_info['num_grupos']} grupos identificados", expanded=True):
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("Total OcorrÃªncias", alert_info['total_ocorrencias'])
                with col2:
                    st.metric("NÂº de Grupos", alert_info['num_grupos'])
                with col3:
                    st.metric("Alertas Agrupados", alert_info['alertas_agrupados'])
                with col4:
                    st.metric("Alertas Isolados", alert_info['alertas_isolados'])
                with col5:
                    st.metric("Dias Ãšnicos", alert_info['unique_days'])
                
                alert_data = self.df_original[self.df_original['short_ci'] == alert_id].copy()
                alert_data, groups_info = identify_alert_groups(
                    alert_data,
                    self.max_gap_hours,
                    self.min_group_size,
                    self.spike_threshold_multiplier
                )
                
                if len(groups_info) > 0:
                    st.subheader("ğŸ“‹ Detalhes dos Grupos Identificados")
                    groups_df = pd.DataFrame(groups_info)
                    groups_df['start_time_str'] = pd.to_datetime(groups_df['start_time']).dt.strftime('%Y-%m-%d %H:%M')
                    groups_df['end_time_str'] = pd.to_datetime(groups_df['end_time']).dt.strftime('%Y-%m-%d %H:%M')
                    groups_df['duration_hours'] = groups_df['duration_hours'].round(2)
                    
                    groups_display = groups_df[['group_id', 'size', 'start_time_str', 'end_time_str', 'duration_hours']].copy()
                    groups_display.columns = ['ID Grupo', 'Tamanho', 'InÃ­cio', 'Fim', 'DuraÃ§Ã£o (h)']
                    st.dataframe(groups_display, use_container_width=True)
                    
                    st.subheader("ğŸ“Š Timeline Visual dos Grupos")
                    
                    fig_timeline = go.Figure()
                    
                    colors = px.colors.qualitative.Plotly
                    for idx, group in groups_df.iterrows():
                        color = colors[int(group['group_id']) % len(colors)]
                        
                        fig_timeline.add_trace(go.Scatter(
                            x=[group['start_time'], group['end_time']],
                            y=[group['group_id'], group['group_id']],
                            mode='lines+markers',
                            name=f"Grupo {int(group['group_id'])}",
                            line=dict(color=color, width=15),
                            marker=dict(size=12, symbol='circle'),
                            hovertemplate=f"<b>Grupo {int(group['group_id'])}</b><br>" +
                                        f"Tamanho: {group['size']} alertas<br>" +
                                        f"DuraÃ§Ã£o: {group['duration_hours']:.2f}h<br>" +
                                        f"InÃ­cio: {group['start_time_str']}<br>" +
                                        f"Fim: {group['end_time_str']}<extra></extra>"
                        ))
                    
                    isolated_data = alert_data[alert_data['is_isolated']]
                    if len(isolated_data) > 0:
                        fig_timeline.add_trace(go.Scatter(
                            x=isolated_data['created_on'],
                            y=[-1] * len(isolated_data),
                            mode='markers',
                            name='Alertas Isolados',
                            marker=dict(size=10, color='red', symbol='x'),
                            hovertemplate='<b>Alerta Isolado</b><br>%{x}<extra></extra>'
                        ))
                    
                    fig_timeline.update_layout(
                        title=f"Timeline de Grupos - Alert ID: {alert_id}",
                        xaxis_title="Data/Hora",
                        yaxis_title="ID do Grupo",
                        yaxis=dict(
                            tickmode='linear',
                            tick0=-1,
                            dtick=1,
                            ticktext=['Isolados'] + [f'Grupo {i}' for i in range(len(groups_info))],
                            tickvals=[-1] + list(range(len(groups_info)))
                        ),
                        height=400,
                        hovermode='closest',
                        showlegend=True
                    )
                    
                    st.plotly_chart(fig_timeline, use_container_width=True, key=f'timeline_{alert_id}')
                    
                    st.subheader("ğŸ“ˆ AnÃ¡lise Temporal dos Grupos")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        fig_sizes = px.bar(
                            groups_df,
                            x='group_id',
                            y='size',
                            title="Tamanho de Cada Grupo",
                            labels={'group_id': 'ID do Grupo', 'size': 'Quantidade de Alertas'},
                            text='size'
                        )
                        fig_sizes.update_traces(textposition='outside')
                        st.plotly_chart(fig_sizes, use_container_width=True, key=f'sizes_{alert_id}')
                    
                    with col2:
                        fig_duration = px.bar(
                            groups_df,
                            x='group_id',
                            y='duration_hours',
                            title="DuraÃ§Ã£o de Cada Grupo (horas)",
                            labels={'group_id': 'ID do Grupo', 'duration_hours': 'DuraÃ§Ã£o (h)'},
                            text='duration_hours'
                        )
                        fig_duration.update_traces(textposition='outside', texttemplate='%{text:.1f}h')
                        st.plotly_chart(fig_duration, use_container_width=True, key=f'duration_{alert_id}')
                    
                    st.subheader("ğŸ“Š EstatÃ­sticas dos Grupos")
                    stats_col1, stats_col2, stats_col3, stats_col4 = st.columns(4)
                    
                    with stats_col1:
                        st.metric("Tamanho MÃ©dio", f"{groups_df['size'].mean():.1f} alertas")
                    with stats_col2:
                        st.metric("Maior Grupo", f"{groups_df['size'].max()} alertas")
                    with stats_col3:
                        st.metric("DuraÃ§Ã£o MÃ©dia", f"{groups_df['duration_hours'].mean():.1f}h")
                    with stats_col4:
                        st.metric("Maior DuraÃ§Ã£o", f"{groups_df['duration_hours'].max():.1f}h")
                    
                    if len(groups_df) > 1:
                        st.subheader("â±ï¸ Intervalos Entre Grupos")
                        gaps = []
                        for i in range(len(groups_df) - 1):
                            gap = (groups_df.iloc[i+1]['start_time'] - groups_df.iloc[i]['end_time']).total_seconds() / 3600
                            gaps.append({
                                'De': f"Grupo {int(groups_df.iloc[i]['group_id'])}",
                                'Para': f"Grupo {int(groups_df.iloc[i+1]['group_id'])}",
                                'Intervalo (h)': round(gap, 2)
                            })
                        
                        gaps_df = pd.DataFrame(gaps)
                        st.dataframe(gaps_df, use_container_width=True)
                        
                        avg_gap = gaps_df['Intervalo (h)'].mean()
                        st.info(f"ğŸ“Š Intervalo mÃ©dio entre grupos: **{avg_gap:.2f} horas**")
                
                else:
                    st.warning("Nenhum grupo identificado para este alerta.")
        
        st.markdown("---")
        st.header("ğŸ“Š Resumo Geral dos Grupos - Todos os Alertas ContÃ­nuos")
        
        all_groups_data = []
        for _, alert in df_continuous.iterrows():
            alert_data = self.df_original[self.df_original['short_ci'] == alert['alert_id']].copy()
            _, groups_info = identify_alert_groups(
                alert_data,
                self.max_gap_hours,
                self.min_group_size,
                self.spike_threshold_multiplier
            )
            for group in groups_info:
                all_groups_data.append({
                    'alert_id': alert['alert_id'],
                    'group_id': group['group_id'],
                    'size': group['size'],
                    'duration_hours': group['duration_hours']
                })
        
        if all_groups_data:
            all_groups_df = pd.DataFrame(all_groups_data)
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total de Grupos", len(all_groups_df))
            with col2:
                st.metric("Tamanho MÃ©dio", f"{all_groups_df['size'].mean():.1f} alertas")
            with col3:
                st.metric("DuraÃ§Ã£o MÃ©dia", f"{all_groups_df['duration_hours'].mean():.1f}h")
            with col4:
                st.metric("Alertas/Grupo MÃ¡x", int(all_groups_df['size'].max()))
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig_all_sizes = px.histogram(
                    all_groups_df,
                    x='size',
                    title="DistribuiÃ§Ã£o de Tamanhos dos Grupos",
                    labels={'size': 'Tamanho do Grupo (alertas)', 'count': 'Quantidade de Grupos'},
                    nbins=20
                )
                st.plotly_chart(fig_all_sizes, use_container_width=True, key='all_sizes_hist')
            
            with col2:
                fig_all_duration = px.histogram(
                    all_groups_df,
                    x='duration_hours',
                    title="DistribuiÃ§Ã£o de DuraÃ§Ãµes dos Grupos",
                    labels={'duration_hours': 'DuraÃ§Ã£o (horas)', 'count': 'Quantidade de Grupos'},
                    nbins=20
                )
                st.plotly_chart(fig_all_duration, use_container_width=True, key='all_duration_hist')

    def analyze_continuous_recurrence_patterns(self):
        st.header("ğŸ” AnÃ¡lise de RecorrÃªncia - Alertas ContÃ­nuos")

        df_continuous = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous']

        if len(df_continuous) == 0:
            st.warning("âš ï¸ Nenhum alerta contÃ­nuo encontrado para anÃ¡lise de recorrÃªncia.")
            return

        st.info(f"ğŸ“Š Analisando padrÃµes de recorrÃªncia de **{len(df_continuous)}** alertas contÃ­nuos")

        continuous_alert_ids = df_continuous['alert_id'].unique()
        df_continuous_details = self.df_original[self.df_original['short_ci'].isin(continuous_alert_ids)].copy()

        df_continuous_details['hour'] = df_continuous_details['created_on'].dt.hour
        df_continuous_details['day_of_week'] = df_continuous_details['created_on'].dt.dayofweek
        df_continuous_details['day_name'] = df_continuous_details['created_on'].dt.day_name()

        st.subheader("â° PadrÃ£o de RecorrÃªncia por Hora do Dia")

        hourly_dist = df_continuous_details['hour'].value_counts().sort_index()
        hourly_pct = (hourly_dist / hourly_dist.sum() * 100).round(2)

        top_3_hours = hourly_pct.nlargest(3)
        total_top_3_hours = top_3_hours.sum()

        col1, col2 = st.columns([2, 1])

        with col1:
            fig_hourly = go.Figure()
            fig_hourly.add_trace(go.Bar(
                x=hourly_dist.index,
                y=hourly_dist.values,
                marker_color=['red' if i in top_3_hours.index else 'lightblue'
                             for i in hourly_dist.index],
                text=hourly_pct.values,
                texttemplate='%{text:.1f}%',
                textposition='outside',
                hovertemplate='Hora: %{x}:00<br>Alertas: %{y}<br>% do total: %{text:.1f}%<extra></extra>'
            ))
            fig_hourly.update_layout(
                title="DistribuiÃ§Ã£o de Alertas ContÃ­nuos por Hora",
                xaxis_title="Hora do Dia",
                yaxis_title="Quantidade de Alertas",
                showlegend=False,
                height=400
            )
            st.plotly_chart(fig_hourly, use_container_width=True, key='recurrence_hourly')

        with col2:
            st.metric("ğŸ• Hora com Mais Alertas", f"{top_3_hours.index[0]}:00")
            st.metric("ğŸ“Š % nesta Hora", f"{top_3_hours.values[0]:.1f}%")
            st.metric("ğŸ” Top 3 Horas (% total)", f"{total_top_3_hours:.1f}%")

            if total_top_3_hours > 60:
                pattern_hour = "ğŸ”´ **Concentrado**"
                hour_desc = "Alertas altamente concentrados em poucas horas"
            elif total_top_3_hours > 40:
                pattern_hour = "ğŸŸ¡ **Moderado**"
                hour_desc = "Alertas parcialmente concentrados"
            else:
                pattern_hour = "ğŸŸ¢ **DistribuÃ­do**"
                hour_desc = "Alertas bem distribuÃ­dos ao longo do dia"

            st.write(f"**PadrÃ£o:** {pattern_hour}")
            st.write(hour_desc)

        st.write("**ğŸ” Top 5 HorÃ¡rios:**")
        top_5_hours = hourly_pct.nlargest(5)
        for hour, pct in top_5_hours.items():
            st.write(f"â€¢ **{hour:02d}:00** - {hourly_dist[hour]} alertas ({pct:.1f}%)")

        st.markdown("---")

        st.subheader("ğŸ“… PadrÃ£o de RecorrÃªncia por Dia da Semana")
        daily_dist = df_continuous_details['day_name'].value_counts()
        days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        daily_dist_ordered = daily_dist.reindex(days_order).fillna(0)
        daily_pct = (daily_dist_ordered / daily_dist_ordered.sum() * 100).round(2)

        top_3_days = daily_pct.nlargest(3)
        total_top_3_days = top_3_days.sum()

        day_translation = {
            'Monday': 'Segunda', 'Tuesday': 'TerÃ§a', 'Wednesday': 'Quarta',
            'Thursday': 'Quinta', 'Friday': 'Sexta', 'Saturday': 'SÃ¡bado', 'Sunday': 'Domingo'
        }

        daily_pct_pt = daily_pct.rename(index=day_translation)
        daily_dist_ordered_pt = daily_dist_ordered.rename(index=day_translation)

        col1, col2 = st.columns([2, 1])

        with col1:
            fig_daily = go.Figure()
            fig_daily.add_trace(go.Bar(
                x=list(daily_pct_pt.index),
                y=daily_dist_ordered_pt.values,
                marker_color=['red' if day in [day_translation[d] for d in top_3_days.index] else 'lightgreen'
                             for day in daily_pct_pt.index],
                text=daily_pct_pt.values,
                texttemplate='%{text:.1f}%',
                textposition='outside',
                hovertemplate='Dia: %{x}<br>Alertas: %{y}<br>% do total: %{text:.1f}%<extra></extra>'
            ))
            fig_daily.update_layout(
                title="DistribuiÃ§Ã£o de Alertas ContÃ­nuos por Dia da Semana",
                xaxis_title="Dia da Semana",
                yaxis_title="Quantidade de Alertas",
                showlegend=False,
                height=400
            )
            st.plotly_chart(fig_daily, use_container_width=True, key='recurrence_daily')

        with col2:
            top_day_en = top_3_days.index[0]
            top_day_pt = day_translation[top_day_en]
            st.metric("ğŸ“… Dia com Mais Alertas", top_day_pt)
            st.metric("ğŸ“Š % neste Dia", f"{top_3_days.values[0]:.1f}%")
            st.metric("ğŸ” Top 3 Dias (% total)", f"{total_top_3_days:.1f}%")

            if total_top_3_days > 60:
                pattern_day = "ğŸ”´ **Concentrado**"
                day_desc = "Alertas altamente concentrados em poucos dias"
            elif total_top_3_days > 45:
                pattern_day = "ğŸŸ¡ **Moderado**"
                day_desc = "Alertas parcialmente concentrados"
            else:
                pattern_day = "ğŸŸ¢ **DistribuÃ­do**"
                day_desc = "Alertas bem distribuÃ­dos na semana"

            st.write(f"**PadrÃ£o:** {pattern_day}")
            st.write(day_desc)

        st.write("**ğŸ” Ranking de Dias:**")
        top_days_sorted = daily_pct.sort_values(ascending=False)
        for day, pct in top_days_sorted.items():
            day_pt = day_translation[day]
            count = daily_dist_ordered[day]
            st.write(f"â€¢ **{day_pt}** - {int(count)} alertas ({pct:.1f}%)")

        st.markdown("---")

        st.subheader("ğŸ¯ Resumo do PadrÃ£o de RecorrÃªncia")

        col1, col2 = st.columns(2)

        with col1:
            st.write("**â° PadrÃ£o HorÃ¡rio:**")
            st.write(f"â€¢ {pattern_hour}")
            st.write(f"â€¢ Top 3 horas concentram {total_top_3_hours:.1f}% dos alertas")
            st.write(f"â€¢ HorÃ¡rio principal: **{top_3_hours.index[0]:02d}:00**")

            if total_top_3_hours > 50:
                st.write("ğŸ’¡ **RecomendaÃ§Ã£o:** Avaliar janela de manutenÃ§Ã£o neste horÃ¡rio")

        with col2:
            st.write("**ğŸ“… PadrÃ£o Semanal:**")
            st.write(f"â€¢ {pattern_day}")
            st.write(f"â€¢ Top 3 dias concentram {total_top_3_days:.1f}% dos alertas")
            st.write(f"â€¢ Dia principal: **{day_translation[top_day_en]}**")

            if total_top_3_days > 50:
                st.write("ğŸ’¡ **RecomendaÃ§Ã£o:** AtenÃ§Ã£o redobrada nestes dias")

        st.markdown("---")
        st.subheader("ğŸ† PadrÃ£o Dominante")

        if total_top_3_hours > total_top_3_days:
            st.success(f"â° **HORA DO DIA** Ã© o padrÃ£o dominante ({total_top_3_hours:.1f}% vs {total_top_3_days:.1f}%)")
            st.write(f"Os alertas contÃ­nuos tendem a ocorrer principalmente no horÃ¡rio das **{top_3_hours.index[0]:02d}:00**")
        elif total_top_3_days > total_top_3_hours:
            st.success(f"ğŸ“… **DIA DA SEMANA** Ã© o padrÃ£o dominante ({total_top_3_days:.1f}% vs {total_top_3_hours:.1f}%)")
            st.write(f"Os alertas contÃ­nuos tendem a ocorrer principalmente Ã s **{day_translation[top_day_en]}**")
        else:
            st.info("ğŸ“Š **PadrÃ£o BALANCEADO** - NÃ£o hÃ¡ concentraÃ§Ã£o clara em hora ou dia especÃ­ficos")

        st.markdown("---")

        st.subheader("ğŸ”¥ Mapa de Calor: Hora Ã— Dia da Semana")

        heatmap_data = df_continuous_details.groupby(['day_of_week', 'hour']).size().reset_index(name='count')
        heatmap_pivot = heatmap_data.pivot(index='hour', columns='day_of_week', values='count').fillna(0)

        day_map = {0: 'Seg', 1: 'Ter', 2: 'Qua', 3: 'Qui', 4: 'Sex', 5: 'SÃ¡b', 6: 'Dom'}
        heatmap_pivot.columns = [day_map[col] for col in heatmap_pivot.columns]

        fig_heatmap = go.Figure(data=go.Heatmap(
            z=heatmap_pivot.values,
            x=heatmap_pivot.columns,
            y=heatmap_pivot.index,
            colorscale='Reds',
            hovertemplate='Dia: %{x}<br>Hora: %{y}:00<br>Alertas: %{z}<extra></extra>'
        ))

        fig_heatmap.update_layout(
            title="ConcentraÃ§Ã£o de Alertas por Dia e Hora",
            xaxis_title="Dia da Semana",
            yaxis_title="Hora do Dia",
            height=600
        )

        st.plotly_chart(fig_heatmap, use_container_width=True, key='recurrence_heatmap')

    def show_global_overview(self):
        st.subheader("ğŸ“ˆ VisÃ£o Geral dos Alertas")
        
        df_to_analyze = self.df_all_alerts
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**ğŸ”¥ Top 10 Alertas Mais Frequentes**")
            top_frequent = df_to_analyze.nlargest(10, 'total_ocorrencias')[['alert_id', 'total_ocorrencias', 'freq_dia', 'pattern_type', 'unique_days']]
            top_frequent.columns = ['Alert ID', 'Total OcorrÃªncias', 'FrequÃªncia/Dia', 'Tipo', 'Dias Ãšnicos']
            st.dataframe(top_frequent, use_container_width=True)
        with col2:
            st.write("**âš¡ Top 10 Alertas Mais RÃ¡pidos (Menor Intervalo)**")
            df_with_intervals = df_to_analyze.dropna(subset=['intervalo_medio_h'])
            if len(df_with_intervals) > 0:
                top_fast = df_with_intervals.nsmallest(10, 'intervalo_medio_h')[['alert_id', 'intervalo_medio_h', 'total_ocorrencias', 'pattern_type']]
                top_fast.columns = ['Alert ID', 'Intervalo MÃ©dio (h)', 'Total OcorrÃªncias', 'Tipo']
                st.dataframe(top_fast, use_container_width=True)
            else:
                st.info("Sem dados de intervalo disponÃ­veis")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            fig_freq = px.histogram(df_to_analyze, x='freq_dia', title="ğŸ“Š DistribuiÃ§Ã£o de FrequÃªncia (alertas/dia)",
                                   labels={'freq_dia': 'Alertas por Dia', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_freq, use_container_width=True)
        with col2:
            fig_int = px.histogram(df_to_analyze, x='freq_semana', title="ğŸ“Š DistribuiÃ§Ã£o de FrequÃªncia (alertas/semana)",
                                  labels={'freq_semana': 'Alertas por semana', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_int, use_container_width=True)
        with col3:
            fig_int = px.histogram(df_to_analyze, x='freq_mes', title="ğŸ“Š DistribuiÃ§Ã£o de FrequÃªncia (alertas/mÃªs)",
                                  labels={'freq_mes': 'Alertas por mÃªs', 'count': 'Quantidade de Alert IDs'})
            st.plotly_chart(fig_int, use_container_width=True)
        with col4:
            df_with_intervals = df_to_analyze.dropna(subset=['intervalo_medio_h'])
            if len(df_with_intervals) > 0:
                fig_int = px.histogram(df_with_intervals, x='intervalo_medio_h', title="â±ï¸ DistribuiÃ§Ã£o de Intervalos MÃ©dios",
                                      labels={'intervalo_medio_h': 'Intervalo MÃ©dio (horas)', 'count': 'Quantidade de Alert IDs'})
                st.plotly_chart(fig_int, use_container_width=True)

    def perform_clustering_analysis(self, use_only_continuous=True):
        st.subheader("ğŸ¯ Agrupamento de Alertas por Perfil de Comportamento")
        
        df_for_clustering = self.df_all_alerts
        if use_only_continuous:
            df_for_clustering = self.df_all_alerts[self.df_all_alerts['pattern_type'] == 'continuous'].copy()
            st.info(f"ğŸ” Usando apenas alertas contÃ­nuos para clustering ({len(df_for_clustering)} alertas)")
        
        if len(df_for_clustering) < 2:
            st.warning("âš ï¸ Dados insuficientes para clustering")
            return None
        
        features = [
            'freq_dia', 'intervalo_medio_h', 'intervalo_std_h',
            'hora_pico', 'pct_fins_semana', 'pct_horario_comercial', 'variabilidade_intervalo'
        ]
        X = df_for_clustering[features].fillna(0)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        st.write("**ğŸ” Determinando NÃºmero Ã“timo de Clusters...**")
        max_clusters = min(10, len(X) - 1)
        silhouette_scores = []
        
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)
            silhouette_scores.append(silhouette_score(X_scaled, cluster_labels))
        
        optimal_k = range(2, max_clusters + 1)[np.argmax(silhouette_scores)]
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ğŸ¯ NÃºmero Ã“timo de Clusters", optimal_k)
        with col2:
            st.metric("ğŸ“Š Silhouette Score", f"{max(silhouette_scores):.3f}")
        
        kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        clusters = kmeans_final.fit_predict(X_scaled)
        
        df_for_clustering['cluster'] = clusters
        
        self.df_all_alerts['cluster'] = np.nan
        self.df_all_alerts.loc[df_for_clustering.index, 'cluster'] = df_for_clustering['cluster']
        
        col1, col2 = st.columns(2)
        with col1:
            fig_scatter = px.scatter(
                df_for_clustering,
                x='freq_dia',
                y='intervalo_medio_h',
                color='cluster',
                size='total_ocorrencias',
                hover_data=['alert_id'],
                title="ğŸ¨ Clusters: FrequÃªncia vs Intervalo MÃ©dio"
            )
            st.plotly_chart(fig_scatter, use_container_width=True, key='cluster_scatter')
        with col2:
            cluster_dist = df_for_clustering['cluster'].value_counts().sort_index()
            fig_dist = px.bar(
                x=cluster_dist.index,
                y=cluster_dist.values,
                title="ğŸ“Š DistribuiÃ§Ã£o de Alertas por Cluster",
                labels={'x': 'Cluster', 'y': 'Quantidade de Alert IDs'}
            )
            st.plotly_chart(fig_dist, use_container_width=True, key='cluster_dist')
        return optimal_k

    def show_cluster_profiles(self, n_clusters):
        st.subheader("ğŸ‘¥ Perfis dos Clusters")
        cluster_tabs = st.tabs([f"Cluster {i}" for i in range(n_clusters)])
        for i in range(n_clusters):
            with cluster_tabs[i]:
                cluster_data = self.df_all_alerts[self.df_all_alerts['cluster'] == i]
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ğŸ“Š Quantidade de Alertas", len(cluster_data))
                with col2:
                    avg_freq = cluster_data['freq_dia'].mean()
                    st.metric("ğŸ“ˆ Freq. MÃ©dia/Dia", f"{avg_freq:.2f}")
                with col3:
                    avg_interval = cluster_data['intervalo_medio_h'].mean()
                    st.metric("â±ï¸ Intervalo MÃ©dio (h)", f"{avg_interval:.2f}")
                with col4:
                    avg_hour = cluster_data['hora_pico'].mean()
                    st.metric("ğŸ• Hora Pico MÃ©dia", f"{avg_hour:.0f}:00")
                st.write("**ğŸ¯ CaracterÃ­sticas do Cluster:**")
                weekend_pct = cluster_data['pct_fins_semana'].mean()
                business_pct = cluster_data['pct_horario_comercial'].mean()
                characteristics = []
                if avg_freq > self.df_all_alerts['freq_dia'].median():
                    characteristics.append("ğŸ”¥ **Alta frequÃªncia**")
                else:
                    characteristics.append("ğŸŒ **Baixa frequÃªncia**")
                if avg_interval < self.df_all_alerts['intervalo_medio_h'].median():
                    characteristics.append("âš¡ **Intervalos curtos**")
                else:
                    characteristics.append("â³ **Intervalos longos**")
                if weekend_pct > 30:
                    characteristics.append("ğŸ—“ï¸ **Ativo nos fins de semana**")
                if business_pct > 70:
                    characteristics.append("ğŸ¢ **Predominantemente em horÃ¡rio comercial**")
                elif business_pct < 30:
                    characteristics.append("ğŸŒ™ **Predominantemente fora do horÃ¡rio comercial**")
                for char in characteristics:
                    st.write(f"â€¢ {char}")
                with st.expander(f"ğŸ“‹ Alertas no Cluster {i}"):
                    cluster_alerts = cluster_data[['alert_id', 'total_ocorrencias', 'freq_dia', 'intervalo_medio_h']].copy()
                    cluster_alerts.columns = ['Alert ID', 'Total OcorrÃªncias', 'Freq/Dia', 'Intervalo MÃ©dio (h)']
                    st.dataframe(cluster_alerts, use_container_width=True, key=f'cluster_table_{i}')

    def show_cluster_recommendations(self):
        st.subheader("ğŸ’¡ RecomendaÃ§Ãµes por Cluster")
        for cluster_id in sorted(self.df_all_alerts['cluster'].dropna().unique()):
            cluster_data = self.df_all_alerts[self.df_all_alerts['cluster'] == cluster_id]
            avg_freq = cluster_data['freq_dia'].mean()
            avg_interval = cluster_data['intervalo_medio_h'].mean()
            weekend_pct = cluster_data['pct_fins_semana'].mean()
            business_pct = cluster_data['pct_horario_comercial'].mean()
            with st.expander(f"ğŸ¯ RecomendaÃ§Ãµes para Cluster {int(cluster_id)} ({len(cluster_data)} alertas)"):
                recommendations = []
                if avg_freq > 5:
                    recommendations.append("ğŸš¨ **Prioridade Alta**: Alertas muito frequentes - investigar causa raiz")
                    recommendations.append("ğŸ”§ **AÃ§Ã£o**: Considerar automaÃ§Ã£o de resposta ou ajuste de thresholds")
                if avg_interval < 1:
                    recommendations.append("âš¡ **Rajadas detectadas**: PossÃ­vel tempestade de alertas")
                    recommendations.append("ğŸ›¡ï¸ **AÃ§Ã£o**: Implementar rate limiting ou supressÃ£o inteligente")
                if weekend_pct > 50:
                    recommendations.append("ğŸ—“ï¸ **PadrÃ£o de fim de semana**: Alertas ativos nos fins de semana")
                    recommendations.append("ğŸ‘¥ **AÃ§Ã£o**: Verificar cobertura de plantÃ£o")
                if business_pct < 30:
                    recommendations.append("ğŸŒ™ **PadrÃ£o noturno**: Principalmente fora do horÃ¡rio comercial")
                    recommendations.append("ğŸ”„ **AÃ§Ã£o**: Considerar processos automatizados noturnos")
                if avg_freq < 0.5:
                    recommendations.append("ğŸ“‰ **Baixa frequÃªncia**: Alertas esporÃ¡dicos")
                    recommendations.append("ğŸ“Š **AÃ§Ã£o**: Revisar relevÃ¢ncia e configuraÃ§Ã£o do alerta")
                for rec in recommendations:
                    st.write(f"â€¢ {rec}")
                if not recommendations:
                    st.write("â€¢ âœ… **PadrÃ£o normal**: Nenhuma aÃ§Ã£o especÃ­fica recomendada")

    def show_basic_stats(self):
        st.header("ğŸ“Š EstatÃ­sticas BÃ¡sicas")
        total = len(self.df)
        period_days = (self.dates.max() - self.dates.min()).days + 1
        avg_per_day = total / period_days
        unique_days = self.df['date'].nunique()
        
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("ğŸ”¥ Total de OcorrÃªncias", total)
        with col2:
            st.metric("ğŸ“… PerÃ­odo (dias)", period_days)
        with col3:
            st.metric("ğŸ“† Dias Ãšnicos", unique_days)
        with col4:
            st.metric("ğŸ“ˆ MÃ©dia/dia", f"{avg_per_day:.2f}")
        with col5:
            last_alert = self.dates.max().strftime("%d/%m %H:%M")
            st.metric("ğŸ• Ãšltimo Alerta", last_alert)
        
        if unique_days == 1:
            st.warning("âš ï¸ **ATENÃ‡ÃƒO:** Todos os alertas ocorreram em apenas 1 dia! Este alerta Ã© classificado como ISOLADO.")
        
        # ============================================================
        # NOVA SEÃ‡ÃƒO: MÃ‰DIAS DE FREQUÃŠNCIA
        # ============================================================
        st.markdown("---")
        st.subheader("ğŸ“Š MÃ©dias de FrequÃªncia de Incidentes")
        
        # Calcular mÃ©dias
        total_hours = period_days * 24
        period_weeks = period_days / 7
        period_months = period_days / 30.44  # MÃ©dia de dias por mÃªs
        
        avg_per_hour = total / total_hours if total_hours > 0 else 0
        avg_per_week = total / period_weeks if period_weeks > 0 else 0
        avg_per_month = total / period_months if period_months > 0 else 0
        
        # Exibir mÃ©tricas em colunas
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ğŸ“… MÃ©dia por Dia", 
                f"{avg_per_day:.2f}",
                help=f"{total} incidentes em {period_days} dias"
            )
            if avg_per_day >= 10:
                st.caption("ğŸ”´ Alta frequÃªncia diÃ¡ria")
            elif avg_per_day >= 3:
                st.caption("ğŸŸ¡ FrequÃªncia diÃ¡ria moderada")
            else:
                st.caption("ğŸŸ¢ FrequÃªncia diÃ¡ria baixa")
        
        with col2:
            st.metric(
                "ğŸ• MÃ©dia por Hora", 
                f"{avg_per_hour:.4f}",
                help=f"{total} incidentes em {total_hours:.0f} horas"
            )
            if avg_per_hour >= 1:
                st.caption("ğŸ”´ Mais de 1 por hora")
            elif avg_per_hour >= 0.1:
                st.caption("ğŸŸ¡ MÃºltiplos por dia")
            else:
                st.caption("ğŸŸ¢ Menos de 1 a cada 10h")
        
        with col3:
            st.metric(
                "ğŸ“† MÃ©dia por Semana", 
                f"{avg_per_week:.2f}",
                help=f"{total} incidentes em {period_weeks:.1f} semanas"
            )
            if avg_per_week >= 50:
                st.caption("ğŸ”´ Alta frequÃªncia semanal")
            elif avg_per_week >= 15:
                st.caption("ğŸŸ¡ FrequÃªncia semanal moderada")
            else:
                st.caption("ğŸŸ¢ FrequÃªncia semanal baixa")
        
        with col4:
            st.metric(
                "ğŸ“Š MÃ©dia por MÃªs", 
                f"{avg_per_month:.2f}",
                help=f"{total} incidentes em {period_months:.1f} meses"
            )
            if avg_per_month >= 200:
                st.caption("ğŸ”´ Alta frequÃªncia mensal")
            elif avg_per_month >= 60:
                st.caption("ğŸŸ¡ FrequÃªncia mensal moderada")
            else:
                st.caption("ğŸŸ¢ FrequÃªncia mensal baixa")
        
        # VisualizaÃ§Ã£o adicional - GrÃ¡fico comparativo
        st.markdown("##### ğŸ“ˆ Comparativo de FrequÃªncias (normalizado)")
        
        # Normalizar valores para escala 0-100 para comparaÃ§Ã£o visual
        max_freq = max(avg_per_day * 10, avg_per_hour * 1000, avg_per_week, avg_per_month / 10)
        if max_freq > 0:
            norm_day = (avg_per_day * 10 / max_freq) * 100
            norm_hour = (avg_per_hour * 1000 / max_freq) * 100
            norm_week = (avg_per_week / max_freq) * 100
            norm_month = ((avg_per_month / 10) / max_freq) * 100
        else:
            norm_day = norm_hour = norm_week = norm_month = 0
        
        fig_freq_comp = go.Figure()
        
        frequencies = ['Por Dia', 'Por Hora\n(x1000)', 'Por Semana', 'Por MÃªs\n(Ã·10)']
        values = [avg_per_day, avg_per_hour * 1000, avg_per_week, avg_per_month / 10]
        normalized = [norm_day, norm_hour, norm_week, norm_month]
        
        fig_freq_comp.add_trace(go.Bar(
            x=frequencies,
            y=values,
            text=[f"{v:.2f}" for v in values],
            textposition='outside',
            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'],
            hovertemplate='<b>%{x}</b><br>Valor: %{text}<br>Intensidade: %{customdata:.1f}%<extra></extra>',
            customdata=normalized
        ))
        
        fig_freq_comp.update_layout(
            title="Comparativo de FrequÃªncias (valores ajustados para visualizaÃ§Ã£o)",
            yaxis_title="FrequÃªncia Ajustada",
            height=350,
            showlegend=False
        )
        
        st.plotly_chart(fig_freq_comp, use_container_width=True, key='frequency_comparison')
        
        # InterpretaÃ§Ã£o automÃ¡tica
        st.markdown("##### ğŸ’¡ InterpretaÃ§Ã£o da FrequÃªncia")
        
        if avg_per_day >= 10:
            st.error("ğŸ”´ **Alerta de Alta FrequÃªncia**: Mais de 10 incidentes por dia em mÃ©dia. Requer atenÃ§Ã£o imediata e possÃ­vel automaÃ§Ã£o.")
        elif avg_per_day >= 3:
            st.warning("ğŸŸ¡ **FrequÃªncia Moderada**: Entre 3-10 incidentes por dia. Considere anÃ¡lise de causa raiz e otimizaÃ§Ã£o.")
        else:
            st.success("ğŸŸ¢ **FrequÃªncia Baixa**: Menos de 3 incidentes por dia. Monitoramento regular Ã© suficiente.")
        
        # EstatÃ­sticas adicionais por perÃ­odo
        with st.expander("ğŸ“Š Ver DistribuiÃ§Ã£o Detalhada por PerÃ­odos"):
            # Por dia
            daily_counts = self.df.groupby('date').size()
            st.write("**ğŸ“… EstatÃ­sticas DiÃ¡rias:**")
            col1, col2, col3 = st.columns(3)
            col1.metric("Dia com Mais Alertas", daily_counts.max())
            col2.metric("Dia com Menos Alertas", daily_counts.min())
            col3.metric("Desvio PadrÃ£o", f"{daily_counts.std():.2f}")
            
            # Por hora do dia
            hourly_counts = self.df.groupby('hour').size()
            st.write("**ğŸ• EstatÃ­sticas por Hora do Dia:**")
            col1, col2, col3 = st.columns(3)
            col1.metric("Hora Mais Ativa", f"{hourly_counts.idxmax()}:00")
            col2.metric("MÃ¡x. Alertas em 1 Hora", hourly_counts.max())
            col3.metric("MÃ©dia por Hora Ativa", f"{hourly_counts.mean():.2f}")
        
        st.markdown("---")
        
        intervals = self.df['time_diff_hours'].dropna()
        if len(intervals) > 0:
            st.subheader("â±ï¸ Intervalos Entre Alertas")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("MÃ©dia (h)", f"{intervals.mean():.2f}")
            with col2:
                st.metric("Mediana (h)", f"{intervals.median():.2f}")
            with col3:
                st.metric("MÃ­nimo (h)", f"{intervals.min():.2f}")
            with col4:
                st.metric("MÃ¡ximo (h)", f"{intervals.max():.2f}")
                

    def batch_analyze_all_short_ci(self, progress_bar=None):
        """
        NOVA FUNCIONALIDADE: AnÃ¡lise de reincidÃªncia em lote para TODOS os short_ci
        """
        try:
            # Verificar se df_original existe
            if self.df_original is None or len(self.df_original) == 0:
                st.error("âŒ Dados nÃ£o carregados. Por favor, faÃ§a upload de um arquivo primeiro.")
                return None
            
            all_results = []
            short_ci_list = self.df_original['short_ci'].unique()
            total = len(short_ci_list)
            
            for idx, short_ci in enumerate(short_ci_list):
                # Atualizar progress bar
                if progress_bar:
                    progress_bar.progress((idx + 1) / total, 
                                         text=f"Analisando {idx + 1}/{total}: {short_ci}")
                
                # Filtrar dados para este short_ci
                df_ci = self.df_original[self.df_original['short_ci'] == short_ci].copy()
                df_ci['created_on'] = pd.to_datetime(df_ci['created_on'], errors='coerce')
                df_ci = df_ci.dropna(subset=['created_on'])
                df_ci = df_ci.sort_values('created_on')
                
                # Verificar se tem dados suficientes
                if len(df_ci) < 3:
                    all_results.append({
                        'short_ci': short_ci,
                        'total_occurrences': len(df_ci),
                        'score': 0,
                        'classification': 'âšª DADOS INSUFICIENTES',
                        'mean_interval_hours': None,
                        'median_interval_hours': None,
                        'cv': None,
                        'regularity_score': 0,
                        'periodicity_detected': False,
                        'dominant_period_hours': None,
                        'predictability_score': 0,
                        'next_occurrence_prediction_hours': None
                    })
                    continue
                
                # Executar anÃ¡lise silenciosa
                analyzer = AdvancedRecurrenceAnalyzer(df_ci, short_ci)
                result = analyzer.analyze_silent()
                
                if result:
                    all_results.append(result)
                else:
                    all_results.append({
                        'short_ci': short_ci,
                        'total_occurrences': len(df_ci),
                        'score': 0,
                        'classification': 'âšª ERRO NA ANÃLISE',
                        'mean_interval_hours': None,
                        'median_interval_hours': None,
                        'cv': None,
                        'regularity_score': 0,
                        'periodicity_detected': False,
                        'dominant_period_hours': None,
                        'predictability_score': 0,
                        'next_occurrence_prediction_hours': None
                    })
            
            return pd.DataFrame(all_results)
        
        except Exception as e:
            st.error(f"Erro na anÃ¡lise em lote: {e}")
            return None

    def complete_analysis_all_short_ci(self, progress_bar=None):
        """
        AnÃ¡lise COMPLETA de todos os short_ci combinando anÃ¡lise global + reincidÃªncia
        Retorna DataFrame consolidado com TODAS as mÃ©tricas
        """
        try:
            # Verificar se df_original existe
            if self.df_original is None or len(self.df_original) == 0:
                st.error("âŒ Dados nÃ£o carregados. Por favor, faÃ§a upload de um arquivo primeiro.")
                return None
            
            # 1. Executar anÃ¡lise global (isolados vs contÃ­nuos)
            if progress_bar:
                progress_bar.progress(0.1, text="Executando anÃ¡lise global...")
            
            alert_ids = self.df_original['short_ci'].unique()
            results_global = []
            
            for alert_id in alert_ids:
                metrics = process_single_alert(
                    alert_id, 
                    self.df_original, 
                    self.max_gap_hours, 
                    self.min_group_size, 
                    self.spike_threshold_multiplier
                )
                if metrics:
                    results_global.append(metrics)
            
            df_global = pd.DataFrame(results_global)
            
            # 2. Executar anÃ¡lise de reincidÃªncia
            if progress_bar:
                progress_bar.progress(0.3, text="Executando anÃ¡lise de reincidÃªncia...")
            
            all_results = []
            total = len(alert_ids)
            
            for idx, short_ci in enumerate(alert_ids):
                if progress_bar:
                    progress = 0.3 + (0.6 * (idx + 1) / total)
                    progress_bar.progress(progress, text=f"Analisando reincidÃªncia {idx + 1}/{total}: {short_ci}")
                
                df_ci = self.df_original[self.df_original['short_ci'] == short_ci].copy()
                df_ci['created_on'] = pd.to_datetime(df_ci['created_on'], errors='coerce')
                df_ci = df_ci.dropna(subset=['created_on'])
                df_ci = df_ci.sort_values('created_on')
                
                if len(df_ci) < 3:
                    all_results.append({
                        'short_ci': short_ci,
                        'reincidencia_score': 0,
                        'reincidencia_status': 'âšª DADOS INSUFICIENTES',
                        'total_occurrences_reincidencia': len(df_ci),
                        'mean_interval_hours_reincidencia': None,
                        'cv_reincidencia': None,
                        'regularity_score': 0,
                        'periodicity_detected': False,
                        'predictability_score': 0
                    })
                    continue
                
                analyzer = AdvancedRecurrenceAnalyzer(df_ci, short_ci)
                result = analyzer.analyze_silent()
                
                if result:
                    all_results.append({
                        'short_ci': short_ci,
                        'reincidencia_score': result['score'],
                        'reincidencia_status': result['classification'],
                        'total_occurrences_reincidencia': result['total_occurrences'],
                        'mean_interval_hours_reincidencia': result['mean_interval_hours'],
                        'cv_reincidencia': result['cv'],
                        'regularity_score': result['regularity_score'],
                        'periodicity_detected': result['periodicity_detected'],
                        'predictability_score': result['predictability_score']
                    })
                else:
                    all_results.append({
                        'short_ci': short_ci,
                        'reincidencia_score': 0,
                        'reincidencia_status': 'âšª ERRO NA ANÃLISE',
                        'total_occurrences_reincidencia': len(df_ci),
                        'mean_interval_hours_reincidencia': None,
                        'cv_reincidencia': None,
                        'regularity_score': 0,
                        'periodicity_detected': False,
                        'predictability_score': 0
                    })
            
            df_reincidencia = pd.DataFrame(all_results)
            
            # 3. Merge dos dois DataFrames
            if progress_bar:
                progress_bar.progress(0.95, text="Consolidando resultados...")
            
            # Renomear colunas do df_global para evitar conflitos
            df_global = df_global.rename(columns={'alert_id': 'short_ci'})
            
            # Fazer merge
            df_consolidated = pd.merge(
                df_global,
                df_reincidencia,
                on='short_ci',
                how='outer'
            )
            
            # Reordenar colunas para priorizar as mais importantes
            priority_columns = [
                'short_ci',
                'reincidencia_score',
                'reincidencia_status',
                'pattern_type',
                'total_ocorrencias',
                'num_grupos',
                'alertas_isolados',
                'alertas_agrupados',
                'pct_isolados'
            ]
            
            # Adicionar colunas restantes
            other_columns = [col for col in df_consolidated.columns if col not in priority_columns]
            final_columns = priority_columns + other_columns
            
            # Reordenar apenas colunas que existem
            final_columns = [col for col in final_columns if col in df_consolidated.columns]
            df_consolidated = df_consolidated[final_columns]
            
            # Ordenar por score de reincidÃªncia decrescente
            df_consolidated = df_consolidated.sort_values('reincidencia_score', ascending=False)
            
            if progress_bar:
                progress_bar.progress(1.0, text="AnÃ¡lise completa!")
            
            return df_consolidated
        
        except Exception as e:
            st.error(f"Erro na anÃ¡lise completa: {e}")
            import traceback
            st.error(traceback.format_exc())
            return None


    def show_individual_alert_analysis(self):
        st.header(f"ğŸ“Œ AnÃ¡lise Individual do Alert ID: {self.alert_id}")

        if self.df is None or len(self.df) == 0:
            st.info("Nenhum dado disponÃ­vel para este alerta.")
            return

        unique_days = self.df['date'].nunique()
        is_single_day = unique_days == 1

        df_isolated = self.df[self.df['is_isolated']]
        df_grouped = self.df[~self.df['is_isolated']]

        st.subheader("ğŸ“Š EstatÃ­sticas Gerais do Alert ID")
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        with col1:
            st.metric("Total OcorrÃªncias", len(self.df))
        with col2:
            st.metric("ğŸ”´ Isolados", len(df_isolated))
        with col3:
            st.metric("ğŸŸ¢ Agrupados", len(df_grouped))
        with col4:
            st.metric("ğŸ“¦ NÂº de Grupos", len(self.groups_info))
        with col5:
            pct_isolated = (len(df_isolated) / len(self.df) * 100) if len(self.df) > 0 else 0
            st.metric("% Isolados", f"{pct_isolated:.1f}%")
        with col6:
            st.metric("ğŸ“† Dias Ãšnicos", unique_days)

        if is_single_day:
            st.warning("âš ï¸ **ATENÃ‡ÃƒO:** Todos os alertas ocorreram em apenas 1 dia! Este padrÃ£o Ã© classificado como ISOLADO.")
            st.info(f"ğŸ“… Data Ãºnica: {self.df['date'].iloc[0]}")

        if len(self.groups_info) > 0:
            st.subheader("ğŸ“¦ InformaÃ§Ãµes dos Grupos")
            groups_df = pd.DataFrame(self.groups_info)
            groups_df['start_time'] = pd.to_datetime(groups_df['start_time']).dt.strftime('%Y-%m-%d %H:%M')
            groups_df['end_time'] = pd.to_datetime(groups_df['end_time']).dt.strftime('%Y-%m-%d %H:%M')
            groups_df['duration_hours'] = groups_df['duration_hours'].round(2)
            groups_df.columns = ['ID Grupo', 'Tamanho', 'InÃ­cio', 'Fim', 'DuraÃ§Ã£o (h)']
            st.dataframe(groups_df, use_container_width=True)

        st.subheader("ğŸ“ˆ GrÃ¡fico de Linhas: Alertas ao Longo do Tempo")

        df_daily = self.df.groupby(['date', 'is_isolated']).size().reset_index(name='count')
        df_daily_pivot = df_daily.pivot(index='date', columns='is_isolated', values='count').fillna(0)

        new_column_names = {}
        if False in df_daily_pivot.columns:
            new_column_names[False] = 'Agrupados'
        if True in df_daily_pivot.columns:
            new_column_names[True] = 'Isolados'

        df_daily_pivot = df_daily_pivot.rename(columns=new_column_names)

        if 'Agrupados' not in df_daily_pivot.columns:
            df_daily_pivot['Agrupados'] = 0
        if 'Isolados' not in df_daily_pivot.columns:
            df_daily_pivot['Isolados'] = 0

        fig_timeline = go.Figure()

        fig_timeline.add_trace(go.Scatter(
            x=df_daily_pivot.index,
            y=df_daily_pivot['Isolados'],
            mode='lines+markers',
            name='Isolados',
            line=dict(color='red', width=2),
            marker=dict(size=8),
            fill='tozeroy',
            fillcolor='rgba(255, 68, 68, 0.3)'
        ))

        fig_timeline.add_trace(go.Scatter(
            x=df_daily_pivot.index,
            y=df_daily_pivot['Agrupados'],
            mode='lines+markers',
            name='Agrupados',
            line=dict(color='green', width=2),
            marker=dict(size=8),
            fill='tozeroy',
            fillcolor='rgba(68, 255, 68, 0.3)'
        ))

        fig_timeline.update_layout(
            title="EvoluÃ§Ã£o DiÃ¡ria: Alertas Isolados vs Agrupados",
            xaxis_title="Data",
            yaxis_title="Quantidade de Alertas",
            hovermode='x unified',
            height=400
        )

        st.plotly_chart(fig_timeline, use_container_width=True, key='individual_line_chart')
    
        tab1, tab2, tab3 = st.tabs(["ğŸ”´ OcorrÃªncias Isoladas", "ğŸŸ¢ OcorrÃªncias Agrupadas", "ğŸ“Š VisualizaÃ§Ã£o Temporal"])
    
        with tab1:
            st.subheader(f"ğŸ”´ OcorrÃªncias Isoladas ({len(df_isolated)})")
            if len(df_isolated) > 0:
                isolated_display = df_isolated[['created_on', 'hour', 'day_name', 'time_diff_hours', 'date']].copy()
                isolated_display['created_on'] = isolated_display['created_on'].dt.strftime('%Y-%m-%d %H:%M:%S')
                isolated_display.columns = ['Data/Hora', 'Hora', 'Dia da Semana', 'Intervalo (h)', 'Data']
                st.dataframe(isolated_display, use_container_width=True)
                st.write(f"**Percentual:** {len(df_isolated)/len(self.df)*100:.2f}% das ocorrÃªncias sÃ£o isoladas")
                
                daily_counts = df_isolated.groupby('date').size().sort_values(ascending=False)
                if len(daily_counts) > 0:
                    st.write("**ğŸ“ˆ Dias com Mais Alertas Isolados:**")
                    top_days = daily_counts.head(5)
                    for date, count in top_days.items():
                        st.write(f"â€¢ {date}: {count} alertas")
            else:
                st.info("Nenhuma ocorrÃªncia isolada detectada neste alerta.")
    
        with tab2:
            st.subheader(f"ğŸŸ¢ OcorrÃªncias Agrupadas ({len(df_grouped)})")
            if len(df_grouped) > 0:
                grouped_display = df_grouped[['created_on', 'hour', 'day_name', 'time_diff_hours', 'group_id']].copy()
                grouped_display['created_on'] = grouped_display['created_on'].dt.strftime('%Y-%m-%d %H:%M:%S')
                grouped_display.columns = ['Data/Hora', 'Hora', 'Dia da Semana', 'Intervalo (h)', 'Grupo']
                st.dataframe(grouped_display, use_container_width=True)
                st.write(f"**Percentual:** {len(df_grouped)/len(self.df)*100:.2f}% das ocorrÃªncias estÃ£o agrupadas")
            else:
                st.info("Nenhuma ocorrÃªncia agrupada detectada neste alerta.")
        
        with tab3:
            st.subheader("ğŸ“Š VisualizaÃ§Ã£o Temporal dos Alertas")
            
            fig = go.Figure()
            
            if len(df_isolated) > 0:
                fig.add_trace(go.Scatter(
                    x=df_isolated['created_on'],
                    y=[1] * len(df_isolated),
                    mode='markers',
                    name='Isolados',
                    marker=dict(size=10, color='red', symbol='x'),
                    hovertemplate='%{x}<br>Isolado<extra></extra>'
                ))
            
            for group_info in self.groups_info:
                group_id = group_info['group_id']
                group_data = df_grouped[df_grouped['group_id'] == group_id]
                fig.add_trace(go.Scatter(
                    x=group_data['created_on'],
                    y=[1] * len(group_data),
                    mode='markers',
                    name=f'Grupo {group_id}',
                    marker=dict(size=10),
                    hovertemplate='%{x}<br>Grupo ' + str(group_id) + '<extra></extra>'
                ))
            
            fig.update_layout(
                title="Timeline de Alertas (Isolados vs Agrupados)",
                xaxis_title="Data/Hora",
                yaxis=dict(showticklabels=False, title=""),
                height=400,
                hovermode='closest'
            )
            st.plotly_chart(fig, use_container_width=True, key='individual_alert_timeline')


# ============================================================
# FUNÃ‡ÃƒO MAIN - COMPLETA
# ============================================================

def main():
    st.title("ğŸš¨ Analisador de Alertas - VERSÃƒO COMPLETA")
    st.markdown("### AnÃ¡lise individual, global e agrupamento inteligente com 21 anÃ¡lises avanÃ§adas")
    st.sidebar.header("âš™ï¸ ConfiguraÃ§Ãµes")
    
    with st.sidebar.expander("ğŸ›ï¸ ParÃ¢metros de Agrupamento", expanded=False):
        max_gap_hours = st.slider(
            "â±ï¸ Gap MÃ¡ximo Entre Alertas (horas)",
            min_value=1,
            max_value=72,
            value=24,
            help="Alertas separados por mais tempo que isso sÃ£o considerados de grupos diferentes"
        )
        min_group_size = st.slider(
            "ğŸ“Š Tamanho MÃ­nimo do Grupo",
            min_value=2,
            max_value=10,
            value=3,
            help="NÃºmero mÃ­nimo de alertas para formar um grupo vÃ¡lido"
        )
        spike_threshold_multiplier = st.slider(
            "ğŸš€ Multiplicador de Spike",
            min_value=2.0,
            max_value=10.0,
            value=5.0,
            step=0.5,
            help="Dias com mais alertas que mÃ©dia Ã— este valor sÃ£o considerados spikes isolados"
        )
    
    analysis_mode = st.sidebar.selectbox(
        "ğŸ¯ Modo de AnÃ¡lise",
        ["ğŸŒ AnÃ¡lise Global", "ğŸ” AnÃ¡lise Individual", "ğŸ”„ AnÃ¡lise de ReincidÃªncia Global (TODOS)", "ğŸ“Š AnÃ¡lise Completa + CSV Consolidado"],
        help="Escolha entre analisar todos os alertas ou um alerta especÃ­fico"
    )
    
    uploaded_file = st.sidebar.file_uploader(
        "ğŸ“ Upload do arquivo CSV",
        type=['csv'],
        help="FaÃ§a upload do arquivo CSV contendo os dados dos alertas"
    )
    
    if uploaded_file is not None:
        analyzer = StreamlitAlertAnalyzer()
        if analyzer.load_data(uploaded_file):
            if analysis_mode == "ğŸŒ AnÃ¡lise Global":
                st.markdown("---")
                use_multiprocessing = st.sidebar.checkbox(
                    "âš¡ Usar Multiprocessing (Mais RÃ¡pido)", 
                    value=True,
                    help="Processa alertas em paralelo para melhor desempenho"
                )
                if st.sidebar.button("ğŸš€ Executar AnÃ¡lise Global", type="primary"):
                    if analyzer.prepare_global_analysis(use_multiprocessing, max_gap_hours, 
                                                       min_group_size, spike_threshold_multiplier):
                        tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
                            "ğŸ“Š VisÃ£o Geral",
                            "ğŸ” Isolados vs ContÃ­nuos",
                            "ğŸ”¬ Grupos Detalhados",
                            "ğŸ” RecorrÃªncia",
                            "ğŸ¯ Agrupamento", 
                            "ğŸ‘¥ Perfis dos Clusters",
                            "ğŸ’¡ RecomendaÃ§Ãµes"
                        ])
                        with tab1:
                            analyzer.show_global_overview()
                        with tab2:
                            analyzer.show_isolated_vs_continuous_analysis()
                        with tab3:
                            analyzer.show_continuous_groups_detailed_view()
                        with tab4:
                            analyzer.analyze_continuous_recurrence_patterns()
                        with tab5:
                            n_clusters = analyzer.perform_clustering_analysis()
                        with tab6:
                            if n_clusters:
                                analyzer.show_cluster_profiles(n_clusters)
                        with tab7:
                            if n_clusters:
                                analyzer.show_cluster_recommendations()
                        
                        st.sidebar.markdown("---")
                        st.sidebar.subheader("ğŸ“¥ Downloads")
                        csv_buffer = io.StringIO()
                        analyzer.df_all_alerts.to_csv(csv_buffer, index=False)
                        st.sidebar.download_button(
                            label="â¬‡ï¸ Baixar AnÃ¡lise Global",
                            data=csv_buffer.getvalue(),
                            file_name=f"analise_global_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.error("âŒ NÃ£o foi possÃ­vel processar os dados para anÃ¡lise global")
            

            elif analysis_mode == "ğŸ”„ AnÃ¡lise de ReincidÃªncia Global (TODOS)":
                # NOVA FUNCIONALIDADE - AnÃ¡lise de reincidÃªncia para todos
                st.subheader("ğŸ”„ AnÃ¡lise de ReincidÃªncia Global (Todos os Short CI)")
                st.markdown("""
                Esta anÃ¡lise irÃ¡ executar as **16 anÃ¡lises essenciais de reincidÃªncia** para 
                **TODOS** os Short CIs do arquivo e gerar um relatÃ³rio consolidado em CSV.
                """)
                
                if st.sidebar.button("ğŸš€ Executar AnÃ¡lise Global de ReincidÃªncia", type="primary"):
                    if analyzer.prepare_global_analysis():
                        num_short_ci = analyzer.df['short_ci'].nunique() if analyzer.df is not None else 0
                        st.info(f"ğŸ“Š Iniciando anÃ¡lise de {num_short_ci} Short CIs...")
                        
                        # Barra de progresso
                        progress_bar = st.progress(0, text="Iniciando anÃ¡lise...")
                        
                        # Executar anÃ¡lise em lote
                        results_df = analyzer.batch_analyze_all_short_ci(progress_bar)
                        
                        progress_bar.empty()
                        
                        if results_df is not None and len(results_df) > 0:
                            st.success(f"âœ… AnÃ¡lise concluÃ­da! {len(results_df)} Short CIs analisados")
                            
                            # Armazenar resultados
                            analyzer.df_all_alerts = results_df
                            
                            # EstatÃ­sticas gerais
                            st.subheader("ğŸ“Š Resumo Geral")
                            
                            col1, col2, col3, col4 = st.columns(4)
                            
                            critical = len(results_df[results_df['classification'].str.contains('CRÃTICO', na=False)])
                            high = len(results_df[results_df['classification'].str.contains('PARCIALMENTE', na=False)])
                            medium = len(results_df[results_df['classification'].str.contains('DETECTÃVEL', na=False)])
                            low = len(results_df[results_df['classification'].str.contains('NÃƒO REINCIDENTE', na=False)])
                            
                            col1.metric("ğŸ”´ CrÃ­ticos (P1)", critical)
                            col2.metric("ğŸŸ  Altos (P2)", high)
                            col3.metric("ğŸŸ¡ MÃ©dios (P3)", medium)
                            col4.metric("ğŸŸ¢ Baixos (P4)", low)
                            
                            # GrÃ¡fico de distribuiÃ§Ã£o
                            st.subheader("ğŸ“Š DistribuiÃ§Ã£o de ClassificaÃ§Ãµes")
                            
                            classification_counts = results_df['classification'].value_counts()
                            fig = px.bar(
                                x=classification_counts.index,
                                y=classification_counts.values,
                                title="DistribuiÃ§Ã£o de ClassificaÃ§Ãµes de ReincidÃªncia",
                                labels={'x': 'ClassificaÃ§Ã£o', 'y': 'Quantidade'},
                                color=classification_counts.values,
                                color_continuous_scale='RdYlGn_r'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Tabela de resultados (top 20)
                            st.subheader("ğŸ† Top 20 Short CIs por Score de ReincidÃªncia")
                            
                            top_20 = results_df.nlargest(20, 'score')[
                                ['short_ci', 'total_occurrences', 'score', 'classification', 
                                 'cv', 'regularity_score', 'predictability_score']
                            ].round(2)
                            
                            st.dataframe(top_20, use_container_width=True)
                            
                            # Tabela completa
                            with st.expander("ğŸ“‹ Ver Todos os Resultados"):
                                st.dataframe(results_df.sort_values('score', ascending=False), 
                                           use_container_width=True)
                            
                            # GrÃ¡fico de scores
                            st.subheader("ğŸ“ˆ DistribuiÃ§Ã£o de Scores")
                            
                            fig = px.histogram(
                                results_df,
                                x='score',
                                nbins=20,
                                title="DistribuiÃ§Ã£o de Scores de ReincidÃªncia",
                                labels={'score': 'Score', 'count': 'FrequÃªncia'},
                                color_discrete_sequence=['steelblue']
                            )
                            fig.add_vline(x=70, line_dash="dash", line_color="red", 
                                        annotation_text="Threshold CrÃ­tico (70)")
                            fig.add_vline(x=50, line_dash="dash", line_color="orange", 
                                        annotation_text="Threshold Alto (50)")
                            fig.add_vline(x=30, line_dash="dash", line_color="yellow", 
                                        annotation_text="Threshold MÃ©dio (30)")
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Download
                            st.sidebar.markdown("---")
                            st.sidebar.subheader("ğŸ“¥ Downloads")
                            csv_buffer = io.StringIO()
                            analyzer.df_all_alerts.to_csv(csv_buffer, index=False)
                            st.sidebar.download_button(
                                label="â¬‡ï¸ Baixar AnÃ¡lise Global de ReincidÃªncia",
                                data=csv_buffer.getvalue(),
                                file_name=f"analise_reincidencia_global_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.error("âŒ NÃ£o foi possÃ­vel processar os dados para anÃ¡lise global")
            

            elif analysis_mode == "ğŸ“Š AnÃ¡lise Completa + CSV Consolidado":
                # ANÃLISE COMPLETA: Global + ReincidÃªncia em um Ãºnico CSV
                st.subheader("ğŸ“Š AnÃ¡lise Completa de Todos os Alertas")
                st.markdown("""
                Esta anÃ¡lise executarÃ¡:
                1. **AnÃ¡lise Global**: Isolados vs ContÃ­nuos, grupos, mÃ©tricas temporais
                2. **AnÃ¡lise de ReincidÃªncia**: 16 anÃ¡lises essenciais com score e classificaÃ§Ã£o
                3. **CSV Consolidado**: Todas as mÃ©tricas em um Ãºnico arquivo
                """)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“Š MÃ©tricas Globais", "20+")
                with col2:
                    st.metric("ğŸ”„ MÃ©tricas ReincidÃªncia", "12+")
                with col3:
                    st.metric("ğŸ“‹ Total de Colunas", "32+")
                
                if st.sidebar.button("ğŸš€ Executar AnÃ¡lise Completa", type="primary", key="complete_analysis"):
                    if analyzer.prepare_global_analysis():
                        num_short_ci = analyzer.df['short_ci'].nunique() if analyzer.df is not None else 0
                        st.info(f"ğŸ“Š Iniciando anÃ¡lise completa de {num_short_ci} Short CIs...")
                        st.warning("â±ï¸ Esta anÃ¡lise pode levar alguns minutos dependendo da quantidade de dados...")
                        
                        # Barra de progresso
                        progress_bar = st.progress(0, text="Iniciando anÃ¡lise completa...")
                        
                        # Executar anÃ¡lise completa
                        df_consolidated = analyzer.complete_analysis_all_short_ci(progress_bar)
                        
                        progress_bar.empty()
                        
                        if df_consolidated is not None and len(df_consolidated) > 0:
                            st.success(f"âœ… AnÃ¡lise completa concluÃ­da! {len(df_consolidated)} Short CIs analisados")
                            
                            # Armazenar resultados
                            analyzer.df_all_alerts = df_consolidated
                            
                            # ===============================================
                            # RESUMO EXECUTIVO
                            # ===============================================
                            st.header("ğŸ“Š Resumo Executivo")
                            
                            col1, col2, col3, col4 = st.columns(4)
                            
                            # Contadores de reincidÃªncia
                            critical = len(df_consolidated[df_consolidated['reincidencia_status'].str.contains('CRÃTICO', na=False)])
                            high = len(df_consolidated[df_consolidated['reincidencia_status'].str.contains('PARCIALMENTE', na=False)])
                            medium = len(df_consolidated[df_consolidated['reincidencia_status'].str.contains('DETECTÃVEL', na=False)])
                            low = len(df_consolidated[df_consolidated['reincidencia_status'].str.contains('NÃƒO REINCIDENTE', na=False)])
                            
                            col1.metric("ğŸ”´ CrÃ­ticos (P1)", critical, help="Score 70-100")
                            col2.metric("ğŸŸ  Altos (P2)", high, help="Score 50-69")
                            col3.metric("ğŸŸ¡ MÃ©dios (P3)", medium, help="Score 30-49")
                            col4.metric("ğŸŸ¢ Baixos (P4)", low, help="Score 0-29")
                            
                            st.markdown("---")
                            
                            # EstatÃ­sticas de padrÃµes
                            col1, col2, col3, col4 = st.columns(4)
                            
                            continuous_count = len(df_consolidated[df_consolidated['pattern_type'] == 'continuous'])
                            isolated_count = len(df_consolidated[df_consolidated['pattern_type'] == 'isolated'])
                            total_alerts = df_consolidated['total_ocorrencias'].sum()
                            avg_freq = df_consolidated['freq_dia'].mean()
                            
                            col1.metric("ğŸŸ¢ ContÃ­nuos", continuous_count)
                            col2.metric("ğŸ”´ Isolados", isolated_count)
                            col3.metric("ğŸ“Š Total Alertas", f"{total_alerts:,.0f}")
                            col4.metric("ğŸ“ˆ Freq. MÃ©dia/Dia", f"{avg_freq:.2f}")
                            
                            # ===============================================
                            # VISUALIZAÃ‡Ã•ES
                            # ===============================================
                            st.markdown("---")
                            st.subheader("ğŸ“ˆ VisualizaÃ§Ãµes")
                            
                            tab1, tab2, tab3 = st.tabs(["ğŸ¯ ReincidÃªncia", "ğŸ“Š PadrÃµes", "ğŸ”¥ Top Alertas"])
                            
                            with tab1:
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    # GrÃ¡fico de pizza - Status de ReincidÃªncia
                                    status_counts = df_consolidated['reincidencia_status'].value_counts()
                                    fig = px.pie(
                                        values=status_counts.values,
                                        names=status_counts.index,
                                        title="DistribuiÃ§Ã£o de Status de ReincidÃªncia",
                                        color_discrete_sequence=['red', 'orange', 'yellow', 'green', 'lightgray']
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                                
                                with col2:
                                    # Histograma de scores
                                    fig = px.histogram(
                                        df_consolidated,
                                        x='reincidencia_score',
                                        nbins=20,
                                        title="DistribuiÃ§Ã£o de Scores de ReincidÃªncia",
                                        labels={'reincidencia_score': 'Score', 'count': 'Quantidade'}
                                    )
                                    fig.add_vline(x=70, line_dash="dash", line_color="red", annotation_text="P1")
                                    fig.add_vline(x=50, line_dash="dash", line_color="orange", annotation_text="P2")
                                    fig.add_vline(x=30, line_dash="dash", line_color="yellow", annotation_text="P3")
                                    st.plotly_chart(fig, use_container_width=True)
                            
                            with tab2:
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    # DistribuiÃ§Ã£o de padrÃµes
                                    pattern_counts = df_consolidated['pattern_type'].value_counts()
                                    fig = px.bar(
                                        x=pattern_counts.index,
                                        y=pattern_counts.values,
                                        title="Isolados vs ContÃ­nuos",
                                        labels={'x': 'Tipo de PadrÃ£o', 'y': 'Quantidade'},
                                        color=pattern_counts.values,
                                        color_continuous_scale='RdYlGn'
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                                
                                with col2:
                                    # Scatter: Score vs FrequÃªncia
                                    fig = px.scatter(
                                        df_consolidated,
                                        x='freq_dia',
                                        y='reincidencia_score',
                                        color='pattern_type',
                                        size='total_ocorrencias',
                                        hover_data=['short_ci'],
                                        title="Score de ReincidÃªncia vs FrequÃªncia DiÃ¡ria",
                                        labels={'freq_dia': 'FrequÃªncia/Dia', 'reincidencia_score': 'Score'}
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                            
                            with tab3:
                                # Top 20 alertas crÃ­ticos
                                st.markdown("### ğŸ”´ Top 20 Alertas Mais CrÃ­ticos")
                                top_critical = df_consolidated.nlargest(20, 'reincidencia_score')[[
                                    'short_ci', 'reincidencia_score', 'reincidencia_status', 
                                    'pattern_type', 'total_ocorrencias', 'freq_dia', 
                                    'regularity_score', 'predictability_score'
                                ]].round(2)
                                st.dataframe(top_critical, use_container_width=True)
                                
                                # Top 10 mais frequentes
                                st.markdown("### ğŸ”¥ Top 10 Alertas Mais Frequentes")
                                top_freq = df_consolidated.nlargest(10, 'freq_dia')[[
                                    'short_ci', 'freq_dia', 'total_ocorrencias', 
                                    'reincidencia_score', 'reincidencia_status', 'pattern_type'
                                ]].round(2)
                                st.dataframe(top_freq, use_container_width=True)
                            
                            # ===============================================
                            # TABELA COMPLETA
                            # ===============================================
                            st.markdown("---")
                            st.subheader("ğŸ“‹ Dados Completos")
                            
                            with st.expander("ğŸ” Ver Tabela Completa de Resultados", expanded=False):
                                # Mostrar primeiras colunas importantes
                                display_columns = [
                                    'short_ci', 'reincidencia_score', 'reincidencia_status',
                                    'pattern_type', 'total_ocorrencias', 'num_grupos',
                                    'alertas_isolados', 'alertas_agrupados', 'freq_dia',
                                    'intervalo_medio_h', 'regularity_score', 'predictability_score'
                                ]
                                display_columns = [col for col in display_columns if col in df_consolidated.columns]
                                
                                st.dataframe(
                                    df_consolidated[display_columns].sort_values('reincidencia_score', ascending=False),
                                    use_container_width=True
                                )
                            
                            # ===============================================
                            # ESTATÃSTICAS DETALHADAS
                            # ===============================================
                            with st.expander("ğŸ“Š EstatÃ­sticas Detalhadas", expanded=False):
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.markdown("#### ğŸ¯ MÃ©tricas de ReincidÃªncia")
                                    st.write(f"**Score MÃ©dio:** {df_consolidated['reincidencia_score'].mean():.2f}")
                                    st.write(f"**Score Mediano:** {df_consolidated['reincidencia_score'].median():.2f}")
                                    st.write(f"**Desvio PadrÃ£o:** {df_consolidated['reincidencia_score'].std():.2f}")
                                    st.write(f"**Score MÃ¡ximo:** {df_consolidated['reincidencia_score'].max():.2f}")
                                    st.write(f"**Score MÃ­nimo:** {df_consolidated['reincidencia_score'].min():.2f}")
                                
                                with col2:
                                    st.markdown("#### ğŸ“ˆ MÃ©tricas de FrequÃªncia")
                                    st.write(f"**Freq. MÃ©dia/Dia:** {df_consolidated['freq_dia'].mean():.2f}")
                                    st.write(f"**Freq. Mediana/Dia:** {df_consolidated['freq_dia'].median():.2f}")
                                    st.write(f"**Total de Grupos:** {df_consolidated['num_grupos'].sum():.0f}")
                                    st.write(f"**% Alertas ContÃ­nuos:** {continuous_count/len(df_consolidated)*100:.1f}%")
                                    st.write(f"**% Alertas Isolados:** {isolated_count/len(df_consolidated)*100:.1f}%")
                            
                            # ===============================================
                            # DOWNLOAD
                            # ===============================================
                            st.markdown("---")
                            st.subheader("ğŸ“¥ Exportar Resultados")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("### ğŸ“Š CSV Completo")
                                st.info("ContÃ©m TODAS as mÃ©tricas: global + reincidÃªncia")
                                csv_buffer = io.StringIO()
                                df_consolidated.to_csv(csv_buffer, index=False)
                                st.download_button(
                                    label="â¬‡ï¸ Baixar AnÃ¡lise Completa (CSV)",
                                    data=csv_buffer.getvalue(),
                                    file_name=f"analise_completa_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                    mime="text/csv",
                                    use_container_width=True
                                )
                                st.caption(f"âœ… {len(df_consolidated.columns)} colunas | {len(df_consolidated)} linhas")
                            
                            with col2:
                                st.markdown("### ğŸ¯ CSV Resumido")
                                st.info("Apenas: short_ci, score e status")
                                summary_df = df_consolidated[['short_ci', 'reincidencia_score', 'reincidencia_status']].copy()
                                summary_df.columns = ['short_ci', 'score', 'status']
                                csv_summary = io.StringIO()
                                summary_df.to_csv(csv_summary, index=False)
                                st.download_button(
                                    label="â¬‡ï¸ Baixar Resumo (CSV)",
                                    data=csv_summary.getvalue(),
                                    file_name=f"resumo_reincidencia_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                    mime="text/csv",
                                    use_container_width=True
                                )
                                st.caption(f"âœ… 3 colunas essenciais | {len(summary_df)} linhas")
                            
                            # TambÃ©m adicionar na sidebar
                            st.sidebar.markdown("---")
                            st.sidebar.subheader("ğŸ“¥ Downloads RÃ¡pidos")
                            st.sidebar.download_button(
                                label="ğŸ“Š CSV Completo",
                                data=csv_buffer.getvalue(),
                                file_name=f"analise_completa_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                mime="text/csv"
                            )
                            st.sidebar.download_button(
                                label="ğŸ¯ CSV Resumido",
                                data=csv_summary.getvalue(),
                                file_name=f"resumo_reincidencia_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.error("âŒ NÃ£o foi possÃ­vel processar a anÃ¡lise completa")
            

            else:  # AnÃ¡lise Individual
                try:
                    id_counts = analyzer.df_original['short_ci'].value_counts()
                    id_options = [f"{uid} ({count} ocorrÃªncias)" for uid, count in id_counts.items()]
                    selected_option = st.sidebar.selectbox(
                        "ğŸ¯ Selecione o Short CI",
                        id_options,
                        help="Escolha o ID do alerta para anÃ¡lise (ordenado por frequÃªncia)"
                    )
                    selected_id = selected_option.split(" (")[0]  # Pega tudo ANTES de " ("

                    if st.sidebar.button("ğŸš€ Executar AnÃ¡lise Individual", type="primary"):
                        analyzer.max_gap_hours = max_gap_hours
                        analyzer.min_group_size = min_group_size
                        analyzer.spike_threshold_multiplier = spike_threshold_multiplier

                        if analyzer.prepare_individual_analysis(selected_id):
                            st.success(f"ğŸ¯ Analisando short_ci: {selected_id} ({len(analyzer.df)} registros)")
                            st.info(f"ğŸ“… **PerÃ­odo analisado:** {analyzer.dates.min()} atÃ© {analyzer.dates.max()}")

                            tab1, tab2, tab3 = st.tabs([
                                "ğŸ” Isolados vs Agrupados",
                                "ğŸ“Š BÃ¡sico", 
                                "â±ï¸ AnÃ¡lise de ReincidÃªncia (16 AnÃ¡lises Essenciais)"
                            ])

                            with tab1:
                                analyzer.show_individual_alert_analysis()
                            with tab2:
                                analyzer.show_basic_stats()
                            with tab3:
                                analyzer.analyze_temporal_recurrence_patterns()

                            st.sidebar.markdown("---")
                            st.sidebar.subheader("ğŸ“¥ Download")

                            csv_buffer = io.StringIO()
                            analyzer.df.to_csv(csv_buffer, index=False)
                            st.sidebar.download_button(
                                label="â¬‡ï¸ Baixar Dados Processados",
                                data=csv_buffer.getvalue(),
                                file_name=f"analise_{selected_id}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                                mime="text/csv"
                            )
                        else:
                            st.error(f"âŒ Nenhum registro encontrado para short_ci: {selected_id}")
                except Exception as e:
                    st.error(f"âŒ Erro ao processar anÃ¡lise individual: {e}")
    else:
        st.info("ğŸ‘† FaÃ§a upload de um arquivo CSV para comeÃ§ar a anÃ¡lise")
        with st.expander("ğŸ“– InstruÃ§Ãµes de Uso - VERSÃƒO COMPLETA"):
            st.markdown("""
            ### ğŸš€ Como usar este analisador COMPLETO:
            
            #### ğŸŒ **AnÃ¡lise Global**
            Analise todos os alertas com 7 abas:
            1. **VisÃ£o Geral:** Top alertas e distribuiÃ§Ãµes
            2. **Isolados vs ContÃ­nuos:** ComparaÃ§Ã£o detalhada com grÃ¡fico temporal
            3. **Grupos Detalhados:** VisualizaÃ§Ã£o interativa dos grupos identificados em alertas contÃ­nuos
            4. **RecorrÃªncia:** PadrÃµes de hora/dia APENAS de alertas contÃ­nuos
            5. **Agrupamento:** Clustering por comportamento
            6. **Perfis:** CaracterÃ­sticas de cada cluster
            7. **RecomendaÃ§Ãµes:** AÃ§Ãµes sugeridas
            
            #### ğŸ” **AnÃ¡lise Individual - 16 ANÃLISES ESSENCIAIS**
            Analise um alerta especÃ­fico em 3 abas:
            
            **1. Isolados vs Agrupados:** ClassificaÃ§Ã£o e timeline
            
            **2. BÃ¡sico:** EstatÃ­sticas gerais
            
            **3. AnÃ¡lise AvanÃ§ada de ReincidÃªncia (16 anÃ¡lises essenciais):**
            
            **AnÃ¡lises EstatÃ­sticas (1-8):**
            - âœ… EstatÃ­sticas de intervalos
            - âœ… ClassificaÃ§Ã£o de regularidade
            - âœ… Periodicidade (FFT)
            - âœ… AutocorrelaÃ§Ã£o
            - âœ… PadrÃµes temporais
            - âœ… Clusters temporais
            - âœ… DetecÃ§Ã£o de bursts
            - âœ… Sazonalidade
            
            **AnÃ¡lises de Previsibilidade (9-10):**
            - âœ… Score de previsibilidade
            - âœ… AnÃ¡lise de estabilidade
            
            **AnÃ¡lises Contextuais (11-13):**
            - âœ… DependÃªncias contextuais (feriados, fins de semana)
            - âœ… Janelas de vulnerabilidade (Top 5 horÃ¡rios crÃ­ticos)
            - âœ… Maturidade do padrÃ£o (evoluÃ§Ã£o temporal)
            
            **AnÃ¡lises Preditivas (14):**
            - âœ… ConfianÃ§a de prediÃ§Ã£o
            
            **AnÃ¡lises AvanÃ§adas de ML (15-16):**
            - âœ… **Cadeias de Markov** (matriz de transiÃ§Ã£o, distribuiÃ§Ã£o estacionÃ¡ria, previsibilidade markoviana)
            - âœ… **Bateria de Testes de Aleatoriedade:**
              - Runs Test (Wald-Wolfowitz)
              - Permutation Entropy
              - Approximate Entropy (ApEn)
              - Serial Correlation (Ljung-Box)
              - Hurst Exponent
            
            **ğŸ¯ ClassificaÃ§Ã£o Final de ReincidÃªncia:**
            - Score 0-100 baseado em 7 critÃ©rios essenciais
            - ClassificaÃ§Ã£o: CrÃ­tico (P1), Alto (P2), MÃ©dio (P3), Baixo (P4)
            - Plano de aÃ§Ã£o recomendado
            - PrediÃ§Ã£o de prÃ³xima ocorrÃªncia com intervalo de confianÃ§a
            - ExportaÃ§Ã£o de relatÃ³rio completo em CSV
            
            ### ğŸ–ï¸ Principais Funcionalidades:
            - âœ¨ IdentificaÃ§Ã£o automÃ¡tica de grupos contÃ­nuos
            - ğŸ“Š VisualizaÃ§Ã£o detalhada de grupos com timeline
            - ğŸ“ˆ AnÃ¡lise de recorrÃªncia (hora/dia) para alertas contÃ­nuos
            - ğŸ¯ Clustering inteligente por perfil de comportamento
            - â±ï¸ **16 anÃ¡lises essenciais de reincidÃªncia focadas em padrÃµes recorrentes**
            - ğŸ”´ SeparaÃ§Ã£o clara entre alertas isolados e contÃ­nuos
            - ğŸ† **Score final de reincidÃªncia (0-100) com 7 critÃ©rios essenciais**
            - ğŸ”— **AnÃ¡lise de Cadeias de Markov**
            - ğŸ² **Bateria completa de 5 testes de aleatoriedade**
            - ğŸŒ **DependÃªncias contextuais (feriados, fins de semana)**
            - ğŸ¯ **Janelas de vulnerabilidade temporal**
            - ğŸ“ˆ **AnÃ¡lise de maturidade do padrÃ£o**
            - ğŸ”® PrediÃ§Ã£o de prÃ³xima ocorrÃªncia com confianÃ§a
            - ğŸ“¥ ExportaÃ§Ã£o de relatÃ³rios completos
            
            ### ğŸ“‹ Colunas necessÃ¡rias no CSV:
            - `short_ci`: Identificador do CI (Configuration Item)
            - `created_on`: Data e hora da criaÃ§Ã£o do alerta
            
            ### âš™ï¸ ParÃ¢metros ConfigurÃ¡veis:
            - **Gap MÃ¡ximo:** Tempo mÃ¡ximo entre alertas do mesmo grupo
            - **Tamanho MÃ­nimo:** Quantidade mÃ­nima de alertas para formar um grupo
            - **Multiplicador de Spike:** Threshold para identificar dias com picos anormais
            
            ### ğŸ“Š MÃ©tricas de Score Final (7 critÃ©rios essenciais - 100 pontos):
            1. **Regularidade (20 pts)** - O mais importante: CV baixo indica padrÃ£o regular
            2. **Periodicidade (20 pts)** - Muito importante: detecÃ§Ã£o de ciclos/perÃ­odos
            3. **Previsibilidade (15 pts)** - Se Ã© previsÃ­vel, hÃ¡ padrÃ£o recorrente
            4. **Determinismo (15 pts)** - Comportamento nÃ£o-aleatÃ³rio indica padrÃ£o
            5. **AutocorrelaÃ§Ã£o (10 pts)** - Eventos correlacionados = recorrÃªncia
            6. **Estabilidade (10 pts)** - PadrÃ£o mantÃ©m-se ao longo do tempo
            7. **PadrÃ£o Markoviano (10 pts)** - TransiÃ§Ãµes de estado previsÃ­veis
            
            ### ğŸ¯ ClassificaÃ§Ãµes:
            - **70-100 pts:** ğŸ”´ ALERTA REINCIDENTE (P1 - CrÃ­tico)
            - **50-69 pts:** ğŸŸ  PARCIALMENTE REINCIDENTE (P2 - Alto)
            - **30-49 pts:** ğŸŸ¡ PADRÃƒO DETECTÃVEL (P3 - MÃ©dio)
            - **0-29 pts:** ğŸŸ¢ NÃƒO REINCIDENTE (P4 - Baixo)
            """)

if __name__ == "__main__":
    main() 